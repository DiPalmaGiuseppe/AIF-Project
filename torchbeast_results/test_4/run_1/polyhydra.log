[2025-01-17 21:33:00,499][root][INFO] - name: null
wandb: false
project: minihack
entity: entity_name
group: default
mock: false
single_ttyrec: true
num_seeds: 0
write_profiler_trace: false
relative_reward: false
fn_penalty_step: constant
penalty_time: 0.0
penalty_step: -0.001
reward_lose: 0
reward_win: 1
save_tty: false
character: null
mode: train
env: MiniHack-Combat-Skill-v0
obs_keys: glyphs,chars,colors,specials,blstats,message
num_actors: 256
total_steps: 500000
batch_size: 32
unroll_length: 80
num_learner_threads: 1
num_inference_threads: 1
disable_cuda: false
learner_device: cuda:0
actor_device: cuda:0
max_learner_queue_size: null
model: baseline
use_lstm: true
hidden_dim: 256
embedding_dim: 64
glyph_type: all_cat
equalize_input_dim: false
equalize_factor: 2
layers: 5
crop_model: cnn
crop_dim: 9
use_index_select: true
entropy_cost: 0.001
baseline_cost: 0.5
discounting: 0.999
reward_clipping: none
normalize_reward: true
learning_rate: 0.0002
grad_norm_clipping: 40
alpha: 0.99
momentum: 0
epsilon: 1.0e-06
state_counter: none
no_extrinsic: false
int:
  twoheaded: true
  input: full
  intrinsic_weight: 0.1
  discounting: 0.99
  baseline_cost: 0.5
  episodic: true
  reward_clipping: none
  normalize_reward: true
ride:
  count_norm: true
  forward_cost: 1
  inverse_cost: 0.1
  hidden_dim: 128
rnd:
  forward_cost: 0.01
msg:
  model: none
  hidden_dim: 64
  embedding_dim: 32

[2025-01-17 21:33:00,561][root][INFO] - Symlinked log directory: /opt/minihack/latest
[2025-01-17 21:33:00,562][root][INFO] - Found archive directory: /opt/minihack/archives
[2025-01-17 21:33:00,566][root][INFO] - Logging results to /opt/minihack
[2025-01-17 21:33:00,625][palaas/out][INFO] - Found log directory: /opt/minihack
[2025-01-17 21:33:00,625][palaas/out][INFO] - Saving arguments to /opt/minihack/meta.json
[2025-01-17 21:33:00,626][palaas/out][INFO] - Saving messages to /opt/minihack/out.log
[2025-01-17 21:33:00,626][palaas/out][INFO] - Saving logs data to /opt/minihack/logs.csv
[2025-01-17 21:33:00,626][palaas/out][INFO] - Saving logs' fields to /opt/minihack/fields.csv
[2025-01-17 21:33:00,628][root][INFO] - Not using CUDA.
[2025-01-17 21:33:00,640][root][INFO] - Using model baseline
[2025-01-17 21:33:00,641][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,672][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,748][root][INFO] - Number of model parameters: 4264078
[2025-01-17 21:33:00,749][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,775][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,849][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,849][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,851][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,849][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,851][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,852][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,852][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,853][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,854][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,851][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,857][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,857][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,861][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,853][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,855][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,865][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,867][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,866][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,852][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,868][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,864][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,872][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,872][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,873][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,872][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,874][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,874][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,860][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,877][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,877][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,878][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,871][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,881][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,864][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,884][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,863][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,886][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,886][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,886][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,886][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,888][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,880][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,892][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,895][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,896][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,897][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,896][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,899][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,902][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,895][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,897][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,905][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,905][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,906][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,906][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,908][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:00,908][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:05,849][root][INFO] - Step 0 @ 0.0 SPS. Inference batcher size: 37. Learner queue size: 0. Other stats: (train_seconds = 5.0)
[2025-01-17 21:33:10,859][root][INFO] - Step 0 @ 0.0 SPS. Inference batcher size: 28. Learner queue size: 25. Other stats: (train_seconds = 10.0)
[2025-01-17 21:33:15,862][root][INFO] - Step 0 @ 0.0 SPS. Inference batcher size: 79. Learner queue size: 25. Other stats: (train_seconds = 15.0)
[2025-01-17 21:33:20,868][root][INFO] - Step 0 @ 0.0 SPS. Inference batcher size: 79. Learner queue size: 25. Other stats: (train_seconds = 20.0)
[2025-01-17 21:33:25,869][root][INFO] - Step 0 @ 0.0 SPS. Inference batcher size: 40. Learner queue size: 25. Other stats: (train_seconds = 25.0)
[2025-01-17 21:33:30,921][root][INFO] - Step 0 @ 0.0 SPS. Inference batcher size: 65. Learner queue size: 25. Other stats: (train_seconds = 30.1)
[2025-01-17 21:33:35,926][root][INFO] - Step 0 @ 0.0 SPS. Inference batcher size: 82. Learner queue size: 25. Other stats: (train_seconds = 35.1)
[2025-01-17 21:33:40,931][root][INFO] - Step 0 @ 0.0 SPS. Inference batcher size: 82. Learner queue size: 25. Other stats: (train_seconds = 40.1)
[2025-01-17 21:33:45,935][root][INFO] - Step 0 @ 0.0 SPS. Inference batcher size: 106. Learner queue size: 25. Other stats: (train_seconds = 45.1)
[2025-01-17 21:33:48,735][palaas/out][INFO] - Updated log fields: ['_tick', '_time', 'train_seconds', 'step', 'mean_episode_return', 'mean_episode_step', 'total_loss', 'entropy_loss', 'pg_loss', 'baseline_loss', 'learner_queue_size']
[2025-01-17 21:33:50,941][root][INFO] - Saving checkpoint to /opt/minihack/checkpoint_0.tar
[2025-01-17 21:33:51,018][root][INFO] - Step 2560 @ 511.5 SPS. Inference batcher size: 141. Learner queue size: 28. Other stats: (train_seconds = 50.1, step = 2560, mean_episode_return = -0.0382, mean_episode_step = 36.732, total_loss = 1439.5, entropy_loss = -11.371, pg_loss = 744.64, baseline_loss = 706.22, learner_queue_size = 26, _tick = 0, _time = 1.7371e+09)
[2025-01-17 21:33:56,023][root][INFO] - Step 2560 @ 0.0 SPS. Inference batcher size: 54. Learner queue size: 30. Other stats: (train_seconds = 55.2, step = 2560, mean_episode_return = -0.0382, mean_episode_step = 36.732, total_loss = 1439.5, entropy_loss = -11.371, pg_loss = 744.64, baseline_loss = 706.22, learner_queue_size = 26, _tick = 0, _time = 1.7371e+09)
[2025-01-17 21:34:01,032][root][INFO] - Step 2560 @ 0.0 SPS. Inference batcher size: 34. Learner queue size: 2. Other stats: (train_seconds = 60.2, step = 2560, mean_episode_return = -0.0382, mean_episode_step = 36.732, total_loss = 1439.5, entropy_loss = -11.371, pg_loss = 744.64, baseline_loss = 706.22, learner_queue_size = 26, _tick = 0, _time = 1.7371e+09)
[2025-01-17 21:34:06,033][root][INFO] - Step 2560 @ 0.0 SPS. Inference batcher size: 79. Learner queue size: 4. Other stats: (train_seconds = 65.2, step = 2560, mean_episode_return = -0.0382, mean_episode_step = 36.732, total_loss = 1439.5, entropy_loss = -11.371, pg_loss = 744.64, baseline_loss = 706.22, learner_queue_size = 26, _tick = 0, _time = 1.7371e+09)
[2025-01-17 21:34:11,038][root][INFO] - Step 2560 @ 0.0 SPS. Inference batcher size: 79. Learner queue size: 4. Other stats: (train_seconds = 70.2, step = 2560, mean_episode_return = -0.0382, mean_episode_step = 36.732, total_loss = 1439.5, entropy_loss = -11.371, pg_loss = 744.64, baseline_loss = 706.22, learner_queue_size = 26, _tick = 0, _time = 1.7371e+09)
[2025-01-17 21:34:16,251][root][INFO] - Step 2560 @ 0.0 SPS. Inference batcher size: 53. Learner queue size: 13. Other stats: (train_seconds = 75.4, step = 2560, mean_episode_return = -0.0382, mean_episode_step = 36.732, total_loss = 1439.5, entropy_loss = -11.371, pg_loss = 744.64, baseline_loss = 706.22, learner_queue_size = 26, _tick = 0, _time = 1.7371e+09)
[2025-01-17 21:34:21,256][root][INFO] - Step 5120 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 80.4, step = 5120, mean_episode_return = -0.030952, mean_episode_step = 31.502, total_loss = -498.19, entropy_loss = -11.358, pg_loss = -685.28, baseline_loss = 198.45, learner_queue_size = 32, _tick = 1, _time = 1.7371e+09)
[2025-01-17 21:34:26,261][root][INFO] - Step 5120 @ 0.0 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (train_seconds = 85.4, step = 5120, mean_episode_return = -0.030952, mean_episode_step = 31.502, total_loss = -498.19, entropy_loss = -11.358, pg_loss = -685.28, baseline_loss = 198.45, learner_queue_size = 32, _tick = 1, _time = 1.7371e+09)
[2025-01-17 21:34:31,267][root][INFO] - Step 5120 @ 0.0 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (train_seconds = 90.4, step = 5120, mean_episode_return = -0.030952, mean_episode_step = 31.502, total_loss = -498.19, entropy_loss = -11.358, pg_loss = -685.28, baseline_loss = 198.45, learner_queue_size = 32, _tick = 1, _time = 1.7371e+09)
[2025-01-17 21:34:36,273][root][INFO] - Step 7680 @ 511.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 95.4, step = 7680, mean_episode_return = -0.085333, mean_episode_step = 78.02, total_loss = -88.114, entropy_loss = -11.356, pg_loss = -157.18, baseline_loss = 80.421, learner_queue_size = 32, _tick = 2, _time = 1.7371e+09)
[2025-01-17 21:34:41,278][root][INFO] - Step 10240 @ 511.4 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 100.4, step = 10240, mean_episode_return = -0.03164, mean_episode_step = 67.621, total_loss = 1067.5, entropy_loss = -11.228, pg_loss = 939.3, baseline_loss = 139.44, learner_queue_size = 32, _tick = 3, _time = 1.7371e+09)
[2025-01-17 21:34:46,283][root][INFO] - Step 10240 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 105.4, step = 10240, mean_episode_return = -0.03164, mean_episode_step = 67.621, total_loss = 1067.5, entropy_loss = -11.228, pg_loss = 939.3, baseline_loss = 139.44, learner_queue_size = 32, _tick = 3, _time = 1.7371e+09)
[2025-01-17 21:34:51,288][root][INFO] - Step 12800 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 110.4, step = 12800, mean_episode_return = -0.03405, mean_episode_step = 32.818, total_loss = 399.41, entropy_loss = -11.257, pg_loss = 314.73, baseline_loss = 95.933, learner_queue_size = 32, _tick = 4, _time = 1.7371e+09)
[2025-01-17 21:34:56,293][root][INFO] - Step 12800 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 115.4, step = 12800, mean_episode_return = -0.03405, mean_episode_step = 32.818, total_loss = 399.41, entropy_loss = -11.257, pg_loss = 314.73, baseline_loss = 95.933, learner_queue_size = 32, _tick = 4, _time = 1.7371e+09)
[2025-01-17 21:35:01,298][root][INFO] - Step 15360 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 120.5, step = 15360, mean_episode_return = 0.022913, mean_episode_step = 40.812, total_loss = -331.23, entropy_loss = -11.221, pg_loss = -409.35, baseline_loss = 89.339, learner_queue_size = 32, _tick = 5, _time = 1.7371e+09)
[2025-01-17 21:35:06,303][root][INFO] - Step 17920 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 125.5, step = 17920, mean_episode_return = -0.033923, mean_episode_step = 58.158, total_loss = 608.3, entropy_loss = -11.243, pg_loss = 473.44, baseline_loss = 146.11, learner_queue_size = 32, _tick = 6, _time = 1.7371e+09)
[2025-01-17 21:35:11,308][root][INFO] - Step 17920 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 130.5, step = 17920, mean_episode_return = -0.033923, mean_episode_step = 58.158, total_loss = 608.3, entropy_loss = -11.243, pg_loss = 473.44, baseline_loss = 146.11, learner_queue_size = 32, _tick = 6, _time = 1.7371e+09)
[2025-01-17 21:35:16,321][root][INFO] - Step 20480 @ 511.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 135.5, step = 20480, mean_episode_return = -0.068296, mean_episode_step = 56.191, total_loss = -250.11, entropy_loss = -11.223, pg_loss = -318.54, baseline_loss = 79.658, learner_queue_size = 32, _tick = 7, _time = 1.7371e+09)
[2025-01-17 21:35:21,326][root][INFO] - Step 20480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 140.5, step = 20480, mean_episode_return = -0.068296, mean_episode_step = 56.191, total_loss = -250.11, entropy_loss = -11.223, pg_loss = -318.54, baseline_loss = 79.658, learner_queue_size = 32, _tick = 7, _time = 1.7371e+09)
[2025-01-17 21:35:26,331][root][INFO] - Step 23040 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 145.5, step = 23040, mean_episode_return = -0.004294, mean_episode_step = 55.553, total_loss = 598.92, entropy_loss = -11.196, pg_loss = 457.84, baseline_loss = 152.28, learner_queue_size = 32, _tick = 8, _time = 1.7371e+09)
[2025-01-17 21:35:31,336][root][INFO] - Step 25600 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 150.5, step = 25600, mean_episode_return = -0.004148, mean_episode_step = 50.408, total_loss = -188.95, entropy_loss = -11.179, pg_loss = -261.58, baseline_loss = 83.809, learner_queue_size = 32, _tick = 9, _time = 1.7371e+09)
[2025-01-17 21:35:36,341][root][INFO] - Step 25600 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 155.5, step = 25600, mean_episode_return = -0.004148, mean_episode_step = 50.408, total_loss = -188.95, entropy_loss = -11.179, pg_loss = -261.58, baseline_loss = 83.809, learner_queue_size = 32, _tick = 9, _time = 1.7371e+09)
[2025-01-17 21:35:41,346][root][INFO] - Step 28160 @ 511.5 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (train_seconds = 160.5, step = 28160, mean_episode_return = -0.0057878, mean_episode_step = 52.015, total_loss = -348.22, entropy_loss = -11.156, pg_loss = -337.99, baseline_loss = 0.92588, learner_queue_size = 32, _tick = 10, _time = 1.7371e+09)
[2025-01-17 21:35:46,351][root][INFO] - Step 28160 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 165.5, step = 28160, mean_episode_return = -0.0057878, mean_episode_step = 52.015, total_loss = -348.22, entropy_loss = -11.156, pg_loss = -337.99, baseline_loss = 0.92588, learner_queue_size = 32, _tick = 10, _time = 1.7371e+09)
[2025-01-17 21:35:51,356][root][INFO] - Step 30720 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 170.5, step = 30720, mean_episode_return = 0.048533, mean_episode_step = 57.763, total_loss = 660.32, entropy_loss = -11.144, pg_loss = 561.34, baseline_loss = 110.13, learner_queue_size = 32, _tick = 11, _time = 1.7371e+09)
[2025-01-17 21:35:56,362][root][INFO] - Step 33280 @ 511.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 175.5, step = 33280, mean_episode_return = 0.0041786, mean_episode_step = 55.975, total_loss = 260.49, entropy_loss = -11.136, pg_loss = 101.47, baseline_loss = 170.16, learner_queue_size = 32, _tick = 12, _time = 1.7371e+09)
[2025-01-17 21:36:01,367][root][INFO] - Step 33280 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 180.5, step = 33280, mean_episode_return = 0.0041786, mean_episode_step = 55.975, total_loss = 260.49, entropy_loss = -11.136, pg_loss = 101.47, baseline_loss = 170.16, learner_queue_size = 32, _tick = 12, _time = 1.7371e+09)
[2025-01-17 21:36:06,374][root][INFO] - Step 35840 @ 511.3 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 185.5, step = 35840, mean_episode_return = -0.060429, mean_episode_step = 52.454, total_loss = 932.25, entropy_loss = -11.076, pg_loss = 768.99, baseline_loss = 174.34, learner_queue_size = 32, _tick = 13, _time = 1.7371e+09)
[2025-01-17 21:36:11,379][root][INFO] - Step 35840 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 190.5, step = 35840, mean_episode_return = -0.060429, mean_episode_step = 52.454, total_loss = 932.25, entropy_loss = -11.076, pg_loss = 768.99, baseline_loss = 174.34, learner_queue_size = 32, _tick = 13, _time = 1.7371e+09)
[2025-01-17 21:36:16,385][root][INFO] - Step 38400 @ 511.4 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 195.5, step = 38400, mean_episode_return = 0.031882, mean_episode_step = 59.514, total_loss = 576.27, entropy_loss = -11.079, pg_loss = 453.01, baseline_loss = 134.33, learner_queue_size = 32, _tick = 14, _time = 1.7371e+09)
[2025-01-17 21:36:21,390][root][INFO] - Step 40960 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 200.5, step = 40960, mean_episode_return = -0.037367, mean_episode_step = 60.538, total_loss = -593.85, entropy_loss = -11.073, pg_loss = -630.31, baseline_loss = 47.54, learner_queue_size = 32, _tick = 15, _time = 1.7371e+09)
[2025-01-17 21:36:26,396][root][INFO] - Step 40960 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 205.6, step = 40960, mean_episode_return = -0.037367, mean_episode_step = 60.538, total_loss = -593.85, entropy_loss = -11.073, pg_loss = -630.31, baseline_loss = 47.54, learner_queue_size = 32, _tick = 15, _time = 1.7371e+09)
[2025-01-17 21:36:31,401][root][INFO] - Step 43520 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 210.6, step = 43520, mean_episode_return = -0.038306, mean_episode_step = 53.824, total_loss = -512.52, entropy_loss = -11.074, pg_loss = -540.39, baseline_loss = 38.938, learner_queue_size = 32, _tick = 16, _time = 1.7371e+09)
[2025-01-17 21:36:36,406][root][INFO] - Step 43520 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 215.6, step = 43520, mean_episode_return = -0.038306, mean_episode_step = 53.824, total_loss = -512.52, entropy_loss = -11.074, pg_loss = -540.39, baseline_loss = 38.938, learner_queue_size = 32, _tick = 16, _time = 1.7371e+09)
[2025-01-17 21:36:41,411][root][INFO] - Step 46080 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 220.6, step = 46080, mean_episode_return = 0.042806, mean_episode_step = 58.354, total_loss = 406.94, entropy_loss = -11.067, pg_loss = 276.16, baseline_loss = 141.84, learner_queue_size = 32, _tick = 17, _time = 1.7371e+09)
[2025-01-17 21:36:46,417][root][INFO] - Step 48640 @ 511.4 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 225.6, step = 48640, mean_episode_return = -0.010529, mean_episode_step = 50.58, total_loss = 80.373, entropy_loss = -11.044, pg_loss = -6.5688, baseline_loss = 97.986, learner_queue_size = 32, _tick = 18, _time = 1.7371e+09)
[2025-01-17 21:36:51,423][root][INFO] - Step 48640 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 230.6, step = 48640, mean_episode_return = -0.010529, mean_episode_step = 50.58, total_loss = 80.373, entropy_loss = -11.044, pg_loss = -6.5688, baseline_loss = 97.986, learner_queue_size = 32, _tick = 18, _time = 1.7371e+09)
[2025-01-17 21:36:56,428][root][INFO] - Step 51200 @ 511.5 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 235.6, step = 51200, mean_episode_return = 0.023714, mean_episode_step = 52.191, total_loss = -343.9, entropy_loss = -11.003, pg_loss = -407.54, baseline_loss = 74.641, learner_queue_size = 32, _tick = 19, _time = 1.7371e+09)
[2025-01-17 21:37:01,435][root][INFO] - Step 53760 @ 511.3 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 240.6, step = 53760, mean_episode_return = -0.054108, mean_episode_step = 43.92, total_loss = 374.25, entropy_loss = -11.001, pg_loss = 270.44, baseline_loss = 114.81, learner_queue_size = 32, _tick = 20, _time = 1.7371e+09)
[2025-01-17 21:37:06,441][root][INFO] - Step 53760 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 245.6, step = 53760, mean_episode_return = -0.054108, mean_episode_step = 43.92, total_loss = 374.25, entropy_loss = -11.001, pg_loss = 270.44, baseline_loss = 114.81, learner_queue_size = 32, _tick = 20, _time = 1.7371e+09)
[2025-01-17 21:37:11,446][root][INFO] - Step 56320 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 250.6, step = 56320, mean_episode_return = 0.03, mean_episode_step = 58.69, total_loss = -229.45, entropy_loss = -11.013, pg_loss = -279.68, baseline_loss = 61.241, learner_queue_size = 32, _tick = 21, _time = 1.7371e+09)
[2025-01-17 21:37:16,451][root][INFO] - Step 56320 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 255.6, step = 56320, mean_episode_return = 0.03, mean_episode_step = 58.69, total_loss = -229.45, entropy_loss = -11.013, pg_loss = -279.68, baseline_loss = 61.241, learner_queue_size = 32, _tick = 21, _time = 1.7371e+09)
[2025-01-17 21:37:21,456][root][INFO] - Step 58880 @ 511.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 260.6, step = 58880, mean_episode_return = -0.0102, mean_episode_step = 49.163, total_loss = 543.09, entropy_loss = -11.018, pg_loss = 386.46, baseline_loss = 167.65, learner_queue_size = 32, _tick = 22, _time = 1.7371e+09)
[2025-01-17 21:37:26,461][root][INFO] - Step 61440 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 265.6, step = 61440, mean_episode_return = 0.049588, mean_episode_step = 55.969, total_loss = 477.23, entropy_loss = -11.003, pg_loss = 332.95, baseline_loss = 155.29, learner_queue_size = 32, _tick = 23, _time = 1.7371e+09)
[2025-01-17 21:37:31,466][root][INFO] - Step 61440 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 270.6, step = 61440, mean_episode_return = 0.049588, mean_episode_step = 55.969, total_loss = 477.23, entropy_loss = -11.003, pg_loss = 332.95, baseline_loss = 155.29, learner_queue_size = 32, _tick = 23, _time = 1.7371e+09)
[2025-01-17 21:37:36,473][root][INFO] - Step 64000 @ 511.3 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 275.6, step = 64000, mean_episode_return = 0.039088, mean_episode_step = 61.257, total_loss = 30.045, entropy_loss = -10.98, pg_loss = -108.8, baseline_loss = 149.83, learner_queue_size = 32, _tick = 24, _time = 1.7371e+09)
[2025-01-17 21:37:41,478][root][INFO] - Step 64000 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 280.6, step = 64000, mean_episode_return = 0.039088, mean_episode_step = 61.257, total_loss = 30.045, entropy_loss = -10.98, pg_loss = -108.8, baseline_loss = 149.83, learner_queue_size = 32, _tick = 24, _time = 1.7371e+09)
[2025-01-17 21:37:46,483][root][INFO] - Step 66560 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 285.6, step = 66560, mean_episode_return = 0.00955, mean_episode_step = 55.975, total_loss = -579.44, entropy_loss = -10.999, pg_loss = -620.06, baseline_loss = 51.626, learner_queue_size = 32, _tick = 25, _time = 1.7371e+09)
[2025-01-17 21:37:51,488][root][INFO] - Step 69120 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 290.6, step = 69120, mean_episode_return = 0.062576, mean_episode_step = 57.247, total_loss = 787.89, entropy_loss = -10.982, pg_loss = 611.7, baseline_loss = 187.17, learner_queue_size = 32, _tick = 26, _time = 1.7371e+09)
[2025-01-17 21:37:56,494][root][INFO] - Step 69120 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 295.6, step = 69120, mean_episode_return = 0.062576, mean_episode_step = 57.247, total_loss = 787.89, entropy_loss = -10.982, pg_loss = 611.7, baseline_loss = 187.17, learner_queue_size = 32, _tick = 26, _time = 1.7371e+09)
[2025-01-17 21:38:01,499][root][INFO] - Step 71680 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 300.7, step = 71680, mean_episode_return = 0.07469, mean_episode_step = 48.867, total_loss = 270.46, entropy_loss = -10.963, pg_loss = 114.72, baseline_loss = 166.7, learner_queue_size = 32, _tick = 27, _time = 1.7371e+09)
[2025-01-17 21:38:06,505][root][INFO] - Step 71680 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 305.7, step = 71680, mean_episode_return = 0.07469, mean_episode_step = 48.867, total_loss = 270.46, entropy_loss = -10.963, pg_loss = 114.72, baseline_loss = 166.7, learner_queue_size = 32, _tick = 27, _time = 1.7371e+09)
[2025-01-17 21:38:11,510][root][INFO] - Step 74240 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 310.7, step = 74240, mean_episode_return = 0.022974, mean_episode_step = 43.571, total_loss = 389.51, entropy_loss = -10.941, pg_loss = 228.63, baseline_loss = 171.82, learner_queue_size = 32, _tick = 28, _time = 1.7371e+09)
[2025-01-17 21:38:16,515][root][INFO] - Step 76800 @ 511.5 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (train_seconds = 315.7, step = 76800, mean_episode_return = -0.018387, mean_episode_step = 53.454, total_loss = 409.69, entropy_loss = -10.895, pg_loss = 254.13, baseline_loss = 166.45, learner_queue_size = 32, _tick = 29, _time = 1.7371e+09)
[2025-01-17 21:38:21,521][root][INFO] - Step 76800 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 320.7, step = 76800, mean_episode_return = -0.018387, mean_episode_step = 53.454, total_loss = 409.69, entropy_loss = -10.895, pg_loss = 254.13, baseline_loss = 166.45, learner_queue_size = 32, _tick = 29, _time = 1.7371e+09)
[2025-01-17 21:38:26,527][root][INFO] - Step 79360 @ 511.4 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 325.7, step = 79360, mean_episode_return = -0.031675, mean_episode_step = 52.637, total_loss = 256.73, entropy_loss = -10.829, pg_loss = 93.599, baseline_loss = 173.96, learner_queue_size = 32, _tick = 30, _time = 1.7371e+09)
[2025-01-17 21:38:31,532][root][INFO] - Step 81920 @ 511.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 330.7, step = 81920, mean_episode_return = -0.025, mean_episode_step = 60.043, total_loss = -471.78, entropy_loss = -10.785, pg_loss = -523.3, baseline_loss = 62.302, learner_queue_size = 32, _tick = 31, _time = 1.7371e+09)
[2025-01-17 21:38:36,537][root][INFO] - Step 81920 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 335.7, step = 81920, mean_episode_return = -0.025, mean_episode_step = 60.043, total_loss = -471.78, entropy_loss = -10.785, pg_loss = -523.3, baseline_loss = 62.302, learner_queue_size = 32, _tick = 31, _time = 1.7371e+09)
[2025-01-17 21:38:41,544][root][INFO] - Step 84480 @ 511.3 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (train_seconds = 340.7, step = 84480, mean_episode_return = -0.0105, mean_episode_step = 52.743, total_loss = 155.91, entropy_loss = -10.793, pg_loss = 35.259, baseline_loss = 131.44, learner_queue_size = 32, _tick = 32, _time = 1.7371e+09)
[2025-01-17 21:38:46,549][root][INFO] - Step 84480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 345.7, step = 84480, mean_episode_return = -0.0105, mean_episode_step = 52.743, total_loss = 155.91, entropy_loss = -10.793, pg_loss = 35.259, baseline_loss = 131.44, learner_queue_size = 32, _tick = 32, _time = 1.7371e+09)
[2025-01-17 21:38:51,554][root][INFO] - Step 87040 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 350.7, step = 87040, mean_episode_return = 0.041972, mean_episode_step = 47.085, total_loss = -350.62, entropy_loss = -10.724, pg_loss = -402.19, baseline_loss = 62.291, learner_queue_size = 32, _tick = 33, _time = 1.7371e+09)
[2025-01-17 21:38:56,559][root][INFO] - Step 89600 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 355.7, step = 89600, mean_episode_return = 0.053556, mean_episode_step = 55.284, total_loss = 379.27, entropy_loss = -10.678, pg_loss = 240.46, baseline_loss = 149.49, learner_queue_size = 32, _tick = 34, _time = 1.7371e+09)
[2025-01-17 21:39:01,565][root][INFO] - Step 89600 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 360.7, step = 89600, mean_episode_return = 0.053556, mean_episode_step = 55.284, total_loss = 379.27, entropy_loss = -10.678, pg_loss = 240.46, baseline_loss = 149.49, learner_queue_size = 32, _tick = 34, _time = 1.7371e+09)
[2025-01-17 21:39:06,570][root][INFO] - Step 92160 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 365.7, step = 92160, mean_episode_return = 0.073, mean_episode_step = 62.51, total_loss = 630.82, entropy_loss = -10.638, pg_loss = 460.4, baseline_loss = 181.06, learner_queue_size = 32, _tick = 35, _time = 1.7371e+09)
[2025-01-17 21:39:11,576][root][INFO] - Step 92160 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 370.7, step = 92160, mean_episode_return = 0.073, mean_episode_step = 62.51, total_loss = 630.82, entropy_loss = -10.638, pg_loss = 460.4, baseline_loss = 181.06, learner_queue_size = 32, _tick = 35, _time = 1.7371e+09)
[2025-01-17 21:39:16,581][root][INFO] - Step 94720 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 375.7, step = 94720, mean_episode_return = 0.10704, mean_episode_step = 65.22, total_loss = 458.37, entropy_loss = -10.583, pg_loss = 290.97, baseline_loss = 177.98, learner_queue_size = 32, _tick = 36, _time = 1.7371e+09)
[2025-01-17 21:39:21,587][root][INFO] - Step 97280 @ 511.4 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (train_seconds = 380.7, step = 97280, mean_episode_return = 0.016, mean_episode_step = 57.725, total_loss = 152.92, entropy_loss = -10.494, pg_loss = 22.944, baseline_loss = 140.47, learner_queue_size = 32, _tick = 37, _time = 1.7371e+09)
[2025-01-17 21:39:26,592][root][INFO] - Step 97280 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 385.7, step = 97280, mean_episode_return = 0.016, mean_episode_step = 57.725, total_loss = 152.92, entropy_loss = -10.494, pg_loss = 22.944, baseline_loss = 140.47, learner_queue_size = 32, _tick = 37, _time = 1.7371e+09)
[2025-01-17 21:39:31,597][root][INFO] - Step 99840 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 390.8, step = 99840, mean_episode_return = 0.13087, mean_episode_step = 64.98, total_loss = -268.34, entropy_loss = -10.421, pg_loss = -392.54, baseline_loss = 134.62, learner_queue_size = 32, _tick = 38, _time = 1.7371e+09)
[2025-01-17 21:39:36,602][root][INFO] - Step 99840 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 395.8, step = 99840, mean_episode_return = 0.13087, mean_episode_step = 64.98, total_loss = -268.34, entropy_loss = -10.421, pg_loss = -392.54, baseline_loss = 134.62, learner_queue_size = 32, _tick = 38, _time = 1.7371e+09)
[2025-01-17 21:39:41,607][root][INFO] - Step 102400 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 400.8, step = 102400, mean_episode_return = 0.10913, mean_episode_step = 54.158, total_loss = -157.64, entropy_loss = -10.415, pg_loss = -252.12, baseline_loss = 104.9, learner_queue_size = 32, _tick = 39, _time = 1.7371e+09)
[2025-01-17 21:39:46,612][root][INFO] - Step 104960 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 405.8, step = 104960, mean_episode_return = -0.030903, mean_episode_step = 59.951, total_loss = -22.558, entropy_loss = -10.355, pg_loss = -110.74, baseline_loss = 98.535, learner_queue_size = 32, _tick = 40, _time = 1.7371e+09)
[2025-01-17 21:39:51,618][root][INFO] - Step 104960 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 410.8, step = 104960, mean_episode_return = -0.030903, mean_episode_step = 59.951, total_loss = -22.558, entropy_loss = -10.355, pg_loss = -110.74, baseline_loss = 98.535, learner_queue_size = 32, _tick = 40, _time = 1.7371e+09)
[2025-01-17 21:39:56,624][root][INFO] - Step 107520 @ 511.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 415.8, step = 107520, mean_episode_return = 0.090485, mean_episode_step = 65.337, total_loss = 207.41, entropy_loss = -10.293, pg_loss = 108.21, baseline_loss = 109.5, learner_queue_size = 32, _tick = 41, _time = 1.7371e+09)
[2025-01-17 21:40:01,630][root][INFO] - Step 110080 @ 511.3 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 420.8, step = 110080, mean_episode_return = 0.020077, mean_episode_step = 48.51, total_loss = -245.63, entropy_loss = -10.206, pg_loss = -326.64, baseline_loss = 91.217, learner_queue_size = 32, _tick = 42, _time = 1.7372e+09)
[2025-01-17 21:40:06,635][root][INFO] - Step 110080 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 425.8, step = 110080, mean_episode_return = 0.020077, mean_episode_step = 48.51, total_loss = -245.63, entropy_loss = -10.206, pg_loss = -326.64, baseline_loss = 91.217, learner_queue_size = 32, _tick = 42, _time = 1.7372e+09)
[2025-01-17 21:40:11,640][root][INFO] - Step 112640 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 430.8, step = 112640, mean_episode_return = 0.074667, mean_episode_step = 57.279, total_loss = 465.39, entropy_loss = -10.157, pg_loss = 350.16, baseline_loss = 125.39, learner_queue_size = 32, _tick = 43, _time = 1.7372e+09)
[2025-01-17 21:40:16,645][root][INFO] - Step 112640 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 435.8, step = 112640, mean_episode_return = 0.074667, mean_episode_step = 57.279, total_loss = 465.39, entropy_loss = -10.157, pg_loss = 350.16, baseline_loss = 125.39, learner_queue_size = 32, _tick = 43, _time = 1.7372e+09)
[2025-01-17 21:40:21,651][root][INFO] - Step 115200 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 440.8, step = 115200, mean_episode_return = 0.080806, mean_episode_step = 61.283, total_loss = -521.31, entropy_loss = -10.103, pg_loss = -545.73, baseline_loss = 34.53, learner_queue_size = 32, _tick = 44, _time = 1.7372e+09)
[2025-01-17 21:40:26,656][root][INFO] - Step 117760 @ 511.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 445.8, step = 117760, mean_episode_return = 0.14659, mean_episode_step = 69.401, total_loss = 594.62, entropy_loss = -10.071, pg_loss = 475.44, baseline_loss = 129.26, learner_queue_size = 32, _tick = 45, _time = 1.7372e+09)
[2025-01-17 21:40:31,662][root][INFO] - Step 117760 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 450.8, step = 117760, mean_episode_return = 0.14659, mean_episode_step = 69.401, total_loss = 594.62, entropy_loss = -10.071, pg_loss = 475.44, baseline_loss = 129.26, learner_queue_size = 32, _tick = 45, _time = 1.7372e+09)
[2025-01-17 21:40:36,669][root][INFO] - Step 120320 @ 511.3 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 455.8, step = 120320, mean_episode_return = 0.10016, mean_episode_step = 60.526, total_loss = -422.72, entropy_loss = -10.092, pg_loss = -510.71, baseline_loss = 98.088, learner_queue_size = 32, _tick = 46, _time = 1.7372e+09)
[2025-01-17 21:40:41,674][root][INFO] - Step 120320 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 460.8, step = 120320, mean_episode_return = 0.10016, mean_episode_step = 60.526, total_loss = -422.72, entropy_loss = -10.092, pg_loss = -510.71, baseline_loss = 98.088, learner_queue_size = 32, _tick = 46, _time = 1.7372e+09)
[2025-01-17 21:40:46,679][root][INFO] - Step 122880 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 465.8, step = 122880, mean_episode_return = 0.076711, mean_episode_step = 63.296, total_loss = -272.15, entropy_loss = -10.063, pg_loss = -335.74, baseline_loss = 73.654, learner_queue_size = 32, _tick = 47, _time = 1.7372e+09)
[2025-01-17 21:40:51,685][root][INFO] - Saving checkpoint to /opt/minihack/checkpoint_0.25.tar
[2025-01-17 21:40:51,749][root][INFO] - Step 125440 @ 511.5 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 470.8, step = 125440, mean_episode_return = 0.03076, mean_episode_step = 63.502, total_loss = 629.15, entropy_loss = -10.049, pg_loss = 473.84, baseline_loss = 165.36, learner_queue_size = 32, _tick = 48, _time = 1.7372e+09)
[2025-01-17 21:40:56,754][root][INFO] - Step 125440 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 475.9, step = 125440, mean_episode_return = 0.03076, mean_episode_step = 63.502, total_loss = 629.15, entropy_loss = -10.049, pg_loss = 473.84, baseline_loss = 165.36, learner_queue_size = 32, _tick = 48, _time = 1.7372e+09)
[2025-01-17 21:41:01,759][root][INFO] - Step 128000 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 480.9, step = 128000, mean_episode_return = 0.053333, mean_episode_step = 63.131, total_loss = 681.69, entropy_loss = -10.02, pg_loss = 526.03, baseline_loss = 165.69, learner_queue_size = 32, _tick = 49, _time = 1.7372e+09)
[2025-01-17 21:41:06,764][root][INFO] - Step 128000 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 485.9, step = 128000, mean_episode_return = 0.053333, mean_episode_step = 63.131, total_loss = 681.69, entropy_loss = -10.02, pg_loss = 526.03, baseline_loss = 165.69, learner_queue_size = 32, _tick = 49, _time = 1.7372e+09)
[2025-01-17 21:41:11,769][root][INFO] - Step 130560 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 490.9, step = 130560, mean_episode_return = 0.094594, mean_episode_step = 71.467, total_loss = -313.97, entropy_loss = -10.041, pg_loss = -420.02, baseline_loss = 116.09, learner_queue_size = 32, _tick = 50, _time = 1.7372e+09)
[2025-01-17 21:41:16,774][root][INFO] - Step 133120 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 495.9, step = 133120, mean_episode_return = 0.26636, mean_episode_step = 69.259, total_loss = 580.59, entropy_loss = -10.013, pg_loss = 402.86, baseline_loss = 187.74, learner_queue_size = 32, _tick = 51, _time = 1.7372e+09)
[2025-01-17 21:41:21,779][root][INFO] - Step 133120 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 500.9, step = 133120, mean_episode_return = 0.26636, mean_episode_step = 69.259, total_loss = 580.59, entropy_loss = -10.013, pg_loss = 402.86, baseline_loss = 187.74, learner_queue_size = 32, _tick = 51, _time = 1.7372e+09)
[2025-01-17 21:41:26,784][root][INFO] - Step 135680 @ 511.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 505.9, step = 135680, mean_episode_return = 0.11594, mean_episode_step = 68.753, total_loss = 285.86, entropy_loss = -9.9891, pg_loss = 109.82, baseline_loss = 186.02, learner_queue_size = 32, _tick = 52, _time = 1.7372e+09)
[2025-01-17 21:41:31,789][root][INFO] - Step 138240 @ 511.5 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 510.9, step = 138240, mean_episode_return = 0.33263, mean_episode_step = 65.411, total_loss = 1933.0, entropy_loss = -10.009, pg_loss = 1624.9, baseline_loss = 318.08, learner_queue_size = 32, _tick = 53, _time = 1.7372e+09)
[2025-01-17 21:41:36,794][root][INFO] - Step 138240 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 515.9, step = 138240, mean_episode_return = 0.33263, mean_episode_step = 65.411, total_loss = 1933.0, entropy_loss = -10.009, pg_loss = 1624.9, baseline_loss = 318.08, learner_queue_size = 32, _tick = 53, _time = 1.7372e+09)
[2025-01-17 21:41:41,799][root][INFO] - Step 140800 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 521.0, step = 140800, mean_episode_return = 0.24912, mean_episode_step = 66.492, total_loss = -242.84, entropy_loss = -9.9961, pg_loss = -349.68, baseline_loss = 116.83, learner_queue_size = 32, _tick = 54, _time = 1.7372e+09)
[2025-01-17 21:41:46,804][root][INFO] - Step 140800 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 526.0, step = 140800, mean_episode_return = 0.24912, mean_episode_step = 66.492, total_loss = -242.84, entropy_loss = -9.9961, pg_loss = -349.68, baseline_loss = 116.83, learner_queue_size = 32, _tick = 54, _time = 1.7372e+09)
[2025-01-17 21:41:51,809][root][INFO] - Step 143360 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 531.0, step = 143360, mean_episode_return = 0.18469, mean_episode_step = 54.67, total_loss = 912.81, entropy_loss = -9.96, pg_loss = 721.92, baseline_loss = 200.85, learner_queue_size = 32, _tick = 55, _time = 1.7372e+09)
[2025-01-17 21:41:56,814][root][INFO] - Step 145920 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 536.0, step = 145920, mean_episode_return = 0.080641, mean_episode_step = 75.052, total_loss = 201.53, entropy_loss = -9.9887, pg_loss = 43.051, baseline_loss = 168.47, learner_queue_size = 32, _tick = 56, _time = 1.7372e+09)
[2025-01-17 21:42:01,819][root][INFO] - Step 145920 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 541.0, step = 145920, mean_episode_return = 0.080641, mean_episode_step = 75.052, total_loss = 201.53, entropy_loss = -9.9887, pg_loss = 43.051, baseline_loss = 168.47, learner_queue_size = 32, _tick = 56, _time = 1.7372e+09)
[2025-01-17 21:42:06,824][root][INFO] - Step 148480 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 546.0, step = 148480, mean_episode_return = 0.16272, mean_episode_step = 68.398, total_loss = 496.58, entropy_loss = -9.9756, pg_loss = 313.43, baseline_loss = 193.13, learner_queue_size = 32, _tick = 57, _time = 1.7372e+09)
[2025-01-17 21:42:11,829][root][INFO] - Step 148480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 551.0, step = 148480, mean_episode_return = 0.16272, mean_episode_step = 68.398, total_loss = 496.58, entropy_loss = -9.9756, pg_loss = 313.43, baseline_loss = 193.13, learner_queue_size = 32, _tick = 57, _time = 1.7372e+09)
[2025-01-17 21:42:16,834][root][INFO] - Step 151040 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 556.0, step = 151040, mean_episode_return = 0.17549, mean_episode_step = 60.878, total_loss = 60.967, entropy_loss = -9.9325, pg_loss = -73.278, baseline_loss = 144.18, learner_queue_size = 32, _tick = 58, _time = 1.7372e+09)
[2025-01-17 21:42:21,840][root][INFO] - Step 153600 @ 511.4 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 561.0, step = 153600, mean_episode_return = 0.21628, mean_episode_step = 65.553, total_loss = -1184.7, entropy_loss = -9.9186, pg_loss = -1234.3, baseline_loss = 59.498, learner_queue_size = 32, _tick = 59, _time = 1.7372e+09)
[2025-01-17 21:42:26,845][root][INFO] - Step 153600 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 566.0, step = 153600, mean_episode_return = 0.21628, mean_episode_step = 65.553, total_loss = -1184.7, entropy_loss = -9.9186, pg_loss = -1234.3, baseline_loss = 59.498, learner_queue_size = 32, _tick = 59, _time = 1.7372e+09)
[2025-01-17 21:42:31,851][root][INFO] - Step 156160 @ 511.4 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 571.0, step = 156160, mean_episode_return = 0.29593, mean_episode_step = 74.529, total_loss = 334.9, entropy_loss = -9.8968, pg_loss = 195.47, baseline_loss = 149.33, learner_queue_size = 32, _tick = 60, _time = 1.7372e+09)
[2025-01-17 21:42:36,857][root][INFO] - Step 156160 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 576.0, step = 156160, mean_episode_return = 0.29593, mean_episode_step = 74.529, total_loss = 334.9, entropy_loss = -9.8968, pg_loss = 195.47, baseline_loss = 149.33, learner_queue_size = 32, _tick = 60, _time = 1.7372e+09)
[2025-01-17 21:42:41,862][root][INFO] - Step 158720 @ 511.4 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 581.0, step = 158720, mean_episode_return = 0.12651, mean_episode_step = 59.787, total_loss = -574.57, entropy_loss = -9.9016, pg_loss = -659.03, baseline_loss = 94.364, learner_queue_size = 32, _tick = 61, _time = 1.7372e+09)
[2025-01-17 21:42:46,867][root][INFO] - Step 158720 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 586.0, step = 158720, mean_episode_return = 0.12651, mean_episode_step = 59.787, total_loss = -574.57, entropy_loss = -9.9016, pg_loss = -659.03, baseline_loss = 94.364, learner_queue_size = 32, _tick = 61, _time = 1.7372e+09)
[2025-01-17 21:42:51,875][root][INFO] - Step 161280 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 591.0, step = 161280, mean_episode_return = 0.21838, mean_episode_step = 65.438, total_loss = 640.47, entropy_loss = -9.855, pg_loss = 484.19, baseline_loss = 166.14, learner_queue_size = 32, _tick = 62, _time = 1.7372e+09)
[2025-01-17 21:42:56,880][root][INFO] - Step 161280 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 596.0, step = 161280, mean_episode_return = 0.21838, mean_episode_step = 65.438, total_loss = 640.47, entropy_loss = -9.855, pg_loss = 484.19, baseline_loss = 166.14, learner_queue_size = 32, _tick = 62, _time = 1.7372e+09)
[2025-01-17 21:43:01,886][root][INFO] - Saving checkpoint to /opt/minihack/checkpoint.tar
[2025-01-17 21:43:01,925][root][INFO] - Step 163840 @ 511.4 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (train_seconds = 601.0, step = 163840, mean_episode_return = 0.19873, mean_episode_step = 64.03, total_loss = 560.78, entropy_loss = -9.838, pg_loss = 422.31, baseline_loss = 148.31, learner_queue_size = 32, _tick = 63, _time = 1.7372e+09)
[2025-01-17 21:43:06,930][root][INFO] - Step 166400 @ 507.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 606.1, step = 166400, mean_episode_return = 0.15996, mean_episode_step = 79.725, total_loss = -556.66, entropy_loss = -9.7885, pg_loss = -658.8, baseline_loss = 111.92, learner_queue_size = 32, _tick = 64, _time = 1.7372e+09)
[2025-01-17 21:43:11,935][root][INFO] - Step 166400 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 611.1, step = 166400, mean_episode_return = 0.15996, mean_episode_step = 79.725, total_loss = -556.66, entropy_loss = -9.7885, pg_loss = -658.8, baseline_loss = 111.92, learner_queue_size = 32, _tick = 64, _time = 1.7372e+09)
[2025-01-17 21:43:16,940][root][INFO] - Step 168960 @ 511.5 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (train_seconds = 616.1, step = 168960, mean_episode_return = 0.27132, mean_episode_step = 66.025, total_loss = 2.868, entropy_loss = -9.8266, pg_loss = -148.67, baseline_loss = 161.37, learner_queue_size = 32, _tick = 65, _time = 1.7372e+09)
[2025-01-17 21:43:21,946][root][INFO] - Step 168960 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 621.1, step = 168960, mean_episode_return = 0.27132, mean_episode_step = 66.025, total_loss = 2.868, entropy_loss = -9.8266, pg_loss = -148.67, baseline_loss = 161.37, learner_queue_size = 32, _tick = 65, _time = 1.7372e+09)
[2025-01-17 21:43:26,951][root][INFO] - Step 171520 @ 511.5 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 626.1, step = 171520, mean_episode_return = 0.37253, mean_episode_step = 62.696, total_loss = 450.94, entropy_loss = -9.7928, pg_loss = 284.13, baseline_loss = 176.61, learner_queue_size = 32, _tick = 66, _time = 1.7372e+09)
[2025-01-17 21:43:31,956][root][INFO] - Step 171520 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 631.1, step = 171520, mean_episode_return = 0.37253, mean_episode_step = 62.696, total_loss = 450.94, entropy_loss = -9.7928, pg_loss = 284.13, baseline_loss = 176.61, learner_queue_size = 32, _tick = 66, _time = 1.7372e+09)
[2025-01-17 21:43:36,961][root][INFO] - Step 174080 @ 511.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 636.1, step = 174080, mean_episode_return = 0.18186, mean_episode_step = 68.82, total_loss = -753.57, entropy_loss = -9.7487, pg_loss = -814.24, baseline_loss = 70.413, learner_queue_size = 32, _tick = 67, _time = 1.7372e+09)
[2025-01-17 21:43:41,968][root][INFO] - Step 174080 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 641.1, step = 174080, mean_episode_return = 0.18186, mean_episode_step = 68.82, total_loss = -753.57, entropy_loss = -9.7487, pg_loss = -814.24, baseline_loss = 70.413, learner_queue_size = 32, _tick = 67, _time = 1.7372e+09)
[2025-01-17 21:43:46,973][root][INFO] - Step 176640 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 646.1, step = 176640, mean_episode_return = 0.29905, mean_episode_step = 67.117, total_loss = -268.62, entropy_loss = -9.7395, pg_loss = -396.92, baseline_loss = 138.03, learner_queue_size = 32, _tick = 68, _time = 1.7372e+09)
[2025-01-17 21:43:51,979][root][INFO] - Step 176640 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 651.1, step = 176640, mean_episode_return = 0.29905, mean_episode_step = 67.117, total_loss = -268.62, entropy_loss = -9.7395, pg_loss = -396.92, baseline_loss = 138.03, learner_queue_size = 32, _tick = 68, _time = 1.7372e+09)
[2025-01-17 21:43:56,984][root][INFO] - Step 179200 @ 511.4 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (train_seconds = 656.1, step = 179200, mean_episode_return = 0.23241, mean_episode_step = 75.063, total_loss = -538.21, entropy_loss = -9.7184, pg_loss = -594.11, baseline_loss = 65.616, learner_queue_size = 32, _tick = 69, _time = 1.7372e+09)
[2025-01-17 21:44:01,990][root][INFO] - Step 181760 @ 511.4 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (train_seconds = 661.1, step = 181760, mean_episode_return = 0.12398, mean_episode_step = 57.026, total_loss = 497.12, entropy_loss = -9.7072, pg_loss = 330.56, baseline_loss = 176.27, learner_queue_size = 32, _tick = 70, _time = 1.7372e+09)
[2025-01-17 21:44:06,997][root][INFO] - Step 181760 @ 0.0 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 666.2, step = 181760, mean_episode_return = 0.12398, mean_episode_step = 57.026, total_loss = 497.12, entropy_loss = -9.7072, pg_loss = 330.56, baseline_loss = 176.27, learner_queue_size = 32, _tick = 70, _time = 1.7372e+09)
[2025-01-17 21:44:12,005][root][INFO] - Step 184320 @ 511.2 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 671.2, step = 184320, mean_episode_return = 0.17303, mean_episode_step = 64.854, total_loss = -75.265, entropy_loss = -9.7121, pg_loss = -247.73, baseline_loss = 182.17, learner_queue_size = 32, _tick = 71, _time = 1.7372e+09)
[2025-01-17 21:44:17,011][root][INFO] - Step 184320 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 676.2, step = 184320, mean_episode_return = 0.17303, mean_episode_step = 64.854, total_loss = -75.265, entropy_loss = -9.7121, pg_loss = -247.73, baseline_loss = 182.17, learner_queue_size = 32, _tick = 71, _time = 1.7372e+09)
[2025-01-17 21:44:22,016][root][INFO] - Step 186880 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 681.2, step = 186880, mean_episode_return = 0.158, mean_episode_step = 60.552, total_loss = -271.35, entropy_loss = -9.6794, pg_loss = -343.72, baseline_loss = 82.055, learner_queue_size = 32, _tick = 72, _time = 1.7372e+09)
[2025-01-17 21:44:27,021][root][INFO] - Step 186880 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 686.2, step = 186880, mean_episode_return = 0.158, mean_episode_step = 60.552, total_loss = -271.35, entropy_loss = -9.6794, pg_loss = -343.72, baseline_loss = 82.055, learner_queue_size = 32, _tick = 72, _time = 1.7372e+09)
[2025-01-17 21:44:32,026][root][INFO] - Step 189440 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 691.2, step = 189440, mean_episode_return = 0.17223, mean_episode_step = 64.341, total_loss = -85.724, entropy_loss = -9.6597, pg_loss = -222.3, baseline_loss = 146.24, learner_queue_size = 32, _tick = 73, _time = 1.7372e+09)
[2025-01-17 21:44:37,032][root][INFO] - Step 189440 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 696.2, step = 189440, mean_episode_return = 0.17223, mean_episode_step = 64.341, total_loss = -85.724, entropy_loss = -9.6597, pg_loss = -222.3, baseline_loss = 146.24, learner_queue_size = 32, _tick = 73, _time = 1.7372e+09)
[2025-01-17 21:44:42,037][root][INFO] - Step 192000 @ 511.4 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 701.2, step = 192000, mean_episode_return = 0.29236, mean_episode_step = 67.479, total_loss = 191.2, entropy_loss = -9.5653, pg_loss = 108.61, baseline_loss = 92.162, learner_queue_size = 32, _tick = 74, _time = 1.7372e+09)
[2025-01-17 21:44:47,043][root][INFO] - Step 192000 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 706.2, step = 192000, mean_episode_return = 0.29236, mean_episode_step = 67.479, total_loss = 191.2, entropy_loss = -9.5653, pg_loss = 108.61, baseline_loss = 92.162, learner_queue_size = 32, _tick = 74, _time = 1.7372e+09)
[2025-01-17 21:44:52,060][root][INFO] - Step 194560 @ 510.4 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 711.2, step = 194560, mean_episode_return = 0.12263, mean_episode_step = 71.962, total_loss = -122.91, entropy_loss = -9.5159, pg_loss = -210.21, baseline_loss = 96.822, learner_queue_size = 32, _tick = 75, _time = 1.7372e+09)
[2025-01-17 21:44:57,065][root][INFO] - Step 197120 @ 511.4 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 716.2, step = 197120, mean_episode_return = 0.24032, mean_episode_step = 62.903, total_loss = -189.37, entropy_loss = -9.54, pg_loss = -247.96, baseline_loss = 68.13, learner_queue_size = 32, _tick = 76, _time = 1.7372e+09)
[2025-01-17 21:45:02,071][root][INFO] - Step 197120 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 721.2, step = 197120, mean_episode_return = 0.24032, mean_episode_step = 62.903, total_loss = -189.37, entropy_loss = -9.54, pg_loss = -247.96, baseline_loss = 68.13, learner_queue_size = 32, _tick = 76, _time = 1.7372e+09)
[2025-01-17 21:45:07,077][root][INFO] - Step 199680 @ 511.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 726.2, step = 199680, mean_episode_return = 0.15379, mean_episode_step = 66.766, total_loss = 692.47, entropy_loss = -9.489, pg_loss = 542.61, baseline_loss = 159.34, learner_queue_size = 32, _tick = 77, _time = 1.7372e+09)
[2025-01-17 21:45:12,082][root][INFO] - Step 199680 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 731.2, step = 199680, mean_episode_return = 0.15379, mean_episode_step = 66.766, total_loss = 692.47, entropy_loss = -9.489, pg_loss = 542.61, baseline_loss = 159.34, learner_queue_size = 32, _tick = 77, _time = 1.7372e+09)
[2025-01-17 21:45:17,088][root][INFO] - Step 202240 @ 511.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 736.2, step = 202240, mean_episode_return = 0.22787, mean_episode_step = 66.84, total_loss = -187.5, entropy_loss = -9.5003, pg_loss = -274.25, baseline_loss = 96.247, learner_queue_size = 32, _tick = 78, _time = 1.7372e+09)
[2025-01-17 21:45:22,093][root][INFO] - Step 202240 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 741.2, step = 202240, mean_episode_return = 0.22787, mean_episode_step = 66.84, total_loss = -187.5, entropy_loss = -9.5003, pg_loss = -274.25, baseline_loss = 96.247, learner_queue_size = 32, _tick = 78, _time = 1.7372e+09)
[2025-01-17 21:45:27,098][root][INFO] - Step 204800 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 746.3, step = 204800, mean_episode_return = 0.12282, mean_episode_step = 67.129, total_loss = 117.83, entropy_loss = -9.481, pg_loss = 28.877, baseline_loss = 98.439, learner_queue_size = 32, _tick = 79, _time = 1.7372e+09)
[2025-01-17 21:45:32,103][root][INFO] - Step 204800 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 751.3, step = 204800, mean_episode_return = 0.12282, mean_episode_step = 67.129, total_loss = 117.83, entropy_loss = -9.481, pg_loss = 28.877, baseline_loss = 98.439, learner_queue_size = 32, _tick = 79, _time = 1.7372e+09)
[2025-01-17 21:45:37,116][root][INFO] - Step 207360 @ 510.7 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 756.3, step = 207360, mean_episode_return = 0.21141, mean_episode_step = 66.953, total_loss = 305.38, entropy_loss = -9.5471, pg_loss = 142.87, baseline_loss = 172.06, learner_queue_size = 32, _tick = 80, _time = 1.7372e+09)
[2025-01-17 21:45:42,124][root][INFO] - Step 209920 @ 511.2 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 761.3, step = 209920, mean_episode_return = 0.15529, mean_episode_step = 66.866, total_loss = -41.212, entropy_loss = -9.4618, pg_loss = -135.42, baseline_loss = 103.67, learner_queue_size = 32, _tick = 81, _time = 1.7372e+09)
[2025-01-17 21:45:47,130][root][INFO] - Step 209920 @ 0.0 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (train_seconds = 766.3, step = 209920, mean_episode_return = 0.15529, mean_episode_step = 66.866, total_loss = -41.212, entropy_loss = -9.4618, pg_loss = -135.42, baseline_loss = 103.67, learner_queue_size = 32, _tick = 81, _time = 1.7372e+09)
[2025-01-17 21:45:52,136][root][INFO] - Step 212480 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 771.3, step = 212480, mean_episode_return = 0.1943, mean_episode_step = 69.029, total_loss = -494.37, entropy_loss = -9.4656, pg_loss = -579.99, baseline_loss = 95.087, learner_queue_size = 32, _tick = 82, _time = 1.7372e+09)
[2025-01-17 21:45:57,141][root][INFO] - Step 212480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 776.3, step = 212480, mean_episode_return = 0.1943, mean_episode_step = 69.029, total_loss = -494.37, entropy_loss = -9.4656, pg_loss = -579.99, baseline_loss = 95.087, learner_queue_size = 32, _tick = 82, _time = 1.7372e+09)
[2025-01-17 21:46:02,146][root][INFO] - Step 215040 @ 511.5 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 781.3, step = 215040, mean_episode_return = 0.32673, mean_episode_step = 78.572, total_loss = -211.7, entropy_loss = -9.4179, pg_loss = -294.95, baseline_loss = 92.662, learner_queue_size = 32, _tick = 83, _time = 1.7372e+09)
[2025-01-17 21:46:07,151][root][INFO] - Step 215040 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 786.3, step = 215040, mean_episode_return = 0.32673, mean_episode_step = 78.572, total_loss = -211.7, entropy_loss = -9.4179, pg_loss = -294.95, baseline_loss = 92.662, learner_queue_size = 32, _tick = 83, _time = 1.7372e+09)
[2025-01-17 21:46:12,157][root][INFO] - Step 217600 @ 511.5 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (train_seconds = 791.3, step = 217600, mean_episode_return = 0.22306, mean_episode_step = 64.464, total_loss = -508.24, entropy_loss = -9.3971, pg_loss = -572.69, baseline_loss = 73.848, learner_queue_size = 32, _tick = 84, _time = 1.7372e+09)
[2025-01-17 21:46:17,162][root][INFO] - Step 217600 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 796.3, step = 217600, mean_episode_return = 0.22306, mean_episode_step = 64.464, total_loss = -508.24, entropy_loss = -9.3971, pg_loss = -572.69, baseline_loss = 73.848, learner_queue_size = 32, _tick = 84, _time = 1.7372e+09)
[2025-01-17 21:46:22,169][root][INFO] - Step 220160 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 801.3, step = 220160, mean_episode_return = 0.21461, mean_episode_step = 58.534, total_loss = 463.05, entropy_loss = -9.3916, pg_loss = 323.37, baseline_loss = 149.07, learner_queue_size = 32, _tick = 85, _time = 1.7372e+09)
[2025-01-17 21:46:27,176][root][INFO] - Step 222720 @ 511.1 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (train_seconds = 806.3, step = 222720, mean_episode_return = 0.16902, mean_episode_step = 65.602, total_loss = -616.81, entropy_loss = -9.3676, pg_loss = -694.47, baseline_loss = 87.023, learner_queue_size = 32, _tick = 86, _time = 1.7372e+09)
[2025-01-17 21:46:32,190][root][INFO] - Step 222720 @ 0.0 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 811.3, step = 222720, mean_episode_return = 0.16902, mean_episode_step = 65.602, total_loss = -616.81, entropy_loss = -9.3676, pg_loss = -694.47, baseline_loss = 87.023, learner_queue_size = 32, _tick = 86, _time = 1.7372e+09)
[2025-01-17 21:46:37,196][root][INFO] - Step 225280 @ 511.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 816.4, step = 225280, mean_episode_return = 0.088433, mean_episode_step = 61.668, total_loss = 66.776, entropy_loss = -9.2944, pg_loss = -22.382, baseline_loss = 98.453, learner_queue_size = 32, _tick = 87, _time = 1.7372e+09)
[2025-01-17 21:46:42,203][root][INFO] - Step 225280 @ 0.0 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 821.4, step = 225280, mean_episode_return = 0.088433, mean_episode_step = 61.668, total_loss = 66.776, entropy_loss = -9.2944, pg_loss = -22.382, baseline_loss = 98.453, learner_queue_size = 32, _tick = 87, _time = 1.7372e+09)
[2025-01-17 21:46:47,208][root][INFO] - Step 227840 @ 511.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 826.4, step = 227840, mean_episode_return = 0.33206, mean_episode_step = 69.492, total_loss = -198.43, entropy_loss = -9.366, pg_loss = -284.26, baseline_loss = 95.201, learner_queue_size = 32, _tick = 88, _time = 1.7372e+09)
[2025-01-17 21:46:52,213][root][INFO] - Step 227840 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 831.4, step = 227840, mean_episode_return = 0.33206, mean_episode_step = 69.492, total_loss = -198.43, entropy_loss = -9.366, pg_loss = -284.26, baseline_loss = 95.201, learner_queue_size = 32, _tick = 88, _time = 1.7372e+09)
[2025-01-17 21:46:57,220][root][INFO] - Step 230400 @ 511.3 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 836.4, step = 230400, mean_episode_return = 0.23776, mean_episode_step = 85.146, total_loss = 446.03, entropy_loss = -9.3299, pg_loss = 362.65, baseline_loss = 92.705, learner_queue_size = 32, _tick = 89, _time = 1.7372e+09)
[2025-01-17 21:47:02,225][root][INFO] - Step 230400 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 841.4, step = 230400, mean_episode_return = 0.23776, mean_episode_step = 85.146, total_loss = 446.03, entropy_loss = -9.3299, pg_loss = 362.65, baseline_loss = 92.705, learner_queue_size = 32, _tick = 89, _time = 1.7372e+09)
[2025-01-17 21:47:07,230][root][INFO] - Step 232960 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 846.4, step = 232960, mean_episode_return = 0.21582, mean_episode_step = 64.614, total_loss = -142.58, entropy_loss = -9.3792, pg_loss = -267.83, baseline_loss = 134.64, learner_queue_size = 32, _tick = 90, _time = 1.7372e+09)
[2025-01-17 21:47:12,237][root][INFO] - Step 232960 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 851.4, step = 232960, mean_episode_return = 0.21582, mean_episode_step = 64.614, total_loss = -142.58, entropy_loss = -9.3792, pg_loss = -267.83, baseline_loss = 134.64, learner_queue_size = 32, _tick = 90, _time = 1.7372e+09)
[2025-01-17 21:47:17,242][root][INFO] - Step 235520 @ 511.5 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (train_seconds = 856.4, step = 235520, mean_episode_return = 0.33083, mean_episode_step = 73.334, total_loss = 5.4482, entropy_loss = -9.3198, pg_loss = -86.173, baseline_loss = 100.94, learner_queue_size = 32, _tick = 91, _time = 1.7372e+09)
[2025-01-17 21:47:22,249][root][INFO] - Step 238080 @ 511.3 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 861.4, step = 238080, mean_episode_return = 0.11416, mean_episode_step = 65.449, total_loss = -126.67, entropy_loss = -9.2531, pg_loss = -207.23, baseline_loss = 89.814, learner_queue_size = 32, _tick = 92, _time = 1.7372e+09)
[2025-01-17 21:47:27,254][root][INFO] - Step 238080 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 866.4, step = 238080, mean_episode_return = 0.11416, mean_episode_step = 65.449, total_loss = -126.67, entropy_loss = -9.2531, pg_loss = -207.23, baseline_loss = 89.814, learner_queue_size = 32, _tick = 92, _time = 1.7372e+09)
[2025-01-17 21:47:32,260][root][INFO] - Step 240640 @ 511.4 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 871.4, step = 240640, mean_episode_return = 0.10761, mean_episode_step = 61.617, total_loss = -106.46, entropy_loss = -9.2271, pg_loss = -154.95, baseline_loss = 57.718, learner_queue_size = 32, _tick = 93, _time = 1.7372e+09)
[2025-01-17 21:47:37,265][root][INFO] - Step 240640 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 876.4, step = 240640, mean_episode_return = 0.10761, mean_episode_step = 61.617, total_loss = -106.46, entropy_loss = -9.2271, pg_loss = -154.95, baseline_loss = 57.718, learner_queue_size = 32, _tick = 93, _time = 1.7372e+09)
[2025-01-17 21:47:42,271][root][INFO] - Step 243200 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 881.4, step = 243200, mean_episode_return = 0.33452, mean_episode_step = 68.032, total_loss = 400.25, entropy_loss = -9.2657, pg_loss = 310.63, baseline_loss = 98.882, learner_queue_size = 32, _tick = 94, _time = 1.7372e+09)
[2025-01-17 21:47:47,276][root][INFO] - Step 243200 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 886.4, step = 243200, mean_episode_return = 0.33452, mean_episode_step = 68.032, total_loss = 400.25, entropy_loss = -9.2657, pg_loss = 310.63, baseline_loss = 98.882, learner_queue_size = 32, _tick = 94, _time = 1.7372e+09)
[2025-01-17 21:47:52,282][root][INFO] - Step 245760 @ 511.4 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 891.4, step = 245760, mean_episode_return = 0.26982, mean_episode_step = 69.379, total_loss = 208.84, entropy_loss = -9.2557, pg_loss = 119.22, baseline_loss = 98.877, learner_queue_size = 32, _tick = 95, _time = 1.7372e+09)
[2025-01-17 21:47:57,287][root][INFO] - Step 245760 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 896.4, step = 245760, mean_episode_return = 0.26982, mean_episode_step = 69.379, total_loss = 208.84, entropy_loss = -9.2557, pg_loss = 119.22, baseline_loss = 98.877, learner_queue_size = 32, _tick = 95, _time = 1.7372e+09)
[2025-01-17 21:48:02,293][root][INFO] - Step 248320 @ 511.4 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (train_seconds = 901.4, step = 248320, mean_episode_return = 0.10239, mean_episode_step = 71.182, total_loss = 651.26, entropy_loss = -9.2, pg_loss = 522.44, baseline_loss = 138.01, learner_queue_size = 32, _tick = 96, _time = 1.7372e+09)
[2025-01-17 21:48:07,305][root][INFO] - Saving checkpoint to /opt/minihack/checkpoint_0.5.tar
[2025-01-17 21:48:07,438][root][INFO] - Step 250880 @ 511.1 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (train_seconds = 906.5, step = 250880, mean_episode_return = 0.17389, mean_episode_step = 60.261, total_loss = 42.264, entropy_loss = -9.221, pg_loss = -75.8, baseline_loss = 127.29, learner_queue_size = 32, _tick = 97, _time = 1.7372e+09)
[2025-01-17 21:48:12,444][root][INFO] - Step 250880 @ 0.0 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 911.6, step = 250880, mean_episode_return = 0.17389, mean_episode_step = 60.261, total_loss = 42.264, entropy_loss = -9.221, pg_loss = -75.8, baseline_loss = 127.29, learner_queue_size = 32, _tick = 97, _time = 1.7372e+09)
[2025-01-17 21:48:17,451][root][INFO] - Step 253440 @ 511.3 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 916.6, step = 253440, mean_episode_return = 0.27294, mean_episode_step = 76.388, total_loss = -63.868, entropy_loss = -9.1977, pg_loss = -175.56, baseline_loss = 120.89, learner_queue_size = 32, _tick = 98, _time = 1.7372e+09)
[2025-01-17 21:48:22,456][root][INFO] - Step 253440 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 921.6, step = 253440, mean_episode_return = 0.27294, mean_episode_step = 76.388, total_loss = -63.868, entropy_loss = -9.1977, pg_loss = -175.56, baseline_loss = 120.89, learner_queue_size = 32, _tick = 98, _time = 1.7372e+09)
[2025-01-17 21:48:27,462][root][INFO] - Step 256000 @ 511.4 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 926.6, step = 256000, mean_episode_return = 0.21746, mean_episode_step = 67.633, total_loss = -105.49, entropy_loss = -9.1522, pg_loss = -189.42, baseline_loss = 93.079, learner_queue_size = 32, _tick = 99, _time = 1.7372e+09)
[2025-01-17 21:48:32,469][root][INFO] - Step 256000 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 931.6, step = 256000, mean_episode_return = 0.21746, mean_episode_step = 67.633, total_loss = -105.49, entropy_loss = -9.1522, pg_loss = -189.42, baseline_loss = 93.079, learner_queue_size = 32, _tick = 99, _time = 1.7372e+09)
[2025-01-17 21:48:37,481][root][INFO] - Step 258560 @ 510.7 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 936.6, step = 258560, mean_episode_return = 0.256, mean_episode_step = 77.975, total_loss = 49.77, entropy_loss = -9.0387, pg_loss = -26.297, baseline_loss = 85.106, learner_queue_size = 32, _tick = 100, _time = 1.7372e+09)
[2025-01-17 21:48:42,486][root][INFO] - Step 258560 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 941.6, step = 258560, mean_episode_return = 0.256, mean_episode_step = 77.975, total_loss = 49.77, entropy_loss = -9.0387, pg_loss = -26.297, baseline_loss = 85.106, learner_queue_size = 32, _tick = 100, _time = 1.7372e+09)
[2025-01-17 21:48:47,492][root][INFO] - Step 261120 @ 511.4 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 946.6, step = 261120, mean_episode_return = 0.24581, mean_episode_step = 70.089, total_loss = -50.42, entropy_loss = -9.0542, pg_loss = -158.28, baseline_loss = 116.92, learner_queue_size = 32, _tick = 101, _time = 1.7372e+09)
[2025-01-17 21:48:52,497][root][INFO] - Step 261120 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 951.7, step = 261120, mean_episode_return = 0.24581, mean_episode_step = 70.089, total_loss = -50.42, entropy_loss = -9.0542, pg_loss = -158.28, baseline_loss = 116.92, learner_queue_size = 32, _tick = 101, _time = 1.7372e+09)
[2025-01-17 21:48:57,502][root][INFO] - Step 263680 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 956.7, step = 263680, mean_episode_return = 0.27203, mean_episode_step = 69.151, total_loss = 232.14, entropy_loss = -8.9867, pg_loss = 151.96, baseline_loss = 89.163, learner_queue_size = 32, _tick = 102, _time = 1.7372e+09)
[2025-01-17 21:49:02,508][root][INFO] - Step 263680 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 961.7, step = 263680, mean_episode_return = 0.27203, mean_episode_step = 69.151, total_loss = 232.14, entropy_loss = -8.9867, pg_loss = 151.96, baseline_loss = 89.163, learner_queue_size = 32, _tick = 102, _time = 1.7372e+09)
[2025-01-17 21:49:07,513][root][INFO] - Step 266240 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 966.7, step = 266240, mean_episode_return = 0.42381, mean_episode_step = 78.329, total_loss = 253.84, entropy_loss = -9.0013, pg_loss = 169.71, baseline_loss = 93.123, learner_queue_size = 32, _tick = 103, _time = 1.7372e+09)
[2025-01-17 21:49:12,518][root][INFO] - Step 268800 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 971.7, step = 268800, mean_episode_return = 0.25578, mean_episode_step = 66.282, total_loss = 207.73, entropy_loss = -9.1019, pg_loss = 81.505, baseline_loss = 135.33, learner_queue_size = 32, _tick = 104, _time = 1.7372e+09)
[2025-01-17 21:49:17,523][root][INFO] - Step 268800 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 976.7, step = 268800, mean_episode_return = 0.25578, mean_episode_step = 66.282, total_loss = 207.73, entropy_loss = -9.1019, pg_loss = 81.505, baseline_loss = 135.33, learner_queue_size = 32, _tick = 104, _time = 1.7372e+09)
[2025-01-17 21:49:22,528][root][INFO] - Step 271360 @ 511.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 981.7, step = 271360, mean_episode_return = 0.27858, mean_episode_step = 67.413, total_loss = 1559.6, entropy_loss = -9.0202, pg_loss = 1349.0, baseline_loss = 219.66, learner_queue_size = 32, _tick = 105, _time = 1.7372e+09)
[2025-01-17 21:49:27,534][root][INFO] - Step 271360 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 986.7, step = 271360, mean_episode_return = 0.27858, mean_episode_step = 67.413, total_loss = 1559.6, entropy_loss = -9.0202, pg_loss = 1349.0, baseline_loss = 219.66, learner_queue_size = 32, _tick = 105, _time = 1.7372e+09)
[2025-01-17 21:49:32,539][root][INFO] - Step 273920 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 991.7, step = 273920, mean_episode_return = 0.18979, mean_episode_step = 60.95, total_loss = -409.59, entropy_loss = -9.0634, pg_loss = -493.08, baseline_loss = 92.558, learner_queue_size = 32, _tick = 106, _time = 1.7372e+09)
[2025-01-17 21:49:37,544][root][INFO] - Step 276480 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 996.7, step = 276480, mean_episode_return = 0.24273, mean_episode_step = 65.719, total_loss = 287.63, entropy_loss = -9.0041, pg_loss = 167.52, baseline_loss = 129.12, learner_queue_size = 32, _tick = 107, _time = 1.7372e+09)
[2025-01-17 21:49:42,549][root][INFO] - Step 276480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1001.7, step = 276480, mean_episode_return = 0.24273, mean_episode_step = 65.719, total_loss = 287.63, entropy_loss = -9.0041, pg_loss = 167.52, baseline_loss = 129.12, learner_queue_size = 32, _tick = 107, _time = 1.7372e+09)
[2025-01-17 21:49:47,554][root][INFO] - Step 279040 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1006.7, step = 279040, mean_episode_return = 0.30711, mean_episode_step = 81.875, total_loss = -451.26, entropy_loss = -8.975, pg_loss = -513.09, baseline_loss = 70.808, learner_queue_size = 32, _tick = 108, _time = 1.7372e+09)
[2025-01-17 21:49:52,559][root][INFO] - Step 281600 @ 511.5 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (train_seconds = 1011.7, step = 281600, mean_episode_return = 0.29907, mean_episode_step = 87.592, total_loss = -143.62, entropy_loss = -8.9616, pg_loss = -237.08, baseline_loss = 102.42, learner_queue_size = 32, _tick = 109, _time = 1.7372e+09)
[2025-01-17 21:49:57,564][root][INFO] - Step 281600 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1016.7, step = 281600, mean_episode_return = 0.29907, mean_episode_step = 87.592, total_loss = -143.62, entropy_loss = -8.9616, pg_loss = -237.08, baseline_loss = 102.42, learner_queue_size = 32, _tick = 109, _time = 1.7372e+09)
[2025-01-17 21:50:02,569][root][INFO] - Step 284160 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1021.7, step = 284160, mean_episode_return = 0.39488, mean_episode_step = 82.939, total_loss = -43.688, entropy_loss = -8.9191, pg_loss = -169.9, baseline_loss = 135.13, learner_queue_size = 32, _tick = 110, _time = 1.7372e+09)
[2025-01-17 21:50:07,574][root][INFO] - Step 284160 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1026.7, step = 284160, mean_episode_return = 0.39488, mean_episode_step = 82.939, total_loss = -43.688, entropy_loss = -8.9191, pg_loss = -169.9, baseline_loss = 135.13, learner_queue_size = 32, _tick = 110, _time = 1.7372e+09)
[2025-01-17 21:50:12,579][root][INFO] - Step 286720 @ 511.5 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (train_seconds = 1031.7, step = 286720, mean_episode_return = 0.25527, mean_episode_step = 89.905, total_loss = -22.272, entropy_loss = -8.8989, pg_loss = -80.954, baseline_loss = 67.581, learner_queue_size = 32, _tick = 111, _time = 1.7372e+09)
[2025-01-17 21:50:17,586][root][INFO] - Step 289280 @ 511.3 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1036.7, step = 289280, mean_episode_return = 0.27295, mean_episode_step = 60.279, total_loss = 194.54, entropy_loss = -8.9741, pg_loss = 89.752, baseline_loss = 113.76, learner_queue_size = 32, _tick = 112, _time = 1.7372e+09)
[2025-01-17 21:50:22,592][root][INFO] - Step 289280 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1041.7, step = 289280, mean_episode_return = 0.27295, mean_episode_step = 60.279, total_loss = 194.54, entropy_loss = -8.9741, pg_loss = 89.752, baseline_loss = 113.76, learner_queue_size = 32, _tick = 112, _time = 1.7372e+09)
[2025-01-17 21:50:27,597][root][INFO] - Step 291840 @ 511.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 1046.8, step = 291840, mean_episode_return = 0.14468, mean_episode_step = 79.863, total_loss = -11.269, entropy_loss = -8.9652, pg_loss = -93.986, baseline_loss = 91.682, learner_queue_size = 32, _tick = 113, _time = 1.7372e+09)
[2025-01-17 21:50:32,602][root][INFO] - Step 294400 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1051.8, step = 294400, mean_episode_return = 0.35106, mean_episode_step = 82.15, total_loss = 98.745, entropy_loss = -9.0067, pg_loss = 37.765, baseline_loss = 69.987, learner_queue_size = 32, _tick = 114, _time = 1.7372e+09)
[2025-01-17 21:50:37,608][root][INFO] - Step 294400 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1056.8, step = 294400, mean_episode_return = 0.35106, mean_episode_step = 82.15, total_loss = 98.745, entropy_loss = -9.0067, pg_loss = 37.765, baseline_loss = 69.987, learner_queue_size = 32, _tick = 114, _time = 1.7372e+09)
[2025-01-17 21:50:42,614][root][INFO] - Step 296960 @ 511.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1061.8, step = 296960, mean_episode_return = 0.3255, mean_episode_step = 77.251, total_loss = 447.08, entropy_loss = -9.0147, pg_loss = 342.56, baseline_loss = 113.54, learner_queue_size = 32, _tick = 115, _time = 1.7372e+09)
[2025-01-17 21:50:47,619][root][INFO] - Step 296960 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1066.8, step = 296960, mean_episode_return = 0.3255, mean_episode_step = 77.251, total_loss = 447.08, entropy_loss = -9.0147, pg_loss = 342.56, baseline_loss = 113.54, learner_queue_size = 32, _tick = 115, _time = 1.7372e+09)
[2025-01-17 21:50:52,625][root][INFO] - Step 299520 @ 511.4 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 1071.8, step = 299520, mean_episode_return = 0.30633, mean_episode_step = 70.103, total_loss = 129.5, entropy_loss = -9.0126, pg_loss = 47.463, baseline_loss = 91.049, learner_queue_size = 32, _tick = 116, _time = 1.7372e+09)
[2025-01-17 21:50:57,631][root][INFO] - Step 302080 @ 511.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1076.8, step = 302080, mean_episode_return = 0.27567, mean_episode_step = 68.483, total_loss = -242.21, entropy_loss = -9.0243, pg_loss = -339.14, baseline_loss = 105.96, learner_queue_size = 32, _tick = 117, _time = 1.7372e+09)
[2025-01-17 21:51:02,637][root][INFO] - Step 302080 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1081.8, step = 302080, mean_episode_return = 0.27567, mean_episode_step = 68.483, total_loss = -242.21, entropy_loss = -9.0243, pg_loss = -339.14, baseline_loss = 105.96, learner_queue_size = 32, _tick = 117, _time = 1.7372e+09)
[2025-01-17 21:51:07,642][root][INFO] - Step 304640 @ 511.5 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (train_seconds = 1086.8, step = 304640, mean_episode_return = 0.52722, mean_episode_step = 79.59, total_loss = 57.109, entropy_loss = -8.9595, pg_loss = -20.045, baseline_loss = 86.113, learner_queue_size = 32, _tick = 118, _time = 1.7372e+09)
[2025-01-17 21:51:12,647][root][INFO] - Step 304640 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1091.8, step = 304640, mean_episode_return = 0.52722, mean_episode_step = 79.59, total_loss = 57.109, entropy_loss = -8.9595, pg_loss = -20.045, baseline_loss = 86.113, learner_queue_size = 32, _tick = 118, _time = 1.7372e+09)
[2025-01-17 21:51:17,652][root][INFO] - Step 307200 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 1096.8, step = 307200, mean_episode_return = 0.32542, mean_episode_step = 71.077, total_loss = 742.36, entropy_loss = -8.9806, pg_loss = 593.09, baseline_loss = 158.25, learner_queue_size = 32, _tick = 119, _time = 1.7372e+09)
[2025-01-17 21:51:22,657][root][INFO] - Step 309760 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1101.8, step = 309760, mean_episode_return = 0.23823, mean_episode_step = 77.76, total_loss = -613.3, entropy_loss = -8.9617, pg_loss = -673.16, baseline_loss = 68.816, learner_queue_size = 32, _tick = 120, _time = 1.7372e+09)
[2025-01-17 21:51:27,662][root][INFO] - Step 309760 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1106.8, step = 309760, mean_episode_return = 0.23823, mean_episode_step = 77.76, total_loss = -613.3, entropy_loss = -8.9617, pg_loss = -673.16, baseline_loss = 68.816, learner_queue_size = 32, _tick = 120, _time = 1.7372e+09)
[2025-01-17 21:51:32,667][root][INFO] - Step 312320 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1111.8, step = 312320, mean_episode_return = 0.45307, mean_episode_step = 79.534, total_loss = 1042.5, entropy_loss = -8.9557, pg_loss = 847.85, baseline_loss = 203.6, learner_queue_size = 32, _tick = 121, _time = 1.7372e+09)
[2025-01-17 21:51:37,673][root][INFO] - Step 314880 @ 511.4 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (train_seconds = 1116.8, step = 314880, mean_episode_return = 0.32937, mean_episode_step = 77.518, total_loss = 84.327, entropy_loss = -8.985, pg_loss = -35.946, baseline_loss = 129.26, learner_queue_size = 32, _tick = 122, _time = 1.7372e+09)
[2025-01-17 21:51:42,678][root][INFO] - Step 314880 @ 0.0 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 1121.8, step = 314880, mean_episode_return = 0.32937, mean_episode_step = 77.518, total_loss = 84.327, entropy_loss = -8.985, pg_loss = -35.946, baseline_loss = 129.26, learner_queue_size = 32, _tick = 122, _time = 1.7372e+09)
[2025-01-17 21:51:47,684][root][INFO] - Step 317440 @ 511.5 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 1126.8, step = 317440, mean_episode_return = 0.31311, mean_episode_step = 73.344, total_loss = 528.89, entropy_loss = -8.9255, pg_loss = 332.77, baseline_loss = 205.05, learner_queue_size = 32, _tick = 123, _time = 1.7372e+09)
[2025-01-17 21:51:52,689][root][INFO] - Step 317440 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1131.8, step = 317440, mean_episode_return = 0.31311, mean_episode_step = 73.344, total_loss = 528.89, entropy_loss = -8.9255, pg_loss = 332.77, baseline_loss = 205.05, learner_queue_size = 32, _tick = 123, _time = 1.7372e+09)
[2025-01-17 21:51:57,696][root][INFO] - Step 320000 @ 511.3 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (train_seconds = 1136.9, step = 320000, mean_episode_return = 0.48464, mean_episode_step = 76.982, total_loss = -241.07, entropy_loss = -8.9107, pg_loss = -348.03, baseline_loss = 115.86, learner_queue_size = 32, _tick = 124, _time = 1.7372e+09)
[2025-01-17 21:52:02,701][root][INFO] - Step 320000 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1141.9, step = 320000, mean_episode_return = 0.48464, mean_episode_step = 76.982, total_loss = -241.07, entropy_loss = -8.9107, pg_loss = -348.03, baseline_loss = 115.86, learner_queue_size = 32, _tick = 124, _time = 1.7372e+09)
[2025-01-17 21:52:07,706][root][INFO] - Step 322560 @ 511.5 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (train_seconds = 1146.9, step = 322560, mean_episode_return = 0.357, mean_episode_step = 79.031, total_loss = 251.98, entropy_loss = -8.877, pg_loss = 135.25, baseline_loss = 125.6, learner_queue_size = 32, _tick = 125, _time = 1.7372e+09)
[2025-01-17 21:52:12,711][root][INFO] - Step 325120 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1151.9, step = 325120, mean_episode_return = 0.19044, mean_episode_step = 59.55, total_loss = 237.84, entropy_loss = -8.8753, pg_loss = 120.37, baseline_loss = 126.34, learner_queue_size = 32, _tick = 126, _time = 1.7372e+09)
[2025-01-17 21:52:17,716][root][INFO] - Step 325120 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1156.9, step = 325120, mean_episode_return = 0.19044, mean_episode_step = 59.55, total_loss = 237.84, entropy_loss = -8.8753, pg_loss = 120.37, baseline_loss = 126.34, learner_queue_size = 32, _tick = 126, _time = 1.7372e+09)
[2025-01-17 21:52:22,721][root][INFO] - Step 327680 @ 511.4 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 1161.9, step = 327680, mean_episode_return = 0.12143, mean_episode_step = 72.944, total_loss = -72.877, entropy_loss = -8.8637, pg_loss = -201.44, baseline_loss = 137.42, learner_queue_size = 32, _tick = 127, _time = 1.7372e+09)
[2025-01-17 21:52:27,726][root][INFO] - Step 327680 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1166.9, step = 327680, mean_episode_return = 0.12143, mean_episode_step = 72.944, total_loss = -72.877, entropy_loss = -8.8637, pg_loss = -201.44, baseline_loss = 137.42, learner_queue_size = 32, _tick = 127, _time = 1.7372e+09)
[2025-01-17 21:52:32,731][root][INFO] - Step 330240 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1171.9, step = 330240, mean_episode_return = 0.19919, mean_episode_step = 74.232, total_loss = -649.66, entropy_loss = -8.8579, pg_loss = -710.38, baseline_loss = 69.578, learner_queue_size = 32, _tick = 128, _time = 1.7372e+09)
[2025-01-17 21:52:37,736][root][INFO] - Step 332800 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 1176.9, step = 332800, mean_episode_return = 0.29445, mean_episode_step = 83.042, total_loss = 563.42, entropy_loss = -8.7822, pg_loss = 449.7, baseline_loss = 122.51, learner_queue_size = 32, _tick = 129, _time = 1.7372e+09)
[2025-01-17 21:52:42,741][root][INFO] - Step 332800 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1181.9, step = 332800, mean_episode_return = 0.29445, mean_episode_step = 83.042, total_loss = 563.42, entropy_loss = -8.7822, pg_loss = 449.7, baseline_loss = 122.51, learner_queue_size = 32, _tick = 129, _time = 1.7372e+09)
[2025-01-17 21:52:47,747][root][INFO] - Step 335360 @ 511.4 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 1186.9, step = 335360, mean_episode_return = 0.30825, mean_episode_step = 79.913, total_loss = 145.68, entropy_loss = -8.8045, pg_loss = 44.644, baseline_loss = 109.84, learner_queue_size = 32, _tick = 130, _time = 1.7372e+09)
[2025-01-17 21:52:52,752][root][INFO] - Step 335360 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1191.9, step = 335360, mean_episode_return = 0.30825, mean_episode_step = 79.913, total_loss = 145.68, entropy_loss = -8.8045, pg_loss = 44.644, baseline_loss = 109.84, learner_queue_size = 32, _tick = 130, _time = 1.7372e+09)
[2025-01-17 21:52:57,757][root][INFO] - Step 337920 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1196.9, step = 337920, mean_episode_return = 0.43767, mean_episode_step = 87.999, total_loss = 205.18, entropy_loss = -8.8055, pg_loss = 91.641, baseline_loss = 122.34, learner_queue_size = 32, _tick = 131, _time = 1.7372e+09)
[2025-01-17 21:53:02,763][root][INFO] - Saving checkpoint to /opt/minihack/checkpoint.tar
[2025-01-17 21:53:02,901][root][INFO] - Step 340480 @ 511.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1201.9, step = 340480, mean_episode_return = 0.37706, mean_episode_step = 85.309, total_loss = -340.49, entropy_loss = -8.858, pg_loss = -429.57, baseline_loss = 97.94, learner_queue_size = 32, _tick = 132, _time = 1.7372e+09)
[2025-01-17 21:53:07,908][root][INFO] - Step 340480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1207.1, step = 340480, mean_episode_return = 0.37706, mean_episode_step = 85.309, total_loss = -340.49, entropy_loss = -8.858, pg_loss = -429.57, baseline_loss = 97.94, learner_queue_size = 32, _tick = 132, _time = 1.7372e+09)
[2025-01-17 21:53:12,915][root][INFO] - Step 343040 @ 511.3 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (train_seconds = 1212.1, step = 343040, mean_episode_return = 0.39426, mean_episode_step = 72.097, total_loss = 138.66, entropy_loss = -8.8509, pg_loss = 17.769, baseline_loss = 129.74, learner_queue_size = 32, _tick = 133, _time = 1.7372e+09)
[2025-01-17 21:53:17,920][root][INFO] - Step 345600 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1217.1, step = 345600, mean_episode_return = 0.46291, mean_episode_step = 83.09, total_loss = 27.593, entropy_loss = -8.8306, pg_loss = -62.659, baseline_loss = 99.082, learner_queue_size = 32, _tick = 134, _time = 1.7372e+09)
[2025-01-17 21:53:22,925][root][INFO] - Step 345600 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1222.1, step = 345600, mean_episode_return = 0.46291, mean_episode_step = 83.09, total_loss = 27.593, entropy_loss = -8.8306, pg_loss = -62.659, baseline_loss = 99.082, learner_queue_size = 32, _tick = 134, _time = 1.7372e+09)
[2025-01-17 21:53:27,930][root][INFO] - Step 348160 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1227.1, step = 348160, mean_episode_return = 0.27243, mean_episode_step = 64.745, total_loss = -241.04, entropy_loss = -8.8598, pg_loss = -328.57, baseline_loss = 96.391, learner_queue_size = 32, _tick = 135, _time = 1.7372e+09)
[2025-01-17 21:53:32,935][root][INFO] - Step 348160 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1232.1, step = 348160, mean_episode_return = 0.27243, mean_episode_step = 64.745, total_loss = -241.04, entropy_loss = -8.8598, pg_loss = -328.57, baseline_loss = 96.391, learner_queue_size = 32, _tick = 135, _time = 1.7372e+09)
[2025-01-17 21:53:37,941][root][INFO] - Step 350720 @ 511.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1237.1, step = 350720, mean_episode_return = 0.25383, mean_episode_step = 71.382, total_loss = 466.87, entropy_loss = -8.7952, pg_loss = 371.41, baseline_loss = 104.25, learner_queue_size = 32, _tick = 136, _time = 1.7372e+09)
[2025-01-17 21:53:42,945][root][INFO] - Step 353280 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1242.1, step = 353280, mean_episode_return = 0.32398, mean_episode_step = 68.514, total_loss = -844.45, entropy_loss = -8.8767, pg_loss = -903.22, baseline_loss = 67.646, learner_queue_size = 32, _tick = 137, _time = 1.7372e+09)
[2025-01-17 21:53:47,950][root][INFO] - Step 353280 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1247.1, step = 353280, mean_episode_return = 0.32398, mean_episode_step = 68.514, total_loss = -844.45, entropy_loss = -8.8767, pg_loss = -903.22, baseline_loss = 67.646, learner_queue_size = 32, _tick = 137, _time = 1.7372e+09)
[2025-01-17 21:53:52,957][root][INFO] - Step 355840 @ 511.3 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (train_seconds = 1252.1, step = 355840, mean_episode_return = 0.33396, mean_episode_step = 73.701, total_loss = 337.48, entropy_loss = -8.818, pg_loss = 233.77, baseline_loss = 112.53, learner_queue_size = 32, _tick = 138, _time = 1.7372e+09)
[2025-01-17 21:53:57,962][root][INFO] - Step 358400 @ 511.4 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 1257.1, step = 358400, mean_episode_return = 0.39362, mean_episode_step = 84.737, total_loss = -127.81, entropy_loss = -8.7933, pg_loss = -221.33, baseline_loss = 102.32, learner_queue_size = 32, _tick = 139, _time = 1.7372e+09)
[2025-01-17 21:54:02,968][root][INFO] - Step 358400 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1262.1, step = 358400, mean_episode_return = 0.39362, mean_episode_step = 84.737, total_loss = -127.81, entropy_loss = -8.7933, pg_loss = -221.33, baseline_loss = 102.32, learner_queue_size = 32, _tick = 139, _time = 1.7372e+09)
[2025-01-17 21:54:07,973][root][INFO] - Step 360960 @ 511.4 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1267.1, step = 360960, mean_episode_return = 0.38035, mean_episode_step = 73.708, total_loss = 1137.8, entropy_loss = -8.8032, pg_loss = 967.6, baseline_loss = 179.01, learner_queue_size = 32, _tick = 140, _time = 1.7372e+09)
[2025-01-17 21:54:12,978][root][INFO] - Step 360960 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1272.1, step = 360960, mean_episode_return = 0.38035, mean_episode_step = 73.708, total_loss = 1137.8, entropy_loss = -8.8032, pg_loss = 967.6, baseline_loss = 179.01, learner_queue_size = 32, _tick = 140, _time = 1.7372e+09)
[2025-01-17 21:54:17,983][root][INFO] - Step 363520 @ 511.5 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 1277.1, step = 363520, mean_episode_return = 0.27195, mean_episode_step = 71.78, total_loss = -365.85, entropy_loss = -8.8324, pg_loss = -459.24, baseline_loss = 102.22, learner_queue_size = 32, _tick = 141, _time = 1.7372e+09)
[2025-01-17 21:54:22,989][root][INFO] - Step 366080 @ 511.4 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 1282.1, step = 366080, mean_episode_return = 0.22192, mean_episode_step = 66.635, total_loss = 324.65, entropy_loss = -8.8094, pg_loss = 203.94, baseline_loss = 129.52, learner_queue_size = 32, _tick = 142, _time = 1.7372e+09)
[2025-01-17 21:54:27,995][root][INFO] - Step 366080 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1287.2, step = 366080, mean_episode_return = 0.22192, mean_episode_step = 66.635, total_loss = 324.65, entropy_loss = -8.8094, pg_loss = 203.94, baseline_loss = 129.52, learner_queue_size = 32, _tick = 142, _time = 1.7372e+09)
[2025-01-17 21:54:33,000][root][INFO] - Step 368640 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 1292.2, step = 368640, mean_episode_return = 0.43794, mean_episode_step = 88.557, total_loss = -618.03, entropy_loss = -8.8089, pg_loss = -696.86, baseline_loss = 87.64, learner_queue_size = 32, _tick = 143, _time = 1.7372e+09)
[2025-01-17 21:54:38,005][root][INFO] - Step 368640 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1297.2, step = 368640, mean_episode_return = 0.43794, mean_episode_step = 88.557, total_loss = -618.03, entropy_loss = -8.8089, pg_loss = -696.86, baseline_loss = 87.64, learner_queue_size = 32, _tick = 143, _time = 1.7372e+09)
[2025-01-17 21:54:43,010][root][INFO] - Step 371200 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1302.2, step = 371200, mean_episode_return = 0.37453, mean_episode_step = 72.744, total_loss = 49.901, entropy_loss = -8.818, pg_loss = -75.792, baseline_loss = 134.51, learner_queue_size = 32, _tick = 144, _time = 1.7372e+09)
[2025-01-17 21:54:48,016][root][INFO] - Step 373760 @ 511.4 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 1307.2, step = 373760, mean_episode_return = 0.31187, mean_episode_step = 70.573, total_loss = -27.672, entropy_loss = -8.7548, pg_loss = -127.53, baseline_loss = 108.61, learner_queue_size = 32, _tick = 145, _time = 1.7372e+09)
[2025-01-17 21:54:53,022][root][INFO] - Step 373760 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1312.2, step = 373760, mean_episode_return = 0.31187, mean_episode_step = 70.573, total_loss = -27.672, entropy_loss = -8.7548, pg_loss = -127.53, baseline_loss = 108.61, learner_queue_size = 32, _tick = 145, _time = 1.7372e+09)
[2025-01-17 21:54:58,027][root][INFO] - Saving checkpoint to /opt/minihack/checkpoint_0.75.tar
[2025-01-17 21:54:58,066][root][INFO] - Step 376320 @ 511.5 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 1317.2, step = 376320, mean_episode_return = 0.35769, mean_episode_step = 75.807, total_loss = 405.26, entropy_loss = -8.7351, pg_loss = 278.26, baseline_loss = 135.73, learner_queue_size = 32, _tick = 146, _time = 1.7372e+09)
[2025-01-17 21:55:03,071][root][INFO] - Step 378880 @ 507.5 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 1322.2, step = 378880, mean_episode_return = 0.29897, mean_episode_step = 73.295, total_loss = -50.982, entropy_loss = -8.744, pg_loss = -148.81, baseline_loss = 106.57, learner_queue_size = 32, _tick = 147, _time = 1.7372e+09)
[2025-01-17 21:55:08,076][root][INFO] - Step 378880 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1327.2, step = 378880, mean_episode_return = 0.29897, mean_episode_step = 73.295, total_loss = -50.982, entropy_loss = -8.744, pg_loss = -148.81, baseline_loss = 106.57, learner_queue_size = 32, _tick = 147, _time = 1.7372e+09)
[2025-01-17 21:55:13,081][root][INFO] - Step 381440 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1332.2, step = 381440, mean_episode_return = 0.48209, mean_episode_step = 89.396, total_loss = 71.365, entropy_loss = -8.728, pg_loss = -1.5562, baseline_loss = 81.65, learner_queue_size = 32, _tick = 148, _time = 1.7372e+09)
[2025-01-17 21:55:18,088][root][INFO] - Step 381440 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1337.2, step = 381440, mean_episode_return = 0.48209, mean_episode_step = 89.396, total_loss = 71.365, entropy_loss = -8.728, pg_loss = -1.5562, baseline_loss = 81.65, learner_queue_size = 32, _tick = 148, _time = 1.7372e+09)
[2025-01-17 21:55:23,094][root][INFO] - Step 384000 @ 511.4 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1342.3, step = 384000, mean_episode_return = 0.32923, mean_episode_step = 75.311, total_loss = 759.12, entropy_loss = -8.7315, pg_loss = 636.42, baseline_loss = 131.43, learner_queue_size = 32, _tick = 149, _time = 1.7372e+09)
[2025-01-17 21:55:28,101][root][INFO] - Step 386560 @ 511.4 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 1347.3, step = 386560, mean_episode_return = 0.23336, mean_episode_step = 70.033, total_loss = -1089.0, entropy_loss = -8.7929, pg_loss = -1141.2, baseline_loss = 60.946, learner_queue_size = 32, _tick = 150, _time = 1.7372e+09)
[2025-01-17 21:55:33,107][root][INFO] - Step 386560 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1352.3, step = 386560, mean_episode_return = 0.23336, mean_episode_step = 70.033, total_loss = -1089.0, entropy_loss = -8.7929, pg_loss = -1141.2, baseline_loss = 60.946, learner_queue_size = 32, _tick = 150, _time = 1.7372e+09)
[2025-01-17 21:55:38,112][root][INFO] - Step 389120 @ 511.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 1357.3, step = 389120, mean_episode_return = 0.41352, mean_episode_step = 86.383, total_loss = -400.28, entropy_loss = -8.7094, pg_loss = -464.07, baseline_loss = 72.498, learner_queue_size = 32, _tick = 151, _time = 1.7372e+09)
[2025-01-17 21:55:43,119][root][INFO] - Step 391680 @ 511.2 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1362.3, step = 391680, mean_episode_return = 0.41307, mean_episode_step = 81.93, total_loss = 10.287, entropy_loss = -8.712, pg_loss = -75.64, baseline_loss = 94.639, learner_queue_size = 32, _tick = 152, _time = 1.7372e+09)
[2025-01-17 21:55:48,125][root][INFO] - Step 391680 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1367.3, step = 391680, mean_episode_return = 0.41307, mean_episode_step = 81.93, total_loss = 10.287, entropy_loss = -8.712, pg_loss = -75.64, baseline_loss = 94.639, learner_queue_size = 32, _tick = 152, _time = 1.7372e+09)
[2025-01-17 21:55:53,130][root][INFO] - Step 394240 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 1372.3, step = 394240, mean_episode_return = 0.27223, mean_episode_step = 75.762, total_loss = -440.11, entropy_loss = -8.7181, pg_loss = -487.42, baseline_loss = 56.027, learner_queue_size = 32, _tick = 153, _time = 1.7372e+09)
[2025-01-17 21:55:58,135][root][INFO] - Step 394240 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1377.3, step = 394240, mean_episode_return = 0.27223, mean_episode_step = 75.762, total_loss = -440.11, entropy_loss = -8.7181, pg_loss = -487.42, baseline_loss = 56.027, learner_queue_size = 32, _tick = 153, _time = 1.7372e+09)
[2025-01-17 21:56:03,141][root][INFO] - Step 396800 @ 511.4 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1382.3, step = 396800, mean_episode_return = 0.21181, mean_episode_step = 75.632, total_loss = 530.79, entropy_loss = -8.6843, pg_loss = 413.97, baseline_loss = 125.51, learner_queue_size = 32, _tick = 154, _time = 1.7372e+09)
[2025-01-17 21:56:08,147][root][INFO] - Step 399360 @ 511.4 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (train_seconds = 1387.3, step = 399360, mean_episode_return = 0.23497, mean_episode_step = 74.593, total_loss = -640.4, entropy_loss = -8.7098, pg_loss = -688.27, baseline_loss = 56.583, learner_queue_size = 32, _tick = 155, _time = 1.7372e+09)
[2025-01-17 21:56:13,153][root][INFO] - Step 399360 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1392.3, step = 399360, mean_episode_return = 0.23497, mean_episode_step = 74.593, total_loss = -640.4, entropy_loss = -8.7098, pg_loss = -688.27, baseline_loss = 56.583, learner_queue_size = 32, _tick = 155, _time = 1.7372e+09)
[2025-01-17 21:56:18,158][root][INFO] - Step 401920 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1397.3, step = 401920, mean_episode_return = 0.57431, mean_episode_step = 81.755, total_loss = 81.653, entropy_loss = -8.6733, pg_loss = 24.539, baseline_loss = 65.788, learner_queue_size = 32, _tick = 156, _time = 1.7372e+09)
[2025-01-17 21:56:23,163][root][INFO] - Step 401920 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1402.3, step = 401920, mean_episode_return = 0.57431, mean_episode_step = 81.755, total_loss = 81.653, entropy_loss = -8.6733, pg_loss = 24.539, baseline_loss = 65.788, learner_queue_size = 32, _tick = 156, _time = 1.7372e+09)
[2025-01-17 21:56:28,169][root][INFO] - Step 404480 @ 511.4 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (train_seconds = 1407.3, step = 404480, mean_episode_return = 0.34443, mean_episode_step = 68.017, total_loss = 564.45, entropy_loss = -8.7077, pg_loss = 444.92, baseline_loss = 128.24, learner_queue_size = 32, _tick = 157, _time = 1.7372e+09)
[2025-01-17 21:56:33,175][root][INFO] - Step 404480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1412.3, step = 404480, mean_episode_return = 0.34443, mean_episode_step = 68.017, total_loss = 564.45, entropy_loss = -8.7077, pg_loss = 444.92, baseline_loss = 128.24, learner_queue_size = 32, _tick = 157, _time = 1.7372e+09)
[2025-01-17 21:56:38,181][root][INFO] - Step 407040 @ 511.3 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1417.3, step = 407040, mean_episode_return = 0.3764, mean_episode_step = 81.153, total_loss = 280.22, entropy_loss = -8.7071, pg_loss = 170.51, baseline_loss = 118.42, learner_queue_size = 32, _tick = 158, _time = 1.7372e+09)
[2025-01-17 21:56:43,188][root][INFO] - Step 407040 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1422.3, step = 407040, mean_episode_return = 0.3764, mean_episode_step = 81.153, total_loss = 280.22, entropy_loss = -8.7071, pg_loss = 170.51, baseline_loss = 118.42, learner_queue_size = 32, _tick = 158, _time = 1.7372e+09)
[2025-01-17 21:56:48,193][root][INFO] - Step 409600 @ 511.4 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 1427.3, step = 409600, mean_episode_return = 0.27058, mean_episode_step = 74.26, total_loss = -705.24, entropy_loss = -8.7376, pg_loss = -772.86, baseline_loss = 76.361, learner_queue_size = 32, _tick = 159, _time = 1.7372e+09)
[2025-01-17 21:56:53,198][root][INFO] - Step 409600 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1432.4, step = 409600, mean_episode_return = 0.27058, mean_episode_step = 74.26, total_loss = -705.24, entropy_loss = -8.7376, pg_loss = -772.86, baseline_loss = 76.361, learner_queue_size = 32, _tick = 159, _time = 1.7372e+09)
[2025-01-17 21:56:58,203][root][INFO] - Step 412160 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 1437.4, step = 412160, mean_episode_return = 0.36356, mean_episode_step = 68.417, total_loss = 502.05, entropy_loss = -8.7313, pg_loss = 344.03, baseline_loss = 166.75, learner_queue_size = 32, _tick = 160, _time = 1.7372e+09)
[2025-01-17 21:57:03,208][root][INFO] - Step 414720 @ 511.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (train_seconds = 1442.4, step = 414720, mean_episode_return = 0.38365, mean_episode_step = 70.664, total_loss = -530.73, entropy_loss = -8.7175, pg_loss = -600.98, baseline_loss = 78.967, learner_queue_size = 32, _tick = 161, _time = 1.7372e+09)
[2025-01-17 21:57:08,213][root][INFO] - Step 414720 @ 0.0 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 1447.4, step = 414720, mean_episode_return = 0.38365, mean_episode_step = 70.664, total_loss = -530.73, entropy_loss = -8.7175, pg_loss = -600.98, baseline_loss = 78.967, learner_queue_size = 32, _tick = 161, _time = 1.7372e+09)
[2025-01-17 21:57:13,218][root][INFO] - Step 417280 @ 511.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 1452.4, step = 417280, mean_episode_return = 0.34235, mean_episode_step = 82.462, total_loss = -34.622, entropy_loss = -8.6896, pg_loss = -117.75, baseline_loss = 91.813, learner_queue_size = 32, _tick = 162, _time = 1.7372e+09)
[2025-01-17 21:57:18,223][root][INFO] - Step 417280 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1457.4, step = 417280, mean_episode_return = 0.34235, mean_episode_step = 82.462, total_loss = -34.622, entropy_loss = -8.6896, pg_loss = -117.75, baseline_loss = 91.813, learner_queue_size = 32, _tick = 162, _time = 1.7372e+09)
[2025-01-17 21:57:23,228][root][INFO] - Step 419840 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 1462.4, step = 419840, mean_episode_return = 0.24216, mean_episode_step = 58.268, total_loss = -31.779, entropy_loss = -8.736, pg_loss = -141.5, baseline_loss = 118.46, learner_queue_size = 32, _tick = 163, _time = 1.7372e+09)
[2025-01-17 21:57:28,233][root][INFO] - Step 419840 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1467.4, step = 419840, mean_episode_return = 0.24216, mean_episode_step = 58.268, total_loss = -31.779, entropy_loss = -8.736, pg_loss = -141.5, baseline_loss = 118.46, learner_queue_size = 32, _tick = 163, _time = 1.7372e+09)
[2025-01-17 21:57:33,238][root][INFO] - Step 422400 @ 511.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 1472.4, step = 422400, mean_episode_return = 0.25378, mean_episode_step = 70.866, total_loss = 260.35, entropy_loss = -8.7031, pg_loss = 139.16, baseline_loss = 129.9, learner_queue_size = 32, _tick = 164, _time = 1.7372e+09)
[2025-01-17 21:57:38,243][root][INFO] - Step 422400 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1477.4, step = 422400, mean_episode_return = 0.25378, mean_episode_step = 70.866, total_loss = 260.35, entropy_loss = -8.7031, pg_loss = 139.16, baseline_loss = 129.9, learner_queue_size = 32, _tick = 164, _time = 1.7372e+09)
[2025-01-17 21:57:43,248][root][INFO] - Step 424960 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1482.4, step = 424960, mean_episode_return = 0.46408, mean_episode_step = 79.86, total_loss = 520.14, entropy_loss = -8.7163, pg_loss = 399.8, baseline_loss = 129.05, learner_queue_size = 32, _tick = 165, _time = 1.7372e+09)
[2025-01-17 21:57:48,253][root][INFO] - Step 424960 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1487.4, step = 424960, mean_episode_return = 0.46408, mean_episode_step = 79.86, total_loss = 520.14, entropy_loss = -8.7163, pg_loss = 399.8, baseline_loss = 129.05, learner_queue_size = 32, _tick = 165, _time = 1.7372e+09)
[2025-01-17 21:57:53,258][root][INFO] - Step 427520 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1492.4, step = 427520, mean_episode_return = 0.21614, mean_episode_step = 63.979, total_loss = -139.76, entropy_loss = -8.7619, pg_loss = -258.45, baseline_loss = 127.45, learner_queue_size = 32, _tick = 166, _time = 1.7372e+09)
[2025-01-17 21:57:58,263][root][INFO] - Step 430080 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1497.4, step = 430080, mean_episode_return = 0.57062, mean_episode_step = 90.791, total_loss = 1325.9, entropy_loss = -8.6811, pg_loss = 1146.0, baseline_loss = 188.64, learner_queue_size = 32, _tick = 167, _time = 1.7372e+09)
[2025-01-17 21:58:03,269][root][INFO] - Step 430080 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1502.4, step = 430080, mean_episode_return = 0.57062, mean_episode_step = 90.791, total_loss = 1325.9, entropy_loss = -8.6811, pg_loss = 1146.0, baseline_loss = 188.64, learner_queue_size = 32, _tick = 167, _time = 1.7372e+09)
[2025-01-17 21:58:08,274][root][INFO] - Step 432640 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 1507.4, step = 432640, mean_episode_return = 0.48041, mean_episode_step = 79.67, total_loss = -338.38, entropy_loss = -8.6964, pg_loss = -409.1, baseline_loss = 79.423, learner_queue_size = 32, _tick = 168, _time = 1.7372e+09)
[2025-01-17 21:58:13,279][root][INFO] - Step 432640 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1512.4, step = 432640, mean_episode_return = 0.48041, mean_episode_step = 79.67, total_loss = -338.38, entropy_loss = -8.6964, pg_loss = -409.1, baseline_loss = 79.423, learner_queue_size = 32, _tick = 168, _time = 1.7372e+09)
[2025-01-17 21:58:18,308][root][INFO] - Step 435200 @ 509.1 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 1517.5, step = 435200, mean_episode_return = 0.33672, mean_episode_step = 65.029, total_loss = 612.15, entropy_loss = -8.7478, pg_loss = 427.07, baseline_loss = 193.82, learner_queue_size = 32, _tick = 169, _time = 1.7372e+09)
[2025-01-17 21:58:23,314][root][INFO] - Step 437760 @ 511.5 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (train_seconds = 1522.5, step = 437760, mean_episode_return = 0.37672, mean_episode_step = 74.773, total_loss = -161.56, entropy_loss = -8.7152, pg_loss = -257.15, baseline_loss = 104.3, learner_queue_size = 32, _tick = 170, _time = 1.7372e+09)
[2025-01-17 21:58:28,319][root][INFO] - Step 437760 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1527.5, step = 437760, mean_episode_return = 0.37672, mean_episode_step = 74.773, total_loss = -161.56, entropy_loss = -8.7152, pg_loss = -257.15, baseline_loss = 104.3, learner_queue_size = 32, _tick = 170, _time = 1.7372e+09)
[2025-01-17 21:58:33,324][root][INFO] - Step 440320 @ 511.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 1532.5, step = 440320, mean_episode_return = 0.24457, mean_episode_step = 72.509, total_loss = 229.44, entropy_loss = -8.6836, pg_loss = 120.46, baseline_loss = 117.67, learner_queue_size = 32, _tick = 171, _time = 1.7372e+09)
[2025-01-17 21:58:38,330][root][INFO] - Step 440320 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1537.5, step = 440320, mean_episode_return = 0.24457, mean_episode_step = 72.509, total_loss = 229.44, entropy_loss = -8.6836, pg_loss = 120.46, baseline_loss = 117.67, learner_queue_size = 32, _tick = 171, _time = 1.7372e+09)
[2025-01-17 21:58:43,336][root][INFO] - Step 442880 @ 511.4 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1542.5, step = 442880, mean_episode_return = 0.49822, mean_episode_step = 75.016, total_loss = 356.28, entropy_loss = -8.6967, pg_loss = 204.14, baseline_loss = 160.84, learner_queue_size = 32, _tick = 172, _time = 1.7372e+09)
[2025-01-17 21:58:48,342][root][INFO] - Step 442880 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1547.5, step = 442880, mean_episode_return = 0.49822, mean_episode_step = 75.016, total_loss = 356.28, entropy_loss = -8.6967, pg_loss = 204.14, baseline_loss = 160.84, learner_queue_size = 32, _tick = 172, _time = 1.7372e+09)
[2025-01-17 21:58:53,386][root][INFO] - Step 445440 @ 507.6 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 1552.5, step = 445440, mean_episode_return = 0.36565, mean_episode_step = 72.158, total_loss = -136.79, entropy_loss = -8.6763, pg_loss = -231.03, baseline_loss = 102.91, learner_queue_size = 32, _tick = 173, _time = 1.7372e+09)
[2025-01-17 21:58:58,391][root][INFO] - Step 445440 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1557.5, step = 445440, mean_episode_return = 0.36565, mean_episode_step = 72.158, total_loss = -136.79, entropy_loss = -8.6763, pg_loss = -231.03, baseline_loss = 102.91, learner_queue_size = 32, _tick = 173, _time = 1.7372e+09)
[2025-01-17 21:59:03,396][root][INFO] - Step 448000 @ 511.5 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 1562.6, step = 448000, mean_episode_return = 0.23861, mean_episode_step = 67.963, total_loss = 176.38, entropy_loss = -8.6751, pg_loss = 45.659, baseline_loss = 139.39, learner_queue_size = 32, _tick = 174, _time = 1.7372e+09)
[2025-01-17 21:59:08,402][root][INFO] - Step 450560 @ 511.4 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1567.6, step = 450560, mean_episode_return = 0.30636, mean_episode_step = 53.13, total_loss = 203.22, entropy_loss = -8.7257, pg_loss = 22.976, baseline_loss = 188.97, learner_queue_size = 32, _tick = 175, _time = 1.7372e+09)
[2025-01-17 21:59:13,408][root][INFO] - Step 450560 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1572.6, step = 450560, mean_episode_return = 0.30636, mean_episode_step = 53.13, total_loss = 203.22, entropy_loss = -8.7257, pg_loss = 22.976, baseline_loss = 188.97, learner_queue_size = 32, _tick = 175, _time = 1.7372e+09)
[2025-01-17 21:59:18,416][root][INFO] - Step 453120 @ 511.2 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 1577.6, step = 453120, mean_episode_return = 0.33212, mean_episode_step = 74.639, total_loss = -132.34, entropy_loss = -8.6472, pg_loss = -267.36, baseline_loss = 143.67, learner_queue_size = 32, _tick = 176, _time = 1.7372e+09)
[2025-01-17 21:59:23,422][root][INFO] - Step 453120 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1582.6, step = 453120, mean_episode_return = 0.33212, mean_episode_step = 74.639, total_loss = -132.34, entropy_loss = -8.6472, pg_loss = -267.36, baseline_loss = 143.67, learner_queue_size = 32, _tick = 176, _time = 1.7372e+09)
[2025-01-17 21:59:28,427][root][INFO] - Step 455680 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1587.6, step = 455680, mean_episode_return = 0.24763, mean_episode_step = 80.027, total_loss = -23.447, entropy_loss = -8.619, pg_loss = -139.67, baseline_loss = 124.84, learner_queue_size = 32, _tick = 177, _time = 1.7372e+09)
[2025-01-17 21:59:33,434][root][INFO] - Step 455680 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1592.6, step = 455680, mean_episode_return = 0.24763, mean_episode_step = 80.027, total_loss = -23.447, entropy_loss = -8.619, pg_loss = -139.67, baseline_loss = 124.84, learner_queue_size = 32, _tick = 177, _time = 1.7372e+09)
[2025-01-17 21:59:38,440][root][INFO] - Step 458240 @ 511.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1597.6, step = 458240, mean_episode_return = 0.30987, mean_episode_step = 71.053, total_loss = 254.58, entropy_loss = -8.6341, pg_loss = 132.97, baseline_loss = 130.24, learner_queue_size = 32, _tick = 178, _time = 1.7372e+09)
[2025-01-17 21:59:43,446][root][INFO] - Step 458240 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1602.6, step = 458240, mean_episode_return = 0.30987, mean_episode_step = 71.053, total_loss = 254.58, entropy_loss = -8.6341, pg_loss = 132.97, baseline_loss = 130.24, learner_queue_size = 32, _tick = 178, _time = 1.7372e+09)
[2025-01-17 21:59:48,451][root][INFO] - Step 460800 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1607.6, step = 460800, mean_episode_return = 0.1513, mean_episode_step = 76.086, total_loss = -287.1, entropy_loss = -8.6108, pg_loss = -377.36, baseline_loss = 98.87, learner_queue_size = 32, _tick = 179, _time = 1.7372e+09)
[2025-01-17 21:59:53,456][root][INFO] - Step 460800 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1612.6, step = 460800, mean_episode_return = 0.1513, mean_episode_step = 76.086, total_loss = -287.1, entropy_loss = -8.6108, pg_loss = -377.36, baseline_loss = 98.87, learner_queue_size = 32, _tick = 179, _time = 1.7372e+09)
[2025-01-17 21:59:58,462][root][INFO] - Step 463360 @ 511.4 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (train_seconds = 1617.6, step = 463360, mean_episode_return = 0.50642, mean_episode_step = 78.418, total_loss = 377.95, entropy_loss = -8.5916, pg_loss = 282.24, baseline_loss = 104.3, learner_queue_size = 32, _tick = 180, _time = 1.7372e+09)
[2025-01-17 22:00:03,467][root][INFO] - Step 463360 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1622.6, step = 463360, mean_episode_return = 0.50642, mean_episode_step = 78.418, total_loss = 377.95, entropy_loss = -8.5916, pg_loss = 282.24, baseline_loss = 104.3, learner_queue_size = 32, _tick = 180, _time = 1.7372e+09)
[2025-01-17 22:00:08,472][root][INFO] - Step 465920 @ 511.5 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (train_seconds = 1627.6, step = 465920, mean_episode_return = 0.43367, mean_episode_step = 73.57, total_loss = 216.2, entropy_loss = -8.6202, pg_loss = 106.28, baseline_loss = 118.54, learner_queue_size = 32, _tick = 181, _time = 1.7372e+09)
[2025-01-17 22:00:13,477][root][INFO] - Step 468480 @ 511.5 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (train_seconds = 1632.6, step = 468480, mean_episode_return = 0.35106, mean_episode_step = 79.009, total_loss = 718.83, entropy_loss = -8.6334, pg_loss = 550.83, baseline_loss = 176.64, learner_queue_size = 32, _tick = 182, _time = 1.7372e+09)
[2025-01-17 22:00:18,482][root][INFO] - Step 468480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1637.6, step = 468480, mean_episode_return = 0.35106, mean_episode_step = 79.009, total_loss = 718.83, entropy_loss = -8.6334, pg_loss = 550.83, baseline_loss = 176.64, learner_queue_size = 32, _tick = 182, _time = 1.7372e+09)
[2025-01-17 22:00:23,487][root][INFO] - Step 471040 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 1642.6, step = 471040, mean_episode_return = 0.32612, mean_episode_step = 80.102, total_loss = -546.78, entropy_loss = -8.6247, pg_loss = -638.15, baseline_loss = 99.998, learner_queue_size = 32, _tick = 183, _time = 1.7372e+09)
[2025-01-17 22:00:28,492][root][INFO] - Step 471040 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1647.6, step = 471040, mean_episode_return = 0.32612, mean_episode_step = 80.102, total_loss = -546.78, entropy_loss = -8.6247, pg_loss = -638.15, baseline_loss = 99.998, learner_queue_size = 32, _tick = 183, _time = 1.7372e+09)
[2025-01-17 22:00:33,497][root][INFO] - Step 473600 @ 511.5 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (train_seconds = 1652.7, step = 473600, mean_episode_return = 0.3777, mean_episode_step = 85.937, total_loss = 102.18, entropy_loss = -8.6123, pg_loss = -39.792, baseline_loss = 150.58, learner_queue_size = 32, _tick = 184, _time = 1.7372e+09)
[2025-01-17 22:00:38,502][root][INFO] - Step 476160 @ 511.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 1657.7, step = 476160, mean_episode_return = 0.42081, mean_episode_step = 83.464, total_loss = 272.71, entropy_loss = -8.6183, pg_loss = 127.82, baseline_loss = 153.51, learner_queue_size = 32, _tick = 185, _time = 1.7372e+09)
[2025-01-17 22:00:43,507][root][INFO] - Step 476160 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1662.7, step = 476160, mean_episode_return = 0.42081, mean_episode_step = 83.464, total_loss = 272.71, entropy_loss = -8.6183, pg_loss = 127.82, baseline_loss = 153.51, learner_queue_size = 32, _tick = 185, _time = 1.7372e+09)
[2025-01-17 22:00:48,512][root][INFO] - Step 478720 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1667.7, step = 478720, mean_episode_return = 0.4689, mean_episode_step = 84.412, total_loss = -84.119, entropy_loss = -8.6145, pg_loss = -187.66, baseline_loss = 112.16, learner_queue_size = 32, _tick = 186, _time = 1.7372e+09)
[2025-01-17 22:00:53,518][root][INFO] - Step 478720 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1672.7, step = 478720, mean_episode_return = 0.4689, mean_episode_step = 84.412, total_loss = -84.119, entropy_loss = -8.6145, pg_loss = -187.66, baseline_loss = 112.16, learner_queue_size = 32, _tick = 186, _time = 1.7372e+09)
[2025-01-17 22:00:58,523][root][INFO] - Step 481280 @ 511.5 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 1677.7, step = 481280, mean_episode_return = 0.26252, mean_episode_step = 72.663, total_loss = -396.51, entropy_loss = -8.6308, pg_loss = -512.14, baseline_loss = 124.26, learner_queue_size = 32, _tick = 187, _time = 1.7372e+09)
[2025-01-17 22:01:03,528][root][INFO] - Step 481280 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1682.7, step = 481280, mean_episode_return = 0.26252, mean_episode_step = 72.663, total_loss = -396.51, entropy_loss = -8.6308, pg_loss = -512.14, baseline_loss = 124.26, learner_queue_size = 32, _tick = 187, _time = 1.7372e+09)
[2025-01-17 22:01:08,533][root][INFO] - Step 483840 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1687.7, step = 483840, mean_episode_return = 0.25022, mean_episode_step = 78.078, total_loss = -854.63, entropy_loss = -8.6191, pg_loss = -924.23, baseline_loss = 78.222, learner_queue_size = 32, _tick = 188, _time = 1.7372e+09)
[2025-01-17 22:01:13,538][root][INFO] - Step 483840 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1692.7, step = 483840, mean_episode_return = 0.25022, mean_episode_step = 78.078, total_loss = -854.63, entropy_loss = -8.6191, pg_loss = -924.23, baseline_loss = 78.222, learner_queue_size = 32, _tick = 188, _time = 1.7372e+09)
[2025-01-17 22:01:18,543][root][INFO] - Step 486400 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1697.7, step = 486400, mean_episode_return = 0.36118, mean_episode_step = 76.179, total_loss = 115.35, entropy_loss = -8.6142, pg_loss = -54.621, baseline_loss = 178.59, learner_queue_size = 32, _tick = 189, _time = 1.7372e+09)
[2025-01-17 22:01:23,549][root][INFO] - Step 486400 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1702.7, step = 486400, mean_episode_return = 0.36118, mean_episode_step = 76.179, total_loss = 115.35, entropy_loss = -8.6142, pg_loss = -54.621, baseline_loss = 178.59, learner_queue_size = 32, _tick = 189, _time = 1.7372e+09)
[2025-01-17 22:01:28,555][root][INFO] - Step 488960 @ 511.4 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 1707.7, step = 488960, mean_episode_return = 0.43785, mean_episode_step = 70.248, total_loss = 380.29, entropy_loss = -8.616, pg_loss = 224.89, baseline_loss = 164.02, learner_queue_size = 32, _tick = 190, _time = 1.7372e+09)
[2025-01-17 22:01:33,560][root][INFO] - Step 491520 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1712.7, step = 491520, mean_episode_return = 0.36271, mean_episode_step = 92.427, total_loss = -631.62, entropy_loss = -8.5574, pg_loss = -695.22, baseline_loss = 72.153, learner_queue_size = 32, _tick = 191, _time = 1.7372e+09)
[2025-01-17 22:01:38,565][root][INFO] - Step 491520 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1717.7, step = 491520, mean_episode_return = 0.36271, mean_episode_step = 92.427, total_loss = -631.62, entropy_loss = -8.5574, pg_loss = -695.22, baseline_loss = 72.153, learner_queue_size = 32, _tick = 191, _time = 1.7372e+09)
[2025-01-17 22:01:43,570][root][INFO] - Step 494080 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 1722.7, step = 494080, mean_episode_return = 0.42517, mean_episode_step = 79.929, total_loss = 738.79, entropy_loss = -8.5857, pg_loss = 576.97, baseline_loss = 170.41, learner_queue_size = 32, _tick = 192, _time = 1.7372e+09)
[2025-01-17 22:01:48,575][root][INFO] - Step 494080 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1727.7, step = 494080, mean_episode_return = 0.42517, mean_episode_step = 79.929, total_loss = 738.79, entropy_loss = -8.5857, pg_loss = 576.97, baseline_loss = 170.41, learner_queue_size = 32, _tick = 192, _time = 1.7372e+09)
[2025-01-17 22:01:53,581][root][INFO] - Step 496640 @ 511.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1732.7, step = 496640, mean_episode_return = 0.51214, mean_episode_step = 91.735, total_loss = -196.83, entropy_loss = -8.5589, pg_loss = -272.3, baseline_loss = 84.024, learner_queue_size = 32, _tick = 193, _time = 1.7372e+09)
[2025-01-17 22:01:58,586][root][INFO] - Step 499200 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 1737.7, step = 499200, mean_episode_return = 0.34384, mean_episode_step = 77.323, total_loss = -65.042, entropy_loss = -8.5939, pg_loss = -175.66, baseline_loss = 119.22, learner_queue_size = 32, _tick = 194, _time = 1.7372e+09)
[2025-01-17 22:02:03,591][root][INFO] - Step 499200 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1742.7, step = 499200, mean_episode_return = 0.34384, mean_episode_step = 77.323, total_loss = -65.042, entropy_loss = -8.5939, pg_loss = -175.66, baseline_loss = 119.22, learner_queue_size = 32, _tick = 194, _time = 1.7372e+09)
[2025-01-17 22:02:08,597][root][INFO] - Step 501760 @ 511.4 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 1747.8, step = 501760, mean_episode_return = 0.38612, mean_episode_step = 66.368, total_loss = -149.1, entropy_loss = -8.6326, pg_loss = -266.73, baseline_loss = 126.26, learner_queue_size = 32, _tick = 195, _time = 1.7372e+09)
[2025-01-17 22:02:08,597][root][INFO] - Learning finished after 501760 steps.
[2025-01-17 22:02:08,606][root][INFO] - Saving checkpoint to /opt/minihack/checkpoint.tar
[2025-01-18 10:00:37,821][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,854][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,823][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,827][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,851][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,821][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:38,008][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,854][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,856][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,821][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,823][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,853][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,821][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,843][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,825][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,821][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,822][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,825][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,824][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,829][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,849][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,824][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,821][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,823][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,825][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,972][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,821][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,821][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,821][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,851][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,822][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,922][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,822][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,822][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,822][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,856][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,821][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,970][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,823][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,822][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,825][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,826][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,825][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,850][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,849][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,823][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,842][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,850][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,830][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,824][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,821][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,823][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,823][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,821][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,858][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,822][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,823][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,850][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,852][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,822][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,891][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,825][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,822][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,856][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,858][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,823][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,828][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,821][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,852][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,849][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,854][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,850][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,855][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,821][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,852][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,851][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,823][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,821][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,823][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,825][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,850][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,824][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,846][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,850][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,850][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,821][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,823][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,850][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,857][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,823][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,845][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,821][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,823][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,821][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,822][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,822][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,823][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,821][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,893][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,821][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:38,059][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,924][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,852][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-18 10:00:37,835][nle.env.base][INFO] - Not saving any NLE data.
