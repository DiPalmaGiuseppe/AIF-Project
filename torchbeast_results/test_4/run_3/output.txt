[[36m2025-01-20 12:54:39,035[0m][[34mroot[0m][[32mINFO[0m] - name: null
wandb: false
project: minihack
entity: entity_name
group: default
mock: false
single_ttyrec: true
num_seeds: 0
write_profiler_trace: false
relative_reward: false
fn_penalty_step: constant
penalty_time: 0.0
penalty_step: -0.001
reward_lose: 0
reward_win: 1
save_tty: false
character: null
mode: train
env: MiniHack-Combat-Skill-v0
obs_keys: glyphs,chars,colors,specials,blstats,message
num_actors: 256
total_steps: 500000
batch_size: 32
unroll_length: 80
num_learner_threads: 1
num_inference_threads: 1
disable_cuda: false
learner_device: cuda:0
actor_device: cuda:0
max_learner_queue_size: null
model: baseline
use_lstm: true
hidden_dim: 256
embedding_dim: 64
glyph_type: all_cat
equalize_input_dim: false
equalize_factor: 2
layers: 5
crop_model: cnn
crop_dim: 9
use_index_select: true
entropy_cost: 0.001
baseline_cost: 0.5
discounting: 0.999
reward_clipping: none
normalize_reward: true
learning_rate: 0.0002
grad_norm_clipping: 40
alpha: 0.99
momentum: 0
epsilon: 1.0e-06
state_counter: none
no_extrinsic: false
int:
  twoheaded: true
  input: full
  intrinsic_weight: 0.1
  discounting: 0.99
  baseline_cost: 0.5
  episodic: true
  reward_clipping: none
  normalize_reward: true
ride:
  count_norm: true
  forward_cost: 1
  inverse_cost: 0.1
  hidden_dim: 128
rnd:
  forward_cost: 0.01
msg:
  model: none
  hidden_dim: 64
  embedding_dim: 32
[0m
[[36m2025-01-20 12:54:39,103[0m][[34mroot[0m][[32mINFO[0m] - Symlinked log directory: /opt/minihack/latest[0m
[[36m2025-01-20 12:54:39,103[0m][[34mroot[0m][[32mINFO[0m] - Found archive directory: /opt/minihack/archives[0m
[[36m2025-01-20 12:54:39,108[0m][[34mroot[0m][[32mINFO[0m] - Logging results to /opt/minihack[0m
[[36m2025-01-20 12:54:39,155[0m][[34mpalaas/out[0m][[32mINFO[0m] - Found log directory: /opt/minihack[0m
[[36m2025-01-20 12:54:39,155[0m][[34mpalaas/out[0m][[32mINFO[0m] - Saving arguments to /opt/minihack/meta.json[0m
[[36m2025-01-20 12:54:39,156[0m][[34mpalaas/out[0m][[32mINFO[0m] - Saving messages to /opt/minihack/out.log[0m
[[36m2025-01-20 12:54:39,156[0m][[34mpalaas/out[0m][[32mINFO[0m] - Saving logs data to /opt/minihack/logs.csv[0m
[[36m2025-01-20 12:54:39,156[0m][[34mpalaas/out[0m][[32mINFO[0m] - Saving logs' fields to /opt/minihack/fields.csv[0m
[[36m2025-01-20 12:54:39,157[0m][[34mroot[0m][[32mINFO[0m] - Not using CUDA.[0m
[[36m2025-01-20 12:54:39,165[0m][[34mroot[0m][[32mINFO[0m] - Using model baseline[0m
[[36m2025-01-20 12:54:39,166[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,226[0m][[34mroot[0m][[32mINFO[0m] - Number of model parameters: 4264078[0m
[[36m2025-01-20 12:54:39,226[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,287[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,287[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,288[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,288[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,290[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,290[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,291[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,291[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,292[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,294[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,295[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,296[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,296[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,298[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,299[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,299[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,301[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,302[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,295[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,320[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,305[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
First Environment waiting for connection to unix:/tmp/poly..opt.minihack.0 ... connection established.
[[36m2025-01-20 12:54:39,325[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,336[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,341[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,350[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,351[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,353[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,353[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,355[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,356[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,357[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,357[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,360[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,361[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,362[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,363[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,363[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,373[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,380[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,385[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,393[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,398[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,402[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,402[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,402[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,406[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:39,406[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 12:54:44,287[0m][[34mroot[0m][[32mINFO[0m] - Step 0 @ 0.0 SPS. Inference batcher size: 54. Learner queue size: 15. Other stats: (train_seconds = 5.0)[0m
[[36m2025-01-20 12:54:49,292[0m][[34mroot[0m][[32mINFO[0m] - Step 0 @ 0.0 SPS. Inference batcher size: 21. Learner queue size: 16. Other stats: (train_seconds = 10.0)[0m
[[36m2025-01-20 12:54:54,305[0m][[34mroot[0m][[32mINFO[0m] - Step 0 @ 0.0 SPS. Inference batcher size: 109. Learner queue size: 20. Other stats: (train_seconds = 15.0)[0m
[[36m2025-01-20 12:54:59,318[0m][[34mroot[0m][[32mINFO[0m] - Step 0 @ 0.0 SPS. Inference batcher size: 158. Learner queue size: 23. Other stats: (train_seconds = 20.0)[0m
[[36m2025-01-20 12:55:04,325[0m][[34mroot[0m][[32mINFO[0m] - Step 0 @ 0.0 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 25.0)[0m
[[36m2025-01-20 12:55:09,329[0m][[34mpalaas/out[0m][[32mINFO[0m] - Updated log fields: ['_tick', '_time', 'train_seconds', 'step', 'mean_episode_return', 'mean_episode_step', 'total_loss', 'entropy_loss', 'pg_loss', 'baseline_loss', 'learner_queue_size'][0m
[[36m2025-01-20 12:55:09,338[0m][[34mroot[0m][[32mINFO[0m] - Saving checkpoint to /opt/minihack/checkpoint_0.tar[0m
[[36m2025-01-20 12:55:09,363[0m][[34mroot[0m][[32mINFO[0m] - Step 2560 @ 510.6 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (train_seconds = 30.1, step = 2560, mean_episode_return = -0.060478, mean_episode_step = 46.47, total_loss = -286.86, entropy_loss = -11.371, pg_loss = -741.73, baseline_loss = 466.24, learner_queue_size = 32, _tick = 0, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:55:14,368[0m][[34mroot[0m][[32mINFO[0m] - Step 2560 @ 0.0 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 35.1, step = 2560, mean_episode_return = -0.060478, mean_episode_step = 46.47, total_loss = -286.86, entropy_loss = -11.371, pg_loss = -741.73, baseline_loss = 466.24, learner_queue_size = 32, _tick = 0, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:55:19,377[0m][[34mroot[0m][[32mINFO[0m] - Step 5120 @ 511.1 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 40.1, step = 5120, mean_episode_return = 0.0097223, mean_episode_step = 33.175, total_loss = -962.5, entropy_loss = -11.371, pg_loss = -1102.1, baseline_loss = 150.99, learner_queue_size = 32, _tick = 1, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:55:24,382[0m][[34mroot[0m][[32mINFO[0m] - Step 5120 @ 0.0 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 45.1, step = 5120, mean_episode_return = 0.0097223, mean_episode_step = 33.175, total_loss = -962.5, entropy_loss = -11.371, pg_loss = -1102.1, baseline_loss = 150.99, learner_queue_size = 32, _tick = 1, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:55:29,388[0m][[34mroot[0m][[32mINFO[0m] - Step 5120 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 50.1, step = 5120, mean_episode_return = 0.0097223, mean_episode_step = 33.175, total_loss = -962.5, entropy_loss = -11.371, pg_loss = -1102.1, baseline_loss = 150.99, learner_queue_size = 32, _tick = 1, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:55:34,393[0m][[34mroot[0m][[32mINFO[0m] - Step 7680 @ 511.5 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 55.1, step = 7680, mean_episode_return = -0.040158, mean_episode_step = 32.081, total_loss = 2270.6, entropy_loss = -11.327, pg_loss = 1961.6, baseline_loss = 320.27, learner_queue_size = 32, _tick = 2, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:55:39,400[0m][[34mroot[0m][[32mINFO[0m] - Step 10240 @ 511.3 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 60.1, step = 10240, mean_episode_return = -0.044864, mean_episode_step = 37.584, total_loss = 2634.4, entropy_loss = -11.368, pg_loss = 2163.9, baseline_loss = 481.92, learner_queue_size = 32, _tick = 3, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:55:44,405[0m][[34mroot[0m][[32mINFO[0m] - Step 10240 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 65.1, step = 10240, mean_episode_return = -0.044864, mean_episode_step = 37.584, total_loss = 2634.4, entropy_loss = -11.368, pg_loss = 2163.9, baseline_loss = 481.92, learner_queue_size = 32, _tick = 3, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:55:49,411[0m][[34mroot[0m][[32mINFO[0m] - Step 12800 @ 511.4 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 70.1, step = 12800, mean_episode_return = -0.054269, mean_episode_step = 51.955, total_loss = -736.1, entropy_loss = -11.368, pg_loss = -823.11, baseline_loss = 98.374, learner_queue_size = 32, _tick = 4, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:55:54,416[0m][[34mroot[0m][[32mINFO[0m] - Step 12800 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 75.1, step = 12800, mean_episode_return = -0.054269, mean_episode_step = 51.955, total_loss = -736.1, entropy_loss = -11.368, pg_loss = -823.11, baseline_loss = 98.374, learner_queue_size = 32, _tick = 4, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:55:59,421[0m][[34mroot[0m][[32mINFO[0m] - Step 15360 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 80.1, step = 15360, mean_episode_return = -0.018643, mean_episode_step = 48.605, total_loss = 732.54, entropy_loss = -11.355, pg_loss = 693.75, baseline_loss = 50.148, learner_queue_size = 32, _tick = 5, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:56:04,427[0m][[34mroot[0m][[32mINFO[0m] - Step 17920 @ 511.4 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 85.1, step = 17920, mean_episode_return = -0.028864, mean_episode_step = 61.838, total_loss = 470.33, entropy_loss = -11.37, pg_loss = 200.26, baseline_loss = 281.44, learner_queue_size = 32, _tick = 6, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:56:09,432[0m][[34mroot[0m][[32mINFO[0m] - Step 17920 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 90.2, step = 17920, mean_episode_return = -0.028864, mean_episode_step = 61.838, total_loss = 470.33, entropy_loss = -11.37, pg_loss = 200.26, baseline_loss = 281.44, learner_queue_size = 32, _tick = 6, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:56:14,437[0m][[34mroot[0m][[32mINFO[0m] - Step 20480 @ 511.5 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (train_seconds = 95.2, step = 20480, mean_episode_return = -0.051481, mean_episode_step = 50.114, total_loss = -937.54, entropy_loss = -11.321, pg_loss = -977.92, baseline_loss = 51.703, learner_queue_size = 32, _tick = 7, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:56:19,442[0m][[34mroot[0m][[32mINFO[0m] - Step 20480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 100.2, step = 20480, mean_episode_return = -0.051481, mean_episode_step = 50.114, total_loss = -937.54, entropy_loss = -11.321, pg_loss = -977.92, baseline_loss = 51.703, learner_queue_size = 32, _tick = 7, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:56:24,447[0m][[34mroot[0m][[32mINFO[0m] - Step 23040 @ 511.5 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 105.2, step = 23040, mean_episode_return = -0.07319, mean_episode_step = 53.805, total_loss = 200.29, entropy_loss = -11.339, pg_loss = 143.11, baseline_loss = 68.513, learner_queue_size = 32, _tick = 8, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:56:29,454[0m][[34mroot[0m][[32mINFO[0m] - Step 25600 @ 511.4 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (train_seconds = 110.2, step = 25600, mean_episode_return = 0.0017501, mean_episode_step = 51.237, total_loss = 1047.8, entropy_loss = -11.358, pg_loss = 817.38, baseline_loss = 241.83, learner_queue_size = 32, _tick = 9, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:56:34,459[0m][[34mroot[0m][[32mINFO[0m] - Step 25600 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 115.2, step = 25600, mean_episode_return = 0.0017501, mean_episode_step = 51.237, total_loss = 1047.8, entropy_loss = -11.358, pg_loss = 817.38, baseline_loss = 241.83, learner_queue_size = 32, _tick = 9, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:56:39,464[0m][[34mroot[0m][[32mINFO[0m] - Step 28160 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 120.2, step = 28160, mean_episode_return = 0.030031, mean_episode_step = 49.439, total_loss = -128.34, entropy_loss = -11.361, pg_loss = -332.59, baseline_loss = 215.62, learner_queue_size = 32, _tick = 10, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:56:44,469[0m][[34mroot[0m][[32mINFO[0m] - Step 30720 @ 511.5 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (train_seconds = 125.2, step = 30720, mean_episode_return = 0.023522, mean_episode_step = 63.948, total_loss = -732.14, entropy_loss = -11.368, pg_loss = -792.92, baseline_loss = 72.145, learner_queue_size = 32, _tick = 11, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:56:49,474[0m][[34mroot[0m][[32mINFO[0m] - Step 30720 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 130.2, step = 30720, mean_episode_return = 0.023522, mean_episode_step = 63.948, total_loss = -732.14, entropy_loss = -11.368, pg_loss = -792.92, baseline_loss = 72.145, learner_queue_size = 32, _tick = 11, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:56:54,479[0m][[34mroot[0m][[32mINFO[0m] - Step 33280 @ 511.5 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (train_seconds = 135.2, step = 33280, mean_episode_return = 0.0041852, mean_episode_step = 45.012, total_loss = -643.94, entropy_loss = -11.37, pg_loss = -680.79, baseline_loss = 48.223, learner_queue_size = 32, _tick = 12, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:56:59,484[0m][[34mroot[0m][[32mINFO[0m] - Step 33280 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 140.2, step = 33280, mean_episode_return = 0.0041852, mean_episode_step = 45.012, total_loss = -643.94, entropy_loss = -11.37, pg_loss = -680.79, baseline_loss = 48.223, learner_queue_size = 32, _tick = 12, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:57:04,489[0m][[34mroot[0m][[32mINFO[0m] - Step 35840 @ 511.5 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 145.2, step = 35840, mean_episode_return = 0.0022105, mean_episode_step = 56.886, total_loss = 399.49, entropy_loss = -11.366, pg_loss = 301.26, baseline_loss = 109.6, learner_queue_size = 32, _tick = 13, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:57:09,494[0m][[34mroot[0m][[32mINFO[0m] - Step 38400 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 150.2, step = 38400, mean_episode_return = -0.065083, mean_episode_step = 60.812, total_loss = -163.2, entropy_loss = -11.368, pg_loss = -218.93, baseline_loss = 67.096, learner_queue_size = 32, _tick = 14, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:57:14,499[0m][[34mroot[0m][[32mINFO[0m] - Step 38400 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 155.2, step = 38400, mean_episode_return = -0.065083, mean_episode_step = 60.812, total_loss = -163.2, entropy_loss = -11.368, pg_loss = -218.93, baseline_loss = 67.096, learner_queue_size = 32, _tick = 14, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:57:19,504[0m][[34mroot[0m][[32mINFO[0m] - Step 40960 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 160.2, step = 40960, mean_episode_return = -0.047905, mean_episode_step = 64.286, total_loss = 1335.2, entropy_loss = -11.359, pg_loss = 1082.6, baseline_loss = 264.05, learner_queue_size = 32, _tick = 15, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:57:24,509[0m][[34mroot[0m][[32mINFO[0m] - Step 43520 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 165.2, step = 43520, mean_episode_return = 0.0014816, mean_episode_step = 60.942, total_loss = 737.52, entropy_loss = -11.354, pg_loss = 469.8, baseline_loss = 279.08, learner_queue_size = 32, _tick = 16, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:57:29,514[0m][[34mroot[0m][[32mINFO[0m] - Step 43520 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 170.2, step = 43520, mean_episode_return = 0.0014816, mean_episode_step = 60.942, total_loss = 737.52, entropy_loss = -11.354, pg_loss = 469.8, baseline_loss = 279.08, learner_queue_size = 32, _tick = 16, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:57:34,519[0m][[34mroot[0m][[32mINFO[0m] - Step 46080 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 175.2, step = 46080, mean_episode_return = -0.064579, mean_episode_step = 72.136, total_loss = 294.63, entropy_loss = -11.309, pg_loss = 84.101, baseline_loss = 221.84, learner_queue_size = 32, _tick = 17, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:57:39,524[0m][[34mroot[0m][[32mINFO[0m] - Step 48640 @ 511.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 180.2, step = 48640, mean_episode_return = 0.021029, mean_episode_step = 55.902, total_loss = -533.78, entropy_loss = -11.211, pg_loss = -576.35, baseline_loss = 53.782, learner_queue_size = 32, _tick = 18, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:57:44,529[0m][[34mroot[0m][[32mINFO[0m] - Step 48640 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 185.2, step = 48640, mean_episode_return = 0.021029, mean_episode_step = 55.902, total_loss = -533.78, entropy_loss = -11.211, pg_loss = -576.35, baseline_loss = 53.782, learner_queue_size = 32, _tick = 18, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:57:49,535[0m][[34mroot[0m][[32mINFO[0m] - Step 51200 @ 511.4 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 190.3, step = 51200, mean_episode_return = 0.06725, mean_episode_step = 69.63, total_loss = 445.43, entropy_loss = -11.19, pg_loss = 336.2, baseline_loss = 120.42, learner_queue_size = 32, _tick = 19, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:57:54,540[0m][[34mroot[0m][[32mINFO[0m] - Step 51200 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 195.3, step = 51200, mean_episode_return = 0.06725, mean_episode_step = 69.63, total_loss = 445.43, entropy_loss = -11.19, pg_loss = 336.2, baseline_loss = 120.42, learner_queue_size = 32, _tick = 19, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:57:59,545[0m][[34mroot[0m][[32mINFO[0m] - Step 53760 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 200.3, step = 53760, mean_episode_return = -0.003724, mean_episode_step = 70.328, total_loss = 96.112, entropy_loss = -11.161, pg_loss = 2.3484, baseline_loss = 104.92, learner_queue_size = 32, _tick = 20, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:58:04,550[0m][[34mroot[0m][[32mINFO[0m] - Step 56320 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 205.3, step = 56320, mean_episode_return = -0.039875, mean_episode_step = 66.772, total_loss = -5.6168, entropy_loss = -11.129, pg_loss = -68.537, baseline_loss = 74.048, learner_queue_size = 32, _tick = 21, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:58:09,555[0m][[34mroot[0m][[32mINFO[0m] - Step 56320 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 210.3, step = 56320, mean_episode_return = -0.039875, mean_episode_step = 66.772, total_loss = -5.6168, entropy_loss = -11.129, pg_loss = -68.537, baseline_loss = 74.048, learner_queue_size = 32, _tick = 21, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:58:14,560[0m][[34mroot[0m][[32mINFO[0m] - Step 58880 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 215.3, step = 58880, mean_episode_return = -0.055813, mean_episode_step = 63.064, total_loss = 688.1, entropy_loss = -10.984, pg_loss = 518.72, baseline_loss = 180.36, learner_queue_size = 32, _tick = 22, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:58:19,576[0m][[34mroot[0m][[32mINFO[0m] - Step 61440 @ 510.3 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 220.3, step = 61440, mean_episode_return = 0.00050008, mean_episode_step = 58.261, total_loss = -359.55, entropy_loss = -10.886, pg_loss = -399.12, baseline_loss = 50.458, learner_queue_size = 32, _tick = 23, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:58:24,581[0m][[34mroot[0m][[32mINFO[0m] - Step 61440 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 225.3, step = 61440, mean_episode_return = 0.00050008, mean_episode_step = 58.261, total_loss = -359.55, entropy_loss = -10.886, pg_loss = -399.12, baseline_loss = 50.458, learner_queue_size = 32, _tick = 23, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:58:29,586[0m][[34mroot[0m][[32mINFO[0m] - Step 64000 @ 511.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 230.3, step = 64000, mean_episode_return = -0.0036249, mean_episode_step = 55.37, total_loss = 446.76, entropy_loss = -10.859, pg_loss = 335.67, baseline_loss = 121.94, learner_queue_size = 32, _tick = 24, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:58:34,591[0m][[34mroot[0m][[32mINFO[0m] - Step 66560 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 235.3, step = 66560, mean_episode_return = 0.0038788, mean_episode_step = 63.886, total_loss = -561.34, entropy_loss = -10.834, pg_loss = -578.55, baseline_loss = 28.041, learner_queue_size = 32, _tick = 25, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:58:39,597[0m][[34mroot[0m][[32mINFO[0m] - Step 66560 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 240.3, step = 66560, mean_episode_return = 0.0038788, mean_episode_step = 63.886, total_loss = -561.34, entropy_loss = -10.834, pg_loss = -578.55, baseline_loss = 28.041, learner_queue_size = 32, _tick = 25, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:58:44,602[0m][[34mroot[0m][[32mINFO[0m] - Step 69120 @ 511.5 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (train_seconds = 245.3, step = 69120, mean_episode_return = -0.014486, mean_episode_step = 60.147, total_loss = -419.85, entropy_loss = -10.817, pg_loss = -447.26, baseline_loss = 38.228, learner_queue_size = 32, _tick = 26, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:58:49,607[0m][[34mroot[0m][[32mINFO[0m] - Step 71680 @ 511.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 250.3, step = 71680, mean_episode_return = 0.15938, mean_episode_step = 57.112, total_loss = -222.35, entropy_loss = -10.757, pg_loss = -230.71, baseline_loss = 19.11, learner_queue_size = 32, _tick = 27, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:58:54,612[0m][[34mroot[0m][[32mINFO[0m] - Step 71680 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 255.3, step = 71680, mean_episode_return = 0.15938, mean_episode_step = 57.112, total_loss = -222.35, entropy_loss = -10.757, pg_loss = -230.71, baseline_loss = 19.11, learner_queue_size = 32, _tick = 27, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:58:59,617[0m][[34mroot[0m][[32mINFO[0m] - Step 74240 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 260.3, step = 74240, mean_episode_return = 0.14684, mean_episode_step = 53.17, total_loss = 551.98, entropy_loss = -10.677, pg_loss = 468.86, baseline_loss = 93.793, learner_queue_size = 32, _tick = 28, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:59:04,622[0m][[34mroot[0m][[32mINFO[0m] - Step 74240 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 265.3, step = 74240, mean_episode_return = 0.14684, mean_episode_step = 53.17, total_loss = 551.98, entropy_loss = -10.677, pg_loss = 468.86, baseline_loss = 93.793, learner_queue_size = 32, _tick = 28, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:59:09,627[0m][[34mroot[0m][[32mINFO[0m] - Step 76800 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 270.3, step = 76800, mean_episode_return = 0.061067, mean_episode_step = 61.116, total_loss = 914.93, entropy_loss = -10.724, pg_loss = 704.98, baseline_loss = 220.67, learner_queue_size = 32, _tick = 29, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:59:14,633[0m][[34mroot[0m][[32mINFO[0m] - Step 79360 @ 511.4 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 275.4, step = 79360, mean_episode_return = 0.050618, mean_episode_step = 54.632, total_loss = 1523.6, entropy_loss = -10.678, pg_loss = 1225.4, baseline_loss = 308.87, learner_queue_size = 32, _tick = 30, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:59:19,638[0m][[34mroot[0m][[32mINFO[0m] - Step 79360 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 280.4, step = 79360, mean_episode_return = 0.050618, mean_episode_step = 54.632, total_loss = 1523.6, entropy_loss = -10.678, pg_loss = 1225.4, baseline_loss = 308.87, learner_queue_size = 32, _tick = 30, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:59:24,643[0m][[34mroot[0m][[32mINFO[0m] - Step 81920 @ 511.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (train_seconds = 285.4, step = 81920, mean_episode_return = 0.030333, mean_episode_step = 43.877, total_loss = -87.757, entropy_loss = -10.638, pg_loss = -177.44, baseline_loss = 100.32, learner_queue_size = 32, _tick = 31, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:59:29,648[0m][[34mroot[0m][[32mINFO[0m] - Step 84480 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 290.4, step = 84480, mean_episode_return = 0.034231, mean_episode_step = 56.664, total_loss = -144.4, entropy_loss = -10.534, pg_loss = -201.96, baseline_loss = 68.094, learner_queue_size = 32, _tick = 32, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:59:34,653[0m][[34mroot[0m][[32mINFO[0m] - Step 84480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 295.4, step = 84480, mean_episode_return = 0.034231, mean_episode_step = 56.664, total_loss = -144.4, entropy_loss = -10.534, pg_loss = -201.96, baseline_loss = 68.094, learner_queue_size = 32, _tick = 32, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:59:39,658[0m][[34mroot[0m][[32mINFO[0m] - Step 87040 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 300.4, step = 87040, mean_episode_return = 0.077, mean_episode_step = 60.69, total_loss = 863.64, entropy_loss = -10.483, pg_loss = 709.0, baseline_loss = 165.12, learner_queue_size = 32, _tick = 33, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:59:44,663[0m][[34mroot[0m][[32mINFO[0m] - Step 89600 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 305.4, step = 89600, mean_episode_return = 0.02535, mean_episode_step = 47.898, total_loss = 449.46, entropy_loss = -10.513, pg_loss = 246.24, baseline_loss = 213.74, learner_queue_size = 32, _tick = 34, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:59:49,668[0m][[34mroot[0m][[32mINFO[0m] - Step 89600 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 310.4, step = 89600, mean_episode_return = 0.02535, mean_episode_step = 47.898, total_loss = 449.46, entropy_loss = -10.513, pg_loss = 246.24, baseline_loss = 213.74, learner_queue_size = 32, _tick = 34, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:59:54,674[0m][[34mroot[0m][[32mINFO[0m] - Step 92160 @ 511.4 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 315.4, step = 92160, mean_episode_return = -0.03231, mean_episode_step = 53.733, total_loss = 334.8, entropy_loss = -10.517, pg_loss = 205.33, baseline_loss = 139.98, learner_queue_size = 32, _tick = 35, _time = 1.7374e+09)[0m
[[36m2025-01-20 12:59:59,679[0m][[34mroot[0m][[32mINFO[0m] - Step 94720 @ 511.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 320.4, step = 94720, mean_episode_return = 0.10441, mean_episode_step = 53.921, total_loss = 944.88, entropy_loss = -10.326, pg_loss = 720.25, baseline_loss = 234.95, learner_queue_size = 32, _tick = 36, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:00:04,685[0m][[34mroot[0m][[32mINFO[0m] - Step 94720 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 325.4, step = 94720, mean_episode_return = 0.10441, mean_episode_step = 53.921, total_loss = 944.88, entropy_loss = -10.326, pg_loss = 720.25, baseline_loss = 234.95, learner_queue_size = 32, _tick = 36, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:00:09,690[0m][[34mroot[0m][[32mINFO[0m] - Step 97280 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 330.4, step = 97280, mean_episode_return = 0.10351, mean_episode_step = 60.08, total_loss = 238.21, entropy_loss = -10.213, pg_loss = 45.932, baseline_loss = 202.49, learner_queue_size = 32, _tick = 37, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:00:14,695[0m][[34mroot[0m][[32mINFO[0m] - Step 99840 @ 511.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 335.4, step = 99840, mean_episode_return = 0.061781, mean_episode_step = 62.217, total_loss = 333.15, entropy_loss = -10.177, pg_loss = 195.74, baseline_loss = 147.59, learner_queue_size = 32, _tick = 38, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:00:19,700[0m][[34mroot[0m][[32mINFO[0m] - Step 99840 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 340.4, step = 99840, mean_episode_return = 0.061781, mean_episode_step = 62.217, total_loss = 333.15, entropy_loss = -10.177, pg_loss = 195.74, baseline_loss = 147.59, learner_queue_size = 32, _tick = 38, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:00:24,707[0m][[34mroot[0m][[32mINFO[0m] - Step 102400 @ 511.3 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 345.4, step = 102400, mean_episode_return = 0.0735, mean_episode_step = 53.988, total_loss = -501.79, entropy_loss = -10.14, pg_loss = -609.26, baseline_loss = 117.61, learner_queue_size = 32, _tick = 39, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:00:29,712[0m][[34mroot[0m][[32mINFO[0m] - Step 102400 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 350.4, step = 102400, mean_episode_return = 0.0735, mean_episode_step = 53.988, total_loss = -501.79, entropy_loss = -10.14, pg_loss = -609.26, baseline_loss = 117.61, learner_queue_size = 32, _tick = 39, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:00:34,717[0m][[34mroot[0m][[32mINFO[0m] - Step 104960 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 355.4, step = 104960, mean_episode_return = 0.025869, mean_episode_step = 53.421, total_loss = -535.69, entropy_loss = -10.225, pg_loss = -626.69, baseline_loss = 101.22, learner_queue_size = 32, _tick = 40, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:00:39,722[0m][[34mroot[0m][[32mINFO[0m] - Step 107520 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 360.4, step = 107520, mean_episode_return = 0.083, mean_episode_step = 54.355, total_loss = -754.88, entropy_loss = -10.21, pg_loss = -796.03, baseline_loss = 51.355, learner_queue_size = 32, _tick = 41, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:00:44,727[0m][[34mroot[0m][[32mINFO[0m] - Step 107520 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 365.4, step = 107520, mean_episode_return = 0.083, mean_episode_step = 54.355, total_loss = -754.88, entropy_loss = -10.21, pg_loss = -796.03, baseline_loss = 51.355, learner_queue_size = 32, _tick = 41, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:00:49,732[0m][[34mroot[0m][[32mINFO[0m] - Step 110080 @ 511.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (train_seconds = 370.5, step = 110080, mean_episode_return = 0.062075, mean_episode_step = 55.592, total_loss = 79.119, entropy_loss = -10.207, pg_loss = -29.196, baseline_loss = 118.52, learner_queue_size = 32, _tick = 42, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:00:54,737[0m][[34mroot[0m][[32mINFO[0m] - Step 112640 @ 511.5 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (train_seconds = 375.5, step = 112640, mean_episode_return = 0.18066, mean_episode_step = 60.772, total_loss = 336.57, entropy_loss = -10.041, pg_loss = 238.23, baseline_loss = 108.38, learner_queue_size = 32, _tick = 43, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:00:59,743[0m][[34mroot[0m][[32mINFO[0m] - Step 112640 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 380.5, step = 112640, mean_episode_return = 0.18066, mean_episode_step = 60.772, total_loss = 336.57, entropy_loss = -10.041, pg_loss = 238.23, baseline_loss = 108.38, learner_queue_size = 32, _tick = 43, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:01:04,748[0m][[34mroot[0m][[32mINFO[0m] - Step 115200 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 385.5, step = 115200, mean_episode_return = 0.16117, mean_episode_step = 60.621, total_loss = 806.61, entropy_loss = -10.035, pg_loss = 646.43, baseline_loss = 170.22, learner_queue_size = 32, _tick = 44, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:01:09,754[0m][[34mroot[0m][[32mINFO[0m] - Step 117760 @ 511.4 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (train_seconds = 390.5, step = 117760, mean_episode_return = 0.20595, mean_episode_step = 66.103, total_loss = -388.77, entropy_loss = -9.9984, pg_loss = -480.21, baseline_loss = 101.44, learner_queue_size = 32, _tick = 45, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:01:14,759[0m][[34mroot[0m][[32mINFO[0m] - Step 117760 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 395.5, step = 117760, mean_episode_return = 0.20595, mean_episode_step = 66.103, total_loss = -388.77, entropy_loss = -9.9984, pg_loss = -480.21, baseline_loss = 101.44, learner_queue_size = 32, _tick = 45, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:01:19,764[0m][[34mroot[0m][[32mINFO[0m] - Step 120320 @ 511.5 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (train_seconds = 400.5, step = 120320, mean_episode_return = 0.18138, mean_episode_step = 52.784, total_loss = 439.28, entropy_loss = -9.9675, pg_loss = 328.47, baseline_loss = 120.78, learner_queue_size = 32, _tick = 46, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:01:24,769[0m][[34mroot[0m][[32mINFO[0m] - Step 122880 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 405.5, step = 122880, mean_episode_return = 0.05879, mean_episode_step = 50.881, total_loss = -776.0, entropy_loss = -9.9039, pg_loss = -833.92, baseline_loss = 67.823, learner_queue_size = 32, _tick = 47, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:01:29,775[0m][[34mroot[0m][[32mINFO[0m] - Step 122880 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 410.5, step = 122880, mean_episode_return = 0.05879, mean_episode_step = 50.881, total_loss = -776.0, entropy_loss = -9.9039, pg_loss = -833.92, baseline_loss = 67.823, learner_queue_size = 32, _tick = 47, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:01:34,780[0m][[34mroot[0m][[32mINFO[0m] - Saving checkpoint to /opt/minihack/checkpoint_0.25.tar[0m
[[36m2025-01-20 13:01:34,825[0m][[34mroot[0m][[32mINFO[0m] - Step 125440 @ 511.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (train_seconds = 415.5, step = 125440, mean_episode_return = 0.1031, mean_episode_step = 54.272, total_loss = -84.651, entropy_loss = -9.8995, pg_loss = -204.85, baseline_loss = 130.1, learner_queue_size = 32, _tick = 48, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:01:39,830[0m][[34mroot[0m][[32mINFO[0m] - Step 128000 @ 506.9 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 420.5, step = 128000, mean_episode_return = 0.25935, mean_episode_step = 72.162, total_loss = 184.44, entropy_loss = -9.8504, pg_loss = 77.772, baseline_loss = 116.52, learner_queue_size = 32, _tick = 49, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:01:44,836[0m][[34mroot[0m][[32mINFO[0m] - Step 128000 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 425.6, step = 128000, mean_episode_return = 0.25935, mean_episode_step = 72.162, total_loss = 184.44, entropy_loss = -9.8504, pg_loss = 77.772, baseline_loss = 116.52, learner_queue_size = 32, _tick = 49, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:01:49,849[0m][[34mroot[0m][[32mINFO[0m] - Step 130560 @ 510.7 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 430.6, step = 130560, mean_episode_return = 0.23168, mean_episode_step = 68.667, total_loss = 598.05, entropy_loss = -9.6872, pg_loss = 473.58, baseline_loss = 134.16, learner_queue_size = 32, _tick = 50, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:01:54,854[0m][[34mroot[0m][[32mINFO[0m] - Step 130560 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 435.6, step = 130560, mean_episode_return = 0.23168, mean_episode_step = 68.667, total_loss = 598.05, entropy_loss = -9.6872, pg_loss = 473.58, baseline_loss = 134.16, learner_queue_size = 32, _tick = 50, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:01:59,859[0m][[34mroot[0m][[32mINFO[0m] - Step 133120 @ 511.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (train_seconds = 440.6, step = 133120, mean_episode_return = 0.2635, mean_episode_step = 61.042, total_loss = -121.0, entropy_loss = -9.7223, pg_loss = -225.28, baseline_loss = 114.0, learner_queue_size = 32, _tick = 51, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:02:04,866[0m][[34mroot[0m][[32mINFO[0m] - Step 135680 @ 511.3 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 445.6, step = 135680, mean_episode_return = 0.1581, mean_episode_step = 66.994, total_loss = 824.93, entropy_loss = -9.6674, pg_loss = 676.6, baseline_loss = 158.0, learner_queue_size = 32, _tick = 52, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:02:09,871[0m][[34mroot[0m][[32mINFO[0m] - Step 135680 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 450.6, step = 135680, mean_episode_return = 0.1581, mean_episode_step = 66.994, total_loss = 824.93, entropy_loss = -9.6674, pg_loss = 676.6, baseline_loss = 158.0, learner_queue_size = 32, _tick = 52, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:02:14,877[0m][[34mroot[0m][[32mINFO[0m] - Step 138240 @ 511.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (train_seconds = 455.6, step = 138240, mean_episode_return = 0.14706, mean_episode_step = 58.273, total_loss = -739.84, entropy_loss = -9.6441, pg_loss = -823.13, baseline_loss = 92.942, learner_queue_size = 32, _tick = 53, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:02:19,882[0m][[34mroot[0m][[32mINFO[0m] - Step 140800 @ 511.4 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (train_seconds = 460.6, step = 140800, mean_episode_return = 0.24621, mean_episode_step = 74.08, total_loss = -52.649, entropy_loss = -9.5666, pg_loss = -162.03, baseline_loss = 118.95, learner_queue_size = 32, _tick = 54, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:02:24,887[0m][[34mroot[0m][[32mINFO[0m] - Step 140800 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 465.6, step = 140800, mean_episode_return = 0.24621, mean_episode_step = 74.08, total_loss = -52.649, entropy_loss = -9.5666, pg_loss = -162.03, baseline_loss = 118.95, learner_queue_size = 32, _tick = 54, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:02:29,892[0m][[34mroot[0m][[32mINFO[0m] - Step 143360 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 470.6, step = 143360, mean_episode_return = 0.15625, mean_episode_step = 63.452, total_loss = 139.81, entropy_loss = -9.3946, pg_loss = 71.131, baseline_loss = 78.076, learner_queue_size = 32, _tick = 55, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:02:34,897[0m][[34mroot[0m][[32mINFO[0m] - Step 145920 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 475.6, step = 145920, mean_episode_return = 0.082549, mean_episode_step = 57.958, total_loss = 315.81, entropy_loss = -9.4142, pg_loss = 187.56, baseline_loss = 137.67, learner_queue_size = 32, _tick = 56, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:02:39,902[0m][[34mroot[0m][[32mINFO[0m] - Step 145920 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 480.6, step = 145920, mean_episode_return = 0.082549, mean_episode_step = 57.958, total_loss = 315.81, entropy_loss = -9.4142, pg_loss = 187.56, baseline_loss = 137.67, learner_queue_size = 32, _tick = 56, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:02:44,908[0m][[34mroot[0m][[32mINFO[0m] - Step 148480 @ 511.4 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 485.6, step = 148480, mean_episode_return = 0.43215, mean_episode_step = 75.166, total_loss = 258.53, entropy_loss = -9.4959, pg_loss = 154.58, baseline_loss = 113.44, learner_queue_size = 32, _tick = 57, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:02:49,913[0m][[34mroot[0m][[32mINFO[0m] - Step 151040 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 490.6, step = 151040, mean_episode_return = 0.19138, mean_episode_step = 62.167, total_loss = -370.35, entropy_loss = -9.3946, pg_loss = -440.27, baseline_loss = 79.314, learner_queue_size = 32, _tick = 58, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:02:54,918[0m][[34mroot[0m][[32mINFO[0m] - Step 151040 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 495.6, step = 151040, mean_episode_return = 0.19138, mean_episode_step = 62.167, total_loss = -370.35, entropy_loss = -9.3946, pg_loss = -440.27, baseline_loss = 79.314, learner_queue_size = 32, _tick = 58, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:02:59,923[0m][[34mroot[0m][[32mINFO[0m] - Step 153600 @ 511.5 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (train_seconds = 500.6, step = 153600, mean_episode_return = 0.29696, mean_episode_step = 65.416, total_loss = -87.951, entropy_loss = -9.3169, pg_loss = -147.79, baseline_loss = 69.156, learner_queue_size = 32, _tick = 59, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:03:04,928[0m][[34mroot[0m][[32mINFO[0m] - Step 153600 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 505.6, step = 153600, mean_episode_return = 0.29696, mean_episode_step = 65.416, total_loss = -87.951, entropy_loss = -9.3169, pg_loss = -147.79, baseline_loss = 69.156, learner_queue_size = 32, _tick = 59, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:03:09,933[0m][[34mroot[0m][[32mINFO[0m] - Step 156160 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 510.7, step = 156160, mean_episode_return = 0.30616, mean_episode_step = 69.38, total_loss = 22.865, entropy_loss = -9.371, pg_loss = -53.467, baseline_loss = 85.703, learner_queue_size = 32, _tick = 60, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:03:14,939[0m][[34mroot[0m][[32mINFO[0m] - Step 158720 @ 511.4 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 515.7, step = 158720, mean_episode_return = 0.29861, mean_episode_step = 75.462, total_loss = 457.15, entropy_loss = -9.5346, pg_loss = 344.23, baseline_loss = 122.46, learner_queue_size = 32, _tick = 61, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:03:19,944[0m][[34mroot[0m][[32mINFO[0m] - Step 158720 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 520.7, step = 158720, mean_episode_return = 0.29861, mean_episode_step = 75.462, total_loss = 457.15, entropy_loss = -9.5346, pg_loss = 344.23, baseline_loss = 122.46, learner_queue_size = 32, _tick = 61, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:03:24,949[0m][[34mroot[0m][[32mINFO[0m] - Step 161280 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 525.7, step = 161280, mean_episode_return = 0.24162, mean_episode_step = 55.206, total_loss = -54.651, entropy_loss = -9.478, pg_loss = -143.21, baseline_loss = 98.041, learner_queue_size = 32, _tick = 62, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:03:29,954[0m][[34mroot[0m][[32mINFO[0m] - Step 163840 @ 511.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 530.7, step = 163840, mean_episode_return = 0.36218, mean_episode_step = 74.655, total_loss = -260.87, entropy_loss = -9.4341, pg_loss = -335.28, baseline_loss = 83.837, learner_queue_size = 32, _tick = 63, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:03:34,959[0m][[34mroot[0m][[32mINFO[0m] - Step 163840 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 535.7, step = 163840, mean_episode_return = 0.36218, mean_episode_step = 74.655, total_loss = -260.87, entropy_loss = -9.4341, pg_loss = -335.28, baseline_loss = 83.837, learner_queue_size = 32, _tick = 63, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:03:39,966[0m][[34mroot[0m][[32mINFO[0m] - Step 166400 @ 511.3 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 540.7, step = 166400, mean_episode_return = 0.29816, mean_episode_step = 64.363, total_loss = 96.972, entropy_loss = -9.3591, pg_loss = 20.863, baseline_loss = 85.468, learner_queue_size = 32, _tick = 64, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:03:44,974[0m][[34mroot[0m][[32mINFO[0m] - Step 168960 @ 511.2 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 545.7, step = 168960, mean_episode_return = 0.28246, mean_episode_step = 56.938, total_loss = 324.57, entropy_loss = -9.1382, pg_loss = 223.44, baseline_loss = 110.28, learner_queue_size = 32, _tick = 65, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:03:49,979[0m][[34mroot[0m][[32mINFO[0m] - Step 168960 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 550.7, step = 168960, mean_episode_return = 0.28246, mean_episode_step = 56.938, total_loss = 324.57, entropy_loss = -9.1382, pg_loss = 223.44, baseline_loss = 110.28, learner_queue_size = 32, _tick = 65, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:03:54,984[0m][[34mroot[0m][[32mINFO[0m] - Step 171520 @ 511.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 555.7, step = 171520, mean_episode_return = 0.22181, mean_episode_step = 62.524, total_loss = 602.38, entropy_loss = -9.1314, pg_loss = 491.02, baseline_loss = 120.5, learner_queue_size = 32, _tick = 66, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:03:59,989[0m][[34mroot[0m][[32mINFO[0m] - Step 171520 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 560.7, step = 171520, mean_episode_return = 0.22181, mean_episode_step = 62.524, total_loss = 602.38, entropy_loss = -9.1314, pg_loss = 491.02, baseline_loss = 120.5, learner_queue_size = 32, _tick = 66, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:04:04,994[0m][[34mroot[0m][[32mINFO[0m] - Step 174080 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 565.7, step = 174080, mean_episode_return = 0.17267, mean_episode_step = 56.986, total_loss = -330.05, entropy_loss = -9.1038, pg_loss = -408.5, baseline_loss = 87.555, learner_queue_size = 32, _tick = 67, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:04:10,000[0m][[34mroot[0m][[32mINFO[0m] - Step 176640 @ 511.4 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (train_seconds = 570.7, step = 176640, mean_episode_return = 0.493, mean_episode_step = 69.535, total_loss = 749.35, entropy_loss = -8.9835, pg_loss = 636.74, baseline_loss = 121.59, learner_queue_size = 32, _tick = 68, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:04:15,005[0m][[34mroot[0m][[32mINFO[0m] - Step 176640 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 575.7, step = 176640, mean_episode_return = 0.493, mean_episode_step = 69.535, total_loss = 749.35, entropy_loss = -8.9835, pg_loss = 636.74, baseline_loss = 121.59, learner_queue_size = 32, _tick = 68, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:04:20,011[0m][[34mroot[0m][[32mINFO[0m] - Step 179200 @ 511.4 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 580.7, step = 179200, mean_episode_return = 0.43618, mean_episode_step = 79.246, total_loss = 53.823, entropy_loss = -9.0403, pg_loss = -28.34, baseline_loss = 91.204, learner_queue_size = 32, _tick = 69, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:04:25,016[0m][[34mroot[0m][[32mINFO[0m] - Step 181760 @ 511.4 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (train_seconds = 585.7, step = 181760, mean_episode_return = 0.30106, mean_episode_step = 61.194, total_loss = 8.656, entropy_loss = -8.879, pg_loss = -74.172, baseline_loss = 91.707, learner_queue_size = 32, _tick = 70, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:04:30,022[0m][[34mroot[0m][[32mINFO[0m] - Step 181760 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 590.7, step = 181760, mean_episode_return = 0.30106, mean_episode_step = 61.194, total_loss = 8.656, entropy_loss = -8.879, pg_loss = -74.172, baseline_loss = 91.707, learner_queue_size = 32, _tick = 70, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:04:35,029[0m][[34mroot[0m][[32mINFO[0m] - Step 184320 @ 511.3 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 595.7, step = 184320, mean_episode_return = 0.28617, mean_episode_step = 62.739, total_loss = 76.232, entropy_loss = -8.9644, pg_loss = 3.1841, baseline_loss = 82.012, learner_queue_size = 32, _tick = 71, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:04:40,035[0m][[34mroot[0m][[32mINFO[0m] - Saving checkpoint to /opt/minihack/checkpoint.tar[0m
[[36m2025-01-20 13:04:40,067[0m][[34mroot[0m][[32mINFO[0m] - Step 186880 @ 511.3 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (train_seconds = 600.8, step = 186880, mean_episode_return = 0.24519, mean_episode_step = 67.457, total_loss = -205.42, entropy_loss = -8.9766, pg_loss = -300.13, baseline_loss = 103.69, learner_queue_size = 32, _tick = 72, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:04:45,073[0m][[34mroot[0m][[32mINFO[0m] - Step 186880 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 605.8, step = 186880, mean_episode_return = 0.24519, mean_episode_step = 67.457, total_loss = -205.42, entropy_loss = -8.9766, pg_loss = -300.13, baseline_loss = 103.69, learner_queue_size = 32, _tick = 72, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:04:50,079[0m][[34mroot[0m][[32mINFO[0m] - Step 189440 @ 511.4 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 610.8, step = 189440, mean_episode_return = 0.2653, mean_episode_step = 76.316, total_loss = -199.58, entropy_loss = -9.0959, pg_loss = -261.48, baseline_loss = 70.99, learner_queue_size = 32, _tick = 73, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:04:55,085[0m][[34mroot[0m][[32mINFO[0m] - Step 192000 @ 511.4 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 615.8, step = 192000, mean_episode_return = 0.095786, mean_episode_step = 56.265, total_loss = 321.7, entropy_loss = -9.051, pg_loss = 240.88, baseline_loss = 89.874, learner_queue_size = 32, _tick = 74, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:05:00,090[0m][[34mroot[0m][[32mINFO[0m] - Step 192000 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 620.8, step = 192000, mean_episode_return = 0.095786, mean_episode_step = 56.265, total_loss = 321.7, entropy_loss = -9.051, pg_loss = 240.88, baseline_loss = 89.874, learner_queue_size = 32, _tick = 74, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:05:05,095[0m][[34mroot[0m][[32mINFO[0m] - Step 194560 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 625.8, step = 194560, mean_episode_return = 0.39891, mean_episode_step = 63.579, total_loss = 216.25, entropy_loss = -9.053, pg_loss = 108.3, baseline_loss = 117.0, learner_queue_size = 32, _tick = 75, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:05:10,100[0m][[34mroot[0m][[32mINFO[0m] - Step 197120 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 630.8, step = 197120, mean_episode_return = 0.34771, mean_episode_step = 74.884, total_loss = -244.69, entropy_loss = -8.9857, pg_loss = -331.93, baseline_loss = 96.218, learner_queue_size = 32, _tick = 76, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:05:15,105[0m][[34mroot[0m][[32mINFO[0m] - Step 197120 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 635.8, step = 197120, mean_episode_return = 0.34771, mean_episode_step = 74.884, total_loss = -244.69, entropy_loss = -8.9857, pg_loss = -331.93, baseline_loss = 96.218, learner_queue_size = 32, _tick = 76, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:05:20,110[0m][[34mroot[0m][[32mINFO[0m] - Step 199680 @ 511.5 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 640.8, step = 199680, mean_episode_return = 0.29262, mean_episode_step = 73.22, total_loss = -41.133, entropy_loss = -8.9097, pg_loss = -111.21, baseline_loss = 78.99, learner_queue_size = 32, _tick = 77, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:05:25,115[0m][[34mroot[0m][[32mINFO[0m] - Step 202240 @ 511.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 645.8, step = 202240, mean_episode_return = 0.43409, mean_episode_step = 75.92, total_loss = 464.54, entropy_loss = -8.9178, pg_loss = 373.6, baseline_loss = 99.853, learner_queue_size = 32, _tick = 78, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:05:30,120[0m][[34mroot[0m][[32mINFO[0m] - Step 202240 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 650.8, step = 202240, mean_episode_return = 0.43409, mean_episode_step = 75.92, total_loss = 464.54, entropy_loss = -8.9178, pg_loss = 373.6, baseline_loss = 99.853, learner_queue_size = 32, _tick = 78, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:05:35,126[0m][[34mroot[0m][[32mINFO[0m] - Step 204800 @ 511.4 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 655.8, step = 204800, mean_episode_return = 0.29968, mean_episode_step = 67.494, total_loss = -354.04, entropy_loss = -8.9851, pg_loss = -412.95, baseline_loss = 67.896, learner_queue_size = 32, _tick = 79, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:05:40,132[0m][[34mroot[0m][[32mINFO[0m] - Step 204800 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 660.9, step = 204800, mean_episode_return = 0.29968, mean_episode_step = 67.494, total_loss = -354.04, entropy_loss = -8.9851, pg_loss = -412.95, baseline_loss = 67.896, learner_queue_size = 32, _tick = 79, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:05:45,137[0m][[34mroot[0m][[32mINFO[0m] - Step 207360 @ 511.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 665.9, step = 207360, mean_episode_return = 0.43104, mean_episode_step = 59.704, total_loss = -621.75, entropy_loss = -8.9737, pg_loss = -675.71, baseline_loss = 62.941, learner_queue_size = 32, _tick = 80, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:05:50,143[0m][[34mroot[0m][[32mINFO[0m] - Step 209920 @ 511.4 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (train_seconds = 670.9, step = 209920, mean_episode_return = 0.46451, mean_episode_step = 66.68, total_loss = -32.993, entropy_loss = -8.9503, pg_loss = -91.402, baseline_loss = 67.359, learner_queue_size = 32, _tick = 81, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:05:55,149[0m][[34mroot[0m][[32mINFO[0m] - Step 209920 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 675.9, step = 209920, mean_episode_return = 0.46451, mean_episode_step = 66.68, total_loss = -32.993, entropy_loss = -8.9503, pg_loss = -91.402, baseline_loss = 67.359, learner_queue_size = 32, _tick = 81, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:06:00,154[0m][[34mroot[0m][[32mINFO[0m] - Step 212480 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 680.9, step = 212480, mean_episode_return = 0.23742, mean_episode_step = 60.711, total_loss = 506.12, entropy_loss = -8.9838, pg_loss = 430.25, baseline_loss = 84.849, learner_queue_size = 32, _tick = 82, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:06:05,159[0m][[34mroot[0m][[32mINFO[0m] - Step 215040 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 685.9, step = 215040, mean_episode_return = 0.51333, mean_episode_step = 81.692, total_loss = -209.37, entropy_loss = -9.0437, pg_loss = -253.88, baseline_loss = 53.556, learner_queue_size = 32, _tick = 83, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:06:10,161[0m][[34mroot[0m][[32mINFO[0m] - Step 215040 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 690.9, step = 215040, mean_episode_return = 0.51333, mean_episode_step = 81.692, total_loss = -209.37, entropy_loss = -9.0437, pg_loss = -253.88, baseline_loss = 53.556, learner_queue_size = 32, _tick = 83, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:06:15,166[0m][[34mroot[0m][[32mINFO[0m] - Step 217600 @ 511.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (train_seconds = 695.9, step = 217600, mean_episode_return = 0.34291, mean_episode_step = 71.558, total_loss = 379.88, entropy_loss = -8.9828, pg_loss = 310.13, baseline_loss = 78.73, learner_queue_size = 32, _tick = 84, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:06:20,174[0m][[34mroot[0m][[32mINFO[0m] - Step 220160 @ 511.2 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 700.9, step = 220160, mean_episode_return = 0.139, mean_episode_step = 57.125, total_loss = -104.76, entropy_loss = -9.0158, pg_loss = -187.99, baseline_loss = 92.242, learner_queue_size = 32, _tick = 85, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:06:25,179[0m][[34mroot[0m][[32mINFO[0m] - Step 220160 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 705.9, step = 220160, mean_episode_return = 0.139, mean_episode_step = 57.125, total_loss = -104.76, entropy_loss = -9.0158, pg_loss = -187.99, baseline_loss = 92.242, learner_queue_size = 32, _tick = 85, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:06:30,184[0m][[34mroot[0m][[32mINFO[0m] - Step 222720 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 710.9, step = 222720, mean_episode_return = 0.35289, mean_episode_step = 60.114, total_loss = 420.54, entropy_loss = -8.9665, pg_loss = 313.27, baseline_loss = 116.23, learner_queue_size = 32, _tick = 86, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:06:35,189[0m][[34mroot[0m][[32mINFO[0m] - Step 225280 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 715.9, step = 225280, mean_episode_return = 0.34833, mean_episode_step = 77.101, total_loss = -366.64, entropy_loss = -8.9717, pg_loss = -426.56, baseline_loss = 68.885, learner_queue_size = 32, _tick = 87, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:06:40,194[0m][[34mroot[0m][[32mINFO[0m] - Step 225280 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 720.9, step = 225280, mean_episode_return = 0.34833, mean_episode_step = 77.101, total_loss = -366.64, entropy_loss = -8.9717, pg_loss = -426.56, baseline_loss = 68.885, learner_queue_size = 32, _tick = 87, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:06:45,199[0m][[34mroot[0m][[32mINFO[0m] - Step 227840 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 725.9, step = 227840, mean_episode_return = 0.30989, mean_episode_step = 78.935, total_loss = -495.59, entropy_loss = -8.9891, pg_loss = -545.25, baseline_loss = 58.653, learner_queue_size = 32, _tick = 88, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:06:50,204[0m][[34mroot[0m][[32mINFO[0m] - Step 230400 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 730.9, step = 230400, mean_episode_return = 0.35121, mean_episode_step = 61.877, total_loss = 462.76, entropy_loss = -9.012, pg_loss = 340.23, baseline_loss = 131.55, learner_queue_size = 32, _tick = 89, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:06:55,210[0m][[34mroot[0m][[32mINFO[0m] - Step 230400 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 735.9, step = 230400, mean_episode_return = 0.35121, mean_episode_step = 61.877, total_loss = 462.76, entropy_loss = -9.012, pg_loss = 340.23, baseline_loss = 131.55, learner_queue_size = 32, _tick = 89, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:07:00,216[0m][[34mroot[0m][[32mINFO[0m] - Step 232960 @ 511.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 740.9, step = 232960, mean_episode_return = 0.29346, mean_episode_step = 62.045, total_loss = 984.85, entropy_loss = -8.9809, pg_loss = 814.44, baseline_loss = 179.39, learner_queue_size = 32, _tick = 90, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:07:05,221[0m][[34mroot[0m][[32mINFO[0m] - Step 235520 @ 511.5 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (train_seconds = 745.9, step = 235520, mean_episode_return = 0.36862, mean_episode_step = 59.102, total_loss = -846.12, entropy_loss = -9.0214, pg_loss = -952.63, baseline_loss = 115.53, learner_queue_size = 32, _tick = 91, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:07:10,228[0m][[34mroot[0m][[32mINFO[0m] - Step 235520 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 750.9, step = 235520, mean_episode_return = 0.36862, mean_episode_step = 59.102, total_loss = -846.12, entropy_loss = -9.0214, pg_loss = -952.63, baseline_loss = 115.53, learner_queue_size = 32, _tick = 91, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:07:15,233[0m][[34mroot[0m][[32mINFO[0m] - Step 238080 @ 511.5 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 756.0, step = 238080, mean_episode_return = 0.33179, mean_episode_step = 65.75, total_loss = 1023.5, entropy_loss = -8.976, pg_loss = 862.91, baseline_loss = 169.58, learner_queue_size = 32, _tick = 92, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:07:20,240[0m][[34mroot[0m][[32mINFO[0m] - Step 240640 @ 511.3 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 761.0, step = 240640, mean_episode_return = 0.32241, mean_episode_step = 68.271, total_loss = 52.001, entropy_loss = -8.9969, pg_loss = -90.553, baseline_loss = 151.55, learner_queue_size = 32, _tick = 93, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:07:25,245[0m][[34mroot[0m][[32mINFO[0m] - Step 240640 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 766.0, step = 240640, mean_episode_return = 0.32241, mean_episode_step = 68.271, total_loss = 52.001, entropy_loss = -8.9969, pg_loss = -90.553, baseline_loss = 151.55, learner_queue_size = 32, _tick = 93, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:07:30,251[0m][[34mroot[0m][[32mINFO[0m] - Step 243200 @ 511.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (train_seconds = 771.0, step = 243200, mean_episode_return = 0.35373, mean_episode_step = 59.713, total_loss = 116.19, entropy_loss = -9.0173, pg_loss = 10.936, baseline_loss = 114.28, learner_queue_size = 32, _tick = 94, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:07:35,258[0m][[34mroot[0m][[32mINFO[0m] - Step 245760 @ 511.3 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 776.0, step = 245760, mean_episode_return = 0.38219, mean_episode_step = 81.114, total_loss = 40.536, entropy_loss = -8.9671, pg_loss = -48.796, baseline_loss = 98.299, learner_queue_size = 32, _tick = 95, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:07:40,263[0m][[34mroot[0m][[32mINFO[0m] - Step 245760 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 781.0, step = 245760, mean_episode_return = 0.38219, mean_episode_step = 81.114, total_loss = 40.536, entropy_loss = -8.9671, pg_loss = -48.796, baseline_loss = 98.299, learner_queue_size = 32, _tick = 95, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:07:45,271[0m][[34mroot[0m][[32mINFO[0m] - Step 248320 @ 511.2 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 786.0, step = 248320, mean_episode_return = 0.56458, mean_episode_step = 80.029, total_loss = 561.16, entropy_loss = -8.8754, pg_loss = 467.81, baseline_loss = 102.22, learner_queue_size = 32, _tick = 96, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:07:50,277[0m][[34mroot[0m][[32mINFO[0m] - Step 248320 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 791.0, step = 248320, mean_episode_return = 0.56458, mean_episode_step = 80.029, total_loss = 561.16, entropy_loss = -8.8754, pg_loss = 467.81, baseline_loss = 102.22, learner_queue_size = 32, _tick = 96, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:07:55,282[0m][[34mroot[0m][[32mINFO[0m] - Saving checkpoint to /opt/minihack/checkpoint_0.5.tar[0m
[[36m2025-01-20 13:07:55,323[0m][[34mroot[0m][[32mINFO[0m] - Step 250880 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 796.0, step = 250880, mean_episode_return = 0.56587, mean_episode_step = 66.578, total_loss = 481.07, entropy_loss = -8.8918, pg_loss = 366.86, baseline_loss = 123.1, learner_queue_size = 32, _tick = 97, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:08:00,328[0m][[34mroot[0m][[32mINFO[0m] - Step 253440 @ 507.3 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 801.0, step = 253440, mean_episode_return = 0.35785, mean_episode_step = 65.918, total_loss = 293.97, entropy_loss = -8.8776, pg_loss = 178.56, baseline_loss = 124.29, learner_queue_size = 32, _tick = 98, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:08:05,333[0m][[34mroot[0m][[32mINFO[0m] - Step 253440 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 806.1, step = 253440, mean_episode_return = 0.35785, mean_episode_step = 65.918, total_loss = 293.97, entropy_loss = -8.8776, pg_loss = 178.56, baseline_loss = 124.29, learner_queue_size = 32, _tick = 98, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:08:10,338[0m][[34mroot[0m][[32mINFO[0m] - Step 256000 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 811.1, step = 256000, mean_episode_return = 0.41081, mean_episode_step = 60.424, total_loss = -151.23, entropy_loss = -8.8827, pg_loss = -257.95, baseline_loss = 115.59, learner_queue_size = 32, _tick = 99, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:08:15,343[0m][[34mroot[0m][[32mINFO[0m] - Step 258560 @ 511.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (train_seconds = 816.1, step = 258560, mean_episode_return = 0.46438, mean_episode_step = 66.891, total_loss = -235.28, entropy_loss = -8.8377, pg_loss = -316.42, baseline_loss = 89.978, learner_queue_size = 32, _tick = 100, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:08:20,348[0m][[34mroot[0m][[32mINFO[0m] - Step 258560 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 821.1, step = 258560, mean_episode_return = 0.46438, mean_episode_step = 66.891, total_loss = -235.28, entropy_loss = -8.8377, pg_loss = -316.42, baseline_loss = 89.978, learner_queue_size = 32, _tick = 100, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:08:25,353[0m][[34mroot[0m][[32mINFO[0m] - Step 261120 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 826.1, step = 261120, mean_episode_return = 0.49172, mean_episode_step = 73.695, total_loss = 157.49, entropy_loss = -8.7441, pg_loss = 65.455, baseline_loss = 100.78, learner_queue_size = 32, _tick = 101, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:08:30,358[0m][[34mroot[0m][[32mINFO[0m] - Step 263680 @ 511.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 831.1, step = 263680, mean_episode_return = 0.47058, mean_episode_step = 80.057, total_loss = -201.92, entropy_loss = -8.7443, pg_loss = -263.3, baseline_loss = 70.129, learner_queue_size = 32, _tick = 102, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:08:35,363[0m][[34mroot[0m][[32mINFO[0m] - Step 263680 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 836.1, step = 263680, mean_episode_return = 0.47058, mean_episode_step = 80.057, total_loss = -201.92, entropy_loss = -8.7443, pg_loss = -263.3, baseline_loss = 70.129, learner_queue_size = 32, _tick = 102, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:08:40,368[0m][[34mroot[0m][[32mINFO[0m] - Step 266240 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 841.1, step = 266240, mean_episode_return = 0.2671, mean_episode_step = 55.086, total_loss = -54.308, entropy_loss = -8.7785, pg_loss = -165.44, baseline_loss = 119.91, learner_queue_size = 32, _tick = 103, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:08:45,373[0m][[34mroot[0m][[32mINFO[0m] - Step 268800 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 846.1, step = 268800, mean_episode_return = 0.31743, mean_episode_step = 62.798, total_loss = -551.51, entropy_loss = -8.7553, pg_loss = -603.06, baseline_loss = 60.304, learner_queue_size = 32, _tick = 104, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:08:50,378[0m][[34mroot[0m][[32mINFO[0m] - Step 268800 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 851.1, step = 268800, mean_episode_return = 0.31743, mean_episode_step = 62.798, total_loss = -551.51, entropy_loss = -8.7553, pg_loss = -603.06, baseline_loss = 60.304, learner_queue_size = 32, _tick = 104, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:08:55,383[0m][[34mroot[0m][[32mINFO[0m] - Step 271360 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 856.1, step = 271360, mean_episode_return = 0.35321, mean_episode_step = 59.489, total_loss = -198.51, entropy_loss = -8.7284, pg_loss = -272.42, baseline_loss = 82.638, learner_queue_size = 32, _tick = 105, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:09:00,388[0m][[34mroot[0m][[32mINFO[0m] - Step 273920 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 861.1, step = 273920, mean_episode_return = 0.38274, mean_episode_step = 76.556, total_loss = 555.4, entropy_loss = -8.6676, pg_loss = 468.93, baseline_loss = 95.129, learner_queue_size = 32, _tick = 106, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:09:05,393[0m][[34mroot[0m][[32mINFO[0m] - Step 273920 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 866.1, step = 273920, mean_episode_return = 0.38274, mean_episode_step = 76.556, total_loss = 555.4, entropy_loss = -8.6676, pg_loss = 468.93, baseline_loss = 95.129, learner_queue_size = 32, _tick = 106, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:09:10,399[0m][[34mroot[0m][[32mINFO[0m] - Step 276480 @ 511.4 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 871.1, step = 276480, mean_episode_return = 0.55486, mean_episode_step = 80.572, total_loss = 254.98, entropy_loss = -8.6449, pg_loss = 173.56, baseline_loss = 90.071, learner_queue_size = 32, _tick = 107, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:09:15,405[0m][[34mroot[0m][[32mINFO[0m] - Step 279040 @ 511.4 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 876.1, step = 279040, mean_episode_return = 0.31908, mean_episode_step = 59.488, total_loss = -210.78, entropy_loss = -8.6774, pg_loss = -291.72, baseline_loss = 89.612, learner_queue_size = 32, _tick = 108, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:09:20,410[0m][[34mroot[0m][[32mINFO[0m] - Step 279040 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 881.1, step = 279040, mean_episode_return = 0.31908, mean_episode_step = 59.488, total_loss = -210.78, entropy_loss = -8.6774, pg_loss = -291.72, baseline_loss = 89.612, learner_queue_size = 32, _tick = 108, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:09:25,415[0m][[34mroot[0m][[32mINFO[0m] - Step 281600 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 886.1, step = 281600, mean_episode_return = 0.46624, mean_episode_step = 70.272, total_loss = 938.69, entropy_loss = -8.6252, pg_loss = 829.83, baseline_loss = 117.48, learner_queue_size = 32, _tick = 109, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:09:30,420[0m][[34mroot[0m][[32mINFO[0m] - Step 284160 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 891.1, step = 284160, mean_episode_return = 0.26061, mean_episode_step = 73.842, total_loss = -224.06, entropy_loss = -8.6389, pg_loss = -274.98, baseline_loss = 59.561, learner_queue_size = 32, _tick = 110, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:09:35,426[0m][[34mroot[0m][[32mINFO[0m] - Step 284160 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 896.1, step = 284160, mean_episode_return = 0.26061, mean_episode_step = 73.842, total_loss = -224.06, entropy_loss = -8.6389, pg_loss = -274.98, baseline_loss = 59.561, learner_queue_size = 32, _tick = 110, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:09:40,432[0m][[34mroot[0m][[32mINFO[0m] - Step 286720 @ 511.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 901.2, step = 286720, mean_episode_return = 0.46414, mean_episode_step = 71.773, total_loss = -379.14, entropy_loss = -8.6546, pg_loss = -451.94, baseline_loss = 81.458, learner_queue_size = 32, _tick = 111, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:09:45,437[0m][[34mroot[0m][[32mINFO[0m] - Step 289280 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 906.2, step = 289280, mean_episode_return = 0.33279, mean_episode_step = 63.712, total_loss = 129.85, entropy_loss = -8.6569, pg_loss = 35.777, baseline_loss = 102.73, learner_queue_size = 32, _tick = 112, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:09:51,394[0m][[34mroot[0m][[32mINFO[0m] - Step 289280 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 911.2, step = 289280, mean_episode_return = 0.33279, mean_episode_step = 63.712, total_loss = 129.85, entropy_loss = -8.6569, pg_loss = 35.777, baseline_loss = 102.73, learner_queue_size = 32, _tick = 112, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:09:56,399[0m][[34mroot[0m][[32mINFO[0m] - Step 291840 @ 511.5 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (train_seconds = 916.2, step = 291840, mean_episode_return = 0.60325, mean_episode_step = 80.948, total_loss = 420.01, entropy_loss = -8.6359, pg_loss = 327.47, baseline_loss = 101.17, learner_queue_size = 32, _tick = 113, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:10:01,404[0m][[34mroot[0m][[32mINFO[0m] - Step 294400 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 921.2, step = 294400, mean_episode_return = 0.25458, mean_episode_step = 66.912, total_loss = 514.46, entropy_loss = -8.6426, pg_loss = 399.26, baseline_loss = 123.84, learner_queue_size = 32, _tick = 114, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:10:06,410[0m][[34mroot[0m][[32mINFO[0m] - Step 294400 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 926.2, step = 294400, mean_episode_return = 0.25458, mean_episode_step = 66.912, total_loss = 514.46, entropy_loss = -8.6426, pg_loss = 399.26, baseline_loss = 123.84, learner_queue_size = 32, _tick = 114, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:10:11,416[0m][[34mroot[0m][[32mINFO[0m] - Step 296960 @ 511.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 931.2, step = 296960, mean_episode_return = 0.36038, mean_episode_step = 63.862, total_loss = 406.49, entropy_loss = -8.6545, pg_loss = 298.5, baseline_loss = 116.65, learner_queue_size = 32, _tick = 115, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:10:16,422[0m][[34mroot[0m][[32mINFO[0m] - Step 299520 @ 511.4 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (train_seconds = 936.2, step = 299520, mean_episode_return = 0.35103, mean_episode_step = 66.279, total_loss = 374.98, entropy_loss = -8.6966, pg_loss = 257.58, baseline_loss = 126.09, learner_queue_size = 32, _tick = 116, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:10:21,427[0m][[34mroot[0m][[32mINFO[0m] - Step 299520 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 941.2, step = 299520, mean_episode_return = 0.35103, mean_episode_step = 66.279, total_loss = 374.98, entropy_loss = -8.6966, pg_loss = 257.58, baseline_loss = 126.09, learner_queue_size = 32, _tick = 116, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:10:26,432[0m][[34mroot[0m][[32mINFO[0m] - Step 302080 @ 511.5 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 946.2, step = 302080, mean_episode_return = 0.48236, mean_episode_step = 68.099, total_loss = -452.14, entropy_loss = -8.6936, pg_loss = -533.18, baseline_loss = 89.735, learner_queue_size = 32, _tick = 117, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:10:31,437[0m][[34mroot[0m][[32mINFO[0m] - Step 302080 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 951.2, step = 302080, mean_episode_return = 0.48236, mean_episode_step = 68.099, total_loss = -452.14, entropy_loss = -8.6936, pg_loss = -533.18, baseline_loss = 89.735, learner_queue_size = 32, _tick = 117, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:10:36,442[0m][[34mroot[0m][[32mINFO[0m] - Step 304640 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 956.2, step = 304640, mean_episode_return = 0.57296, mean_episode_step = 71.391, total_loss = 555.17, entropy_loss = -8.6715, pg_loss = 449.27, baseline_loss = 114.57, learner_queue_size = 32, _tick = 118, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:10:41,455[0m][[34mroot[0m][[32mINFO[0m] - Step 307200 @ 510.7 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 961.2, step = 307200, mean_episode_return = 0.36946, mean_episode_step = 77.874, total_loss = -550.9, entropy_loss = -8.6632, pg_loss = -620.53, baseline_loss = 78.302, learner_queue_size = 32, _tick = 119, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:10:46,460[0m][[34mroot[0m][[32mINFO[0m] - Step 307200 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 966.2, step = 307200, mean_episode_return = 0.36946, mean_episode_step = 77.874, total_loss = -550.9, entropy_loss = -8.6632, pg_loss = -620.53, baseline_loss = 78.302, learner_queue_size = 32, _tick = 119, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:10:51,467[0m][[34mroot[0m][[32mINFO[0m] - Step 309760 @ 511.3 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 971.2, step = 309760, mean_episode_return = 0.46726, mean_episode_step = 75.586, total_loss = -389.47, entropy_loss = -8.6711, pg_loss = -478.69, baseline_loss = 97.884, learner_queue_size = 32, _tick = 120, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:10:56,482[0m][[34mroot[0m][[32mINFO[0m] - Step 312320 @ 510.4 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 976.2, step = 312320, mean_episode_return = 0.47021, mean_episode_step = 79.154, total_loss = -127.78, entropy_loss = -8.6778, pg_loss = -266.43, baseline_loss = 147.32, learner_queue_size = 32, _tick = 121, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:11:01,488[0m][[34mroot[0m][[32mINFO[0m] - Step 312320 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 981.3, step = 312320, mean_episode_return = 0.47021, mean_episode_step = 79.154, total_loss = -127.78, entropy_loss = -8.6778, pg_loss = -266.43, baseline_loss = 147.32, learner_queue_size = 32, _tick = 121, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:11:06,493[0m][[34mroot[0m][[32mINFO[0m] - Step 314880 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 986.3, step = 314880, mean_episode_return = 0.37943, mean_episode_step = 75.292, total_loss = -189.88, entropy_loss = -8.61, pg_loss = -251.49, baseline_loss = 70.219, learner_queue_size = 32, _tick = 122, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:11:11,499[0m][[34mroot[0m][[32mINFO[0m] - Step 317440 @ 511.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 991.3, step = 317440, mean_episode_return = 0.6556, mean_episode_step = 66.845, total_loss = -158.8, entropy_loss = -8.6357, pg_loss = -259.25, baseline_loss = 109.08, learner_queue_size = 32, _tick = 123, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:11:16,504[0m][[34mroot[0m][[32mINFO[0m] - Step 317440 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 996.3, step = 317440, mean_episode_return = 0.6556, mean_episode_step = 66.845, total_loss = -158.8, entropy_loss = -8.6357, pg_loss = -259.25, baseline_loss = 109.08, learner_queue_size = 32, _tick = 123, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:11:21,509[0m][[34mroot[0m][[32mINFO[0m] - Step 320000 @ 511.5 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 1001.3, step = 320000, mean_episode_return = 0.58482, mean_episode_step = 65.744, total_loss = 240.87, entropy_loss = -8.648, pg_loss = 137.84, baseline_loss = 111.68, learner_queue_size = 32, _tick = 124, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:11:26,515[0m][[34mroot[0m][[32mINFO[0m] - Step 322560 @ 511.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1006.3, step = 322560, mean_episode_return = 0.38516, mean_episode_step = 77.673, total_loss = -514.41, entropy_loss = -8.6215, pg_loss = -569.35, baseline_loss = 63.563, learner_queue_size = 32, _tick = 125, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:11:31,520[0m][[34mroot[0m][[32mINFO[0m] - Step 322560 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1011.3, step = 322560, mean_episode_return = 0.38516, mean_episode_step = 77.673, total_loss = -514.41, entropy_loss = -8.6215, pg_loss = -569.35, baseline_loss = 63.563, learner_queue_size = 32, _tick = 125, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:11:36,526[0m][[34mroot[0m][[32mINFO[0m] - Step 325120 @ 511.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1016.3, step = 325120, mean_episode_return = 0.39496, mean_episode_step = 74.549, total_loss = 872.81, entropy_loss = -8.6152, pg_loss = 706.49, baseline_loss = 174.93, learner_queue_size = 32, _tick = 126, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:11:41,534[0m][[34mroot[0m][[32mINFO[0m] - Step 327680 @ 511.3 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1021.3, step = 327680, mean_episode_return = 0.58213, mean_episode_step = 86.054, total_loss = 1.7849, entropy_loss = -8.6089, pg_loss = -83.371, baseline_loss = 93.765, learner_queue_size = 32, _tick = 127, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:11:46,539[0m][[34mroot[0m][[32mINFO[0m] - Step 327680 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1026.3, step = 327680, mean_episode_return = 0.58213, mean_episode_step = 86.054, total_loss = 1.7849, entropy_loss = -8.6089, pg_loss = -83.371, baseline_loss = 93.765, learner_queue_size = 32, _tick = 127, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:11:51,545[0m][[34mroot[0m][[32mINFO[0m] - Step 330240 @ 511.4 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 1031.3, step = 330240, mean_episode_return = 0.63687, mean_episode_step = 77.582, total_loss = 976.62, entropy_loss = -8.5866, pg_loss = 840.87, baseline_loss = 144.33, learner_queue_size = 32, _tick = 128, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:11:56,550[0m][[34mroot[0m][[32mINFO[0m] - Step 332800 @ 511.5 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 1036.3, step = 332800, mean_episode_return = 0.50976, mean_episode_step = 75.574, total_loss = 307.83, entropy_loss = -8.618, pg_loss = 175.75, baseline_loss = 140.7, learner_queue_size = 32, _tick = 129, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:12:01,556[0m][[34mroot[0m][[32mINFO[0m] - Step 332800 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1041.3, step = 332800, mean_episode_return = 0.50976, mean_episode_step = 75.574, total_loss = 307.83, entropy_loss = -8.618, pg_loss = 175.75, baseline_loss = 140.7, learner_queue_size = 32, _tick = 129, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:12:06,561[0m][[34mroot[0m][[32mINFO[0m] - Step 335360 @ 511.4 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1046.3, step = 335360, mean_episode_return = 0.55804, mean_episode_step = 76.968, total_loss = -31.653, entropy_loss = -8.606, pg_loss = -104.43, baseline_loss = 81.385, learner_queue_size = 32, _tick = 130, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:12:11,566[0m][[34mroot[0m][[32mINFO[0m] - Step 335360 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1051.3, step = 335360, mean_episode_return = 0.55804, mean_episode_step = 76.968, total_loss = -31.653, entropy_loss = -8.606, pg_loss = -104.43, baseline_loss = 81.385, learner_queue_size = 32, _tick = 130, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:12:16,571[0m][[34mroot[0m][[32mINFO[0m] - Step 337920 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1056.3, step = 337920, mean_episode_return = 0.50122, mean_episode_step = 79.622, total_loss = -442.32, entropy_loss = -8.634, pg_loss = -514.52, baseline_loss = 80.829, learner_queue_size = 32, _tick = 131, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:12:21,576[0m][[34mroot[0m][[32mINFO[0m] - Step 340480 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 1061.3, step = 340480, mean_episode_return = 0.5612, mean_episode_step = 68.976, total_loss = 579.5, entropy_loss = -8.6557, pg_loss = 434.54, baseline_loss = 153.62, learner_queue_size = 32, _tick = 132, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:12:26,581[0m][[34mroot[0m][[32mINFO[0m] - Step 340480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1066.3, step = 340480, mean_episode_return = 0.5612, mean_episode_step = 68.976, total_loss = 579.5, entropy_loss = -8.6557, pg_loss = 434.54, baseline_loss = 153.62, learner_queue_size = 32, _tick = 132, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:12:31,587[0m][[34mroot[0m][[32mINFO[0m] - Step 343040 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1071.4, step = 343040, mean_episode_return = 0.44085, mean_episode_step = 66.844, total_loss = -688.73, entropy_loss = -8.6441, pg_loss = -767.74, baseline_loss = 87.658, learner_queue_size = 32, _tick = 133, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:12:36,592[0m][[34mroot[0m][[32mINFO[0m] - Step 345600 @ 511.4 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (train_seconds = 1076.4, step = 345600, mean_episode_return = 0.43654, mean_episode_step = 72.933, total_loss = 309.47, entropy_loss = -8.6382, pg_loss = 184.0, baseline_loss = 134.11, learner_queue_size = 32, _tick = 134, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:12:41,598[0m][[34mroot[0m][[32mINFO[0m] - Step 345600 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1081.4, step = 345600, mean_episode_return = 0.43654, mean_episode_step = 72.933, total_loss = 309.47, entropy_loss = -8.6382, pg_loss = 184.0, baseline_loss = 134.11, learner_queue_size = 32, _tick = 134, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:12:46,603[0m][[34mroot[0m][[32mINFO[0m] - Step 348160 @ 511.4 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 1086.4, step = 348160, mean_episode_return = 0.43676, mean_episode_step = 67.521, total_loss = 191.28, entropy_loss = -8.6468, pg_loss = 65.525, baseline_loss = 134.4, learner_queue_size = 32, _tick = 135, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:12:51,608[0m][[34mroot[0m][[32mINFO[0m] - Step 350720 @ 511.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 1091.4, step = 350720, mean_episode_return = 0.66154, mean_episode_step = 81.882, total_loss = -73.179, entropy_loss = -8.6436, pg_loss = -143.37, baseline_loss = 78.836, learner_queue_size = 32, _tick = 136, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:12:56,613[0m][[34mroot[0m][[32mINFO[0m] - Step 350720 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1096.4, step = 350720, mean_episode_return = 0.66154, mean_episode_step = 81.882, total_loss = -73.179, entropy_loss = -8.6436, pg_loss = -143.37, baseline_loss = 78.836, learner_queue_size = 32, _tick = 136, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:13:01,618[0m][[34mroot[0m][[32mINFO[0m] - Step 353280 @ 511.5 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 1101.4, step = 353280, mean_episode_return = 0.33485, mean_episode_step = 71.692, total_loss = -559.9, entropy_loss = -8.6692, pg_loss = -618.28, baseline_loss = 67.051, learner_queue_size = 32, _tick = 137, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:13:06,623[0m][[34mroot[0m][[32mINFO[0m] - Step 355840 @ 511.5 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (train_seconds = 1106.4, step = 355840, mean_episode_return = 0.39497, mean_episode_step = 69.456, total_loss = -261.35, entropy_loss = -8.68, pg_loss = -329.16, baseline_loss = 76.491, learner_queue_size = 32, _tick = 138, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:13:11,628[0m][[34mroot[0m][[32mINFO[0m] - Step 355840 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1111.4, step = 355840, mean_episode_return = 0.39497, mean_episode_step = 69.456, total_loss = -261.35, entropy_loss = -8.68, pg_loss = -329.16, baseline_loss = 76.491, learner_queue_size = 32, _tick = 138, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:13:16,633[0m][[34mroot[0m][[32mINFO[0m] - Step 358400 @ 511.5 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (train_seconds = 1116.4, step = 358400, mean_episode_return = 0.54777, mean_episode_step = 75.742, total_loss = -107.89, entropy_loss = -8.6562, pg_loss = -171.59, baseline_loss = 72.356, learner_queue_size = 32, _tick = 139, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:13:21,638[0m][[34mroot[0m][[32mINFO[0m] - Step 360960 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1121.4, step = 360960, mean_episode_return = 0.60503, mean_episode_step = 64.832, total_loss = 314.14, entropy_loss = -8.6643, pg_loss = 234.81, baseline_loss = 87.993, learner_queue_size = 32, _tick = 140, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:13:26,643[0m][[34mroot[0m][[32mINFO[0m] - Step 360960 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1126.4, step = 360960, mean_episode_return = 0.60503, mean_episode_step = 64.832, total_loss = 314.14, entropy_loss = -8.6643, pg_loss = 234.81, baseline_loss = 87.993, learner_queue_size = 32, _tick = 140, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:13:31,648[0m][[34mroot[0m][[32mINFO[0m] - Step 363520 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1131.4, step = 363520, mean_episode_return = 0.46583, mean_episode_step = 74.784, total_loss = 810.02, entropy_loss = -8.6691, pg_loss = 634.0, baseline_loss = 184.69, learner_queue_size = 32, _tick = 141, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:13:36,653[0m][[34mroot[0m][[32mINFO[0m] - Step 366080 @ 511.5 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 1136.4, step = 366080, mean_episode_return = 0.3338, mean_episode_step = 66.274, total_loss = 505.16, entropy_loss = -8.6794, pg_loss = 357.92, baseline_loss = 155.92, learner_queue_size = 32, _tick = 142, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:13:41,658[0m][[34mroot[0m][[32mINFO[0m] - Step 366080 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1141.4, step = 366080, mean_episode_return = 0.3338, mean_episode_step = 66.274, total_loss = 505.16, entropy_loss = -8.6794, pg_loss = 357.92, baseline_loss = 155.92, learner_queue_size = 32, _tick = 142, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:13:46,665[0m][[34mroot[0m][[32mINFO[0m] - Step 368640 @ 511.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1146.4, step = 368640, mean_episode_return = 0.51892, mean_episode_step = 72.156, total_loss = -553.15, entropy_loss = -8.6757, pg_loss = -622.38, baseline_loss = 77.907, learner_queue_size = 32, _tick = 143, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:13:51,670[0m][[34mroot[0m][[32mINFO[0m] - Step 368640 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1151.4, step = 368640, mean_episode_return = 0.51892, mean_episode_step = 72.156, total_loss = -553.15, entropy_loss = -8.6757, pg_loss = -622.38, baseline_loss = 77.907, learner_queue_size = 32, _tick = 143, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:13:56,676[0m][[34mroot[0m][[32mINFO[0m] - Step 371200 @ 511.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1156.4, step = 371200, mean_episode_return = 0.24148, mean_episode_step = 69.625, total_loss = -300.64, entropy_loss = -8.6516, pg_loss = -388.53, baseline_loss = 96.537, learner_queue_size = 32, _tick = 144, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:14:01,682[0m][[34mroot[0m][[32mINFO[0m] - Step 373760 @ 511.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1161.4, step = 373760, mean_episode_return = 0.40974, mean_episode_step = 72.616, total_loss = 47.912, entropy_loss = -8.6675, pg_loss = -59.367, baseline_loss = 115.95, learner_queue_size = 32, _tick = 145, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:14:06,688[0m][[34mroot[0m][[32mINFO[0m] - Step 373760 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1166.5, step = 373760, mean_episode_return = 0.40974, mean_episode_step = 72.616, total_loss = 47.912, entropy_loss = -8.6675, pg_loss = -59.367, baseline_loss = 115.95, learner_queue_size = 32, _tick = 145, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:14:11,693[0m][[34mroot[0m][[32mINFO[0m] - Saving checkpoint to /opt/minihack/checkpoint_0.75.tar[0m
[[36m2025-01-20 13:14:11,731[0m][[34mroot[0m][[32mINFO[0m] - Step 376320 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1171.5, step = 376320, mean_episode_return = 0.4476, mean_episode_step = 63.313, total_loss = -16.353, entropy_loss = -8.6483, pg_loss = -100.13, baseline_loss = 92.424, learner_queue_size = 32, _tick = 146, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:14:16,737[0m][[34mroot[0m][[32mINFO[0m] - Step 378880 @ 507.5 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 1176.5, step = 378880, mean_episode_return = 0.36107, mean_episode_step = 63.544, total_loss = -216.74, entropy_loss = -8.6748, pg_loss = -299.43, baseline_loss = 91.357, learner_queue_size = 32, _tick = 147, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:14:21,742[0m][[34mroot[0m][[32mINFO[0m] - Step 378880 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1181.5, step = 378880, mean_episode_return = 0.36107, mean_episode_step = 63.544, total_loss = -216.74, entropy_loss = -8.6748, pg_loss = -299.43, baseline_loss = 91.357, learner_queue_size = 32, _tick = 147, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:14:26,747[0m][[34mroot[0m][[32mINFO[0m] - Step 381440 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1186.5, step = 381440, mean_episode_return = 0.48785, mean_episode_step = 76.131, total_loss = 607.62, entropy_loss = -8.6512, pg_loss = 489.12, baseline_loss = 127.15, learner_queue_size = 32, _tick = 148, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:14:31,752[0m][[34mroot[0m][[32mINFO[0m] - Step 384000 @ 511.5 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 1191.5, step = 384000, mean_episode_return = 0.462, mean_episode_step = 72.607, total_loss = 572.87, entropy_loss = -8.6611, pg_loss = 452.41, baseline_loss = 129.13, learner_queue_size = 32, _tick = 149, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:14:36,757[0m][[34mroot[0m][[32mINFO[0m] - Step 384000 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1196.5, step = 384000, mean_episode_return = 0.462, mean_episode_step = 72.607, total_loss = 572.87, entropy_loss = -8.6611, pg_loss = 452.41, baseline_loss = 129.13, learner_queue_size = 32, _tick = 149, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:14:41,762[0m][[34mroot[0m][[32mINFO[0m] - Saving checkpoint to /opt/minihack/checkpoint.tar[0m
[[36m2025-01-20 13:14:41,800[0m][[34mroot[0m][[32mINFO[0m] - Step 386560 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 1201.5, step = 386560, mean_episode_return = 0.43282, mean_episode_step = 78.035, total_loss = -493.57, entropy_loss = -8.6715, pg_loss = -561.11, baseline_loss = 76.212, learner_queue_size = 32, _tick = 150, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:14:46,805[0m][[34mroot[0m][[32mINFO[0m] - Step 389120 @ 507.6 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1206.6, step = 389120, mean_episode_return = 0.66248, mean_episode_step = 78.718, total_loss = 602.94, entropy_loss = -8.6605, pg_loss = 488.3, baseline_loss = 123.3, learner_queue_size = 32, _tick = 151, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:14:51,810[0m][[34mroot[0m][[32mINFO[0m] - Step 389120 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1211.6, step = 389120, mean_episode_return = 0.66248, mean_episode_step = 78.718, total_loss = 602.94, entropy_loss = -8.6605, pg_loss = 488.3, baseline_loss = 123.3, learner_queue_size = 32, _tick = 151, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:14:56,815[0m][[34mroot[0m][[32mINFO[0m] - Step 391680 @ 511.5 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (train_seconds = 1216.6, step = 391680, mean_episode_return = 0.43284, mean_episode_step = 75.298, total_loss = 328.44, entropy_loss = -8.6702, pg_loss = 238.91, baseline_loss = 98.202, learner_queue_size = 32, _tick = 152, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:15:01,821[0m][[34mroot[0m][[32mINFO[0m] - Step 394240 @ 511.4 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 1221.6, step = 394240, mean_episode_return = 0.72328, mean_episode_step = 74.612, total_loss = 950.47, entropy_loss = -8.6745, pg_loss = 795.68, baseline_loss = 163.46, learner_queue_size = 32, _tick = 153, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:15:06,826[0m][[34mroot[0m][[32mINFO[0m] - Step 394240 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1226.6, step = 394240, mean_episode_return = 0.72328, mean_episode_step = 74.612, total_loss = 950.47, entropy_loss = -8.6745, pg_loss = 795.68, baseline_loss = 163.46, learner_queue_size = 32, _tick = 153, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:15:11,832[0m][[34mroot[0m][[32mINFO[0m] - Step 396800 @ 511.4 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 1231.6, step = 396800, mean_episode_return = 0.44719, mean_episode_step = 77.604, total_loss = -162.76, entropy_loss = -8.675, pg_loss = -245.76, baseline_loss = 91.67, learner_queue_size = 32, _tick = 154, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:15:16,837[0m][[34mroot[0m][[32mINFO[0m] - Step 399360 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1236.6, step = 399360, mean_episode_return = 0.49861, mean_episode_step = 76.732, total_loss = -601.12, entropy_loss = -8.6792, pg_loss = -696.22, baseline_loss = 103.78, learner_queue_size = 32, _tick = 155, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:15:21,842[0m][[34mroot[0m][[32mINFO[0m] - Step 399360 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1241.6, step = 399360, mean_episode_return = 0.49861, mean_episode_step = 76.732, total_loss = -601.12, entropy_loss = -8.6792, pg_loss = -696.22, baseline_loss = 103.78, learner_queue_size = 32, _tick = 155, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:15:26,847[0m][[34mroot[0m][[32mINFO[0m] - Step 401920 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 1246.6, step = 401920, mean_episode_return = 0.61016, mean_episode_step = 79.963, total_loss = 174.76, entropy_loss = -8.6571, pg_loss = 88.653, baseline_loss = 94.759, learner_queue_size = 32, _tick = 156, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:15:31,852[0m][[34mroot[0m][[32mINFO[0m] - Step 404480 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1251.6, step = 404480, mean_episode_return = 0.56265, mean_episode_step = 66.268, total_loss = -208.36, entropy_loss = -8.6773, pg_loss = -304.34, baseline_loss = 104.65, learner_queue_size = 32, _tick = 157, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:15:36,858[0m][[34mroot[0m][[32mINFO[0m] - Step 404480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1256.6, step = 404480, mean_episode_return = 0.56265, mean_episode_step = 66.268, total_loss = -208.36, entropy_loss = -8.6773, pg_loss = -304.34, baseline_loss = 104.65, learner_queue_size = 32, _tick = 157, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:15:41,863[0m][[34mroot[0m][[32mINFO[0m] - Step 407040 @ 511.5 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (train_seconds = 1261.6, step = 407040, mean_episode_return = 0.48097, mean_episode_step = 75.699, total_loss = -813.17, entropy_loss = -8.6647, pg_loss = -876.64, baseline_loss = 72.134, learner_queue_size = 32, _tick = 158, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:15:46,868[0m][[34mroot[0m][[32mINFO[0m] - Step 407040 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1266.6, step = 407040, mean_episode_return = 0.48097, mean_episode_step = 75.699, total_loss = -813.17, entropy_loss = -8.6647, pg_loss = -876.64, baseline_loss = 72.134, learner_queue_size = 32, _tick = 158, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:15:51,873[0m][[34mroot[0m][[32mINFO[0m] - Step 409600 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1271.6, step = 409600, mean_episode_return = 0.48785, mean_episode_step = 59.386, total_loss = 656.44, entropy_loss = -8.6865, pg_loss = 495.5, baseline_loss = 169.62, learner_queue_size = 32, _tick = 159, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:15:56,878[0m][[34mroot[0m][[32mINFO[0m] - Step 412160 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1276.6, step = 412160, mean_episode_return = 0.33034, mean_episode_step = 66.81, total_loss = 433.21, entropy_loss = -8.659, pg_loss = 312.78, baseline_loss = 129.09, learner_queue_size = 32, _tick = 160, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:16:01,883[0m][[34mroot[0m][[32mINFO[0m] - Step 412160 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1281.6, step = 412160, mean_episode_return = 0.33034, mean_episode_step = 66.81, total_loss = 433.21, entropy_loss = -8.659, pg_loss = 312.78, baseline_loss = 129.09, learner_queue_size = 32, _tick = 160, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:16:06,888[0m][[34mroot[0m][[32mINFO[0m] - Step 414720 @ 511.5 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (train_seconds = 1286.7, step = 414720, mean_episode_return = 0.74216, mean_episode_step = 75.42, total_loss = -666.0, entropy_loss = -8.6511, pg_loss = -727.31, baseline_loss = 69.966, learner_queue_size = 32, _tick = 161, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:16:11,893[0m][[34mroot[0m][[32mINFO[0m] - Step 417280 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1291.7, step = 417280, mean_episode_return = 0.46944, mean_episode_step = 74.358, total_loss = 13.075, entropy_loss = -8.645, pg_loss = -91.265, baseline_loss = 112.99, learner_queue_size = 32, _tick = 162, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:16:16,898[0m][[34mroot[0m][[32mINFO[0m] - Step 417280 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1296.7, step = 417280, mean_episode_return = 0.46944, mean_episode_step = 74.358, total_loss = 13.075, entropy_loss = -8.645, pg_loss = -91.265, baseline_loss = 112.99, learner_queue_size = 32, _tick = 162, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:16:21,903[0m][[34mroot[0m][[32mINFO[0m] - Step 419840 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1301.7, step = 419840, mean_episode_return = 0.54488, mean_episode_step = 79.574, total_loss = 370.52, entropy_loss = -8.621, pg_loss = 261.15, baseline_loss = 117.99, learner_queue_size = 32, _tick = 163, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:16:26,908[0m][[34mroot[0m][[32mINFO[0m] - Step 422400 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1306.7, step = 422400, mean_episode_return = 0.44758, mean_episode_step = 74.765, total_loss = 179.85, entropy_loss = -8.6228, pg_loss = 79.15, baseline_loss = 109.32, learner_queue_size = 32, _tick = 164, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:16:31,919[0m][[34mroot[0m][[32mINFO[0m] - Step 422400 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1311.7, step = 422400, mean_episode_return = 0.44758, mean_episode_step = 74.765, total_loss = 179.85, entropy_loss = -8.6228, pg_loss = 79.15, baseline_loss = 109.32, learner_queue_size = 32, _tick = 164, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:16:36,931[0m][[34mroot[0m][[32mINFO[0m] - Step 424960 @ 510.8 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 1316.7, step = 424960, mean_episode_return = 0.45059, mean_episode_step = 71.418, total_loss = 968.05, entropy_loss = -8.621, pg_loss = 848.71, baseline_loss = 127.96, learner_queue_size = 32, _tick = 165, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:16:41,936[0m][[34mroot[0m][[32mINFO[0m] - Step 427520 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1321.7, step = 427520, mean_episode_return = 0.62591, mean_episode_step = 73.147, total_loss = -212.5, entropy_loss = -8.632, pg_loss = -285.49, baseline_loss = 81.626, learner_queue_size = 32, _tick = 166, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:16:46,944[0m][[34mroot[0m][[32mINFO[0m] - Step 427520 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1326.7, step = 427520, mean_episode_return = 0.62591, mean_episode_step = 73.147, total_loss = -212.5, entropy_loss = -8.632, pg_loss = -285.49, baseline_loss = 81.626, learner_queue_size = 32, _tick = 166, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:16:51,949[0m][[34mroot[0m][[32mINFO[0m] - Step 430080 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1331.7, step = 430080, mean_episode_return = 0.415, mean_episode_step = 64.405, total_loss = -186.95, entropy_loss = -8.6609, pg_loss = -303.26, baseline_loss = 124.96, learner_queue_size = 32, _tick = 167, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:16:56,954[0m][[34mroot[0m][[32mINFO[0m] - Step 432640 @ 511.5 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 1336.7, step = 432640, mean_episode_return = 0.55075, mean_episode_step = 87.288, total_loss = -227.68, entropy_loss = -8.6219, pg_loss = -302.46, baseline_loss = 83.401, learner_queue_size = 32, _tick = 168, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:17:01,959[0m][[34mroot[0m][[32mINFO[0m] - Step 432640 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1341.7, step = 432640, mean_episode_return = 0.55075, mean_episode_step = 87.288, total_loss = -227.68, entropy_loss = -8.6219, pg_loss = -302.46, baseline_loss = 83.401, learner_queue_size = 32, _tick = 168, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:17:06,964[0m][[34mroot[0m][[32mINFO[0m] - Step 435200 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 1346.7, step = 435200, mean_episode_return = 0.53655, mean_episode_step = 76.291, total_loss = -228.26, entropy_loss = -8.6213, pg_loss = -299.82, baseline_loss = 80.182, learner_queue_size = 32, _tick = 169, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:17:11,969[0m][[34mroot[0m][[32mINFO[0m] - Step 437760 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1351.7, step = 437760, mean_episode_return = 0.45003, mean_episode_step = 67.912, total_loss = -383.62, entropy_loss = -8.6428, pg_loss = -452.64, baseline_loss = 77.657, learner_queue_size = 32, _tick = 170, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:17:16,974[0m][[34mroot[0m][[32mINFO[0m] - Step 437760 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1356.7, step = 437760, mean_episode_return = 0.45003, mean_episode_step = 67.912, total_loss = -383.62, entropy_loss = -8.6428, pg_loss = -452.64, baseline_loss = 77.657, learner_queue_size = 32, _tick = 170, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:17:21,979[0m][[34mroot[0m][[32mINFO[0m] - Step 440320 @ 511.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 1361.7, step = 440320, mean_episode_return = 0.54439, mean_episode_step = 69.904, total_loss = 191.41, entropy_loss = -8.6296, pg_loss = 111.43, baseline_loss = 88.616, learner_queue_size = 32, _tick = 171, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:17:26,985[0m][[34mroot[0m][[32mINFO[0m] - Step 442880 @ 511.4 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (train_seconds = 1366.8, step = 442880, mean_episode_return = 0.54482, mean_episode_step = 65.45, total_loss = 40.462, entropy_loss = -8.6667, pg_loss = -53.732, baseline_loss = 102.86, learner_queue_size = 32, _tick = 172, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:17:31,991[0m][[34mroot[0m][[32mINFO[0m] - Step 442880 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1371.8, step = 442880, mean_episode_return = 0.54482, mean_episode_step = 65.45, total_loss = 40.462, entropy_loss = -8.6667, pg_loss = -53.732, baseline_loss = 102.86, learner_queue_size = 32, _tick = 172, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:17:36,996[0m][[34mroot[0m][[32mINFO[0m] - Step 445440 @ 511.5 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 1376.8, step = 445440, mean_episode_return = 0.41844, mean_episode_step = 74.276, total_loss = -163.47, entropy_loss = -8.6728, pg_loss = -253.05, baseline_loss = 98.257, learner_queue_size = 32, _tick = 173, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:17:42,000[0m][[34mroot[0m][[32mINFO[0m] - Step 445440 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1381.8, step = 445440, mean_episode_return = 0.41844, mean_episode_step = 74.276, total_loss = -163.47, entropy_loss = -8.6728, pg_loss = -253.05, baseline_loss = 98.257, learner_queue_size = 32, _tick = 173, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:17:47,006[0m][[34mroot[0m][[32mINFO[0m] - Step 448000 @ 511.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1386.8, step = 448000, mean_episode_return = 0.57176, mean_episode_step = 81.755, total_loss = -592.97, entropy_loss = -8.6538, pg_loss = -639.43, baseline_loss = 55.112, learner_queue_size = 32, _tick = 174, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:17:52,011[0m][[34mroot[0m][[32mINFO[0m] - Step 450560 @ 511.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 1391.8, step = 450560, mean_episode_return = 0.42663, mean_episode_step = 63.495, total_loss = -494.35, entropy_loss = -8.6795, pg_loss = -566.74, baseline_loss = 81.068, learner_queue_size = 32, _tick = 175, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:17:57,017[0m][[34mroot[0m][[32mINFO[0m] - Step 450560 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1396.8, step = 450560, mean_episode_return = 0.42663, mean_episode_step = 63.495, total_loss = -494.35, entropy_loss = -8.6795, pg_loss = -566.74, baseline_loss = 81.068, learner_queue_size = 32, _tick = 175, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:18:02,022[0m][[34mroot[0m][[32mINFO[0m] - Step 453120 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1401.8, step = 453120, mean_episode_return = 0.43845, mean_episode_step = 70.701, total_loss = -25.315, entropy_loss = -8.6475, pg_loss = -87.472, baseline_loss = 70.804, learner_queue_size = 32, _tick = 176, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:18:07,027[0m][[34mroot[0m][[32mINFO[0m] - Step 455680 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 1406.8, step = 455680, mean_episode_return = 0.34121, mean_episode_step = 71.948, total_loss = -406.9, entropy_loss = -8.651, pg_loss = -477.71, baseline_loss = 79.455, learner_queue_size = 32, _tick = 177, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:18:12,032[0m][[34mroot[0m][[32mINFO[0m] - Step 455680 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1411.8, step = 455680, mean_episode_return = 0.34121, mean_episode_step = 71.948, total_loss = -406.9, entropy_loss = -8.651, pg_loss = -477.71, baseline_loss = 79.455, learner_queue_size = 32, _tick = 177, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:18:17,037[0m][[34mroot[0m][[32mINFO[0m] - Step 458240 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1416.8, step = 458240, mean_episode_return = 0.441, mean_episode_step = 54.749, total_loss = 382.4, entropy_loss = -8.6658, pg_loss = 300.32, baseline_loss = 90.739, learner_queue_size = 32, _tick = 178, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:18:22,042[0m][[34mroot[0m][[32mINFO[0m] - Step 460800 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1421.8, step = 460800, mean_episode_return = 0.62358, mean_episode_step = 85.818, total_loss = -382.21, entropy_loss = -8.6167, pg_loss = -437.9, baseline_loss = 64.307, learner_queue_size = 32, _tick = 179, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:18:27,047[0m][[34mroot[0m][[32mINFO[0m] - Step 460800 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1426.8, step = 460800, mean_episode_return = 0.62358, mean_episode_step = 85.818, total_loss = -382.21, entropy_loss = -8.6167, pg_loss = -437.9, baseline_loss = 64.307, learner_queue_size = 32, _tick = 179, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:18:32,052[0m][[34mroot[0m][[32mINFO[0m] - Step 463360 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1431.8, step = 463360, mean_episode_return = 0.31236, mean_episode_step = 70.112, total_loss = 93.962, entropy_loss = -8.6218, pg_loss = 18.966, baseline_loss = 83.618, learner_queue_size = 32, _tick = 180, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:18:37,057[0m][[34mroot[0m][[32mINFO[0m] - Step 465920 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 1436.8, step = 465920, mean_episode_return = 0.40944, mean_episode_step = 71.523, total_loss = -540.46, entropy_loss = -8.6386, pg_loss = -608.45, baseline_loss = 76.629, learner_queue_size = 32, _tick = 181, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:18:42,062[0m][[34mroot[0m][[32mINFO[0m] - Step 465920 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1441.8, step = 465920, mean_episode_return = 0.40944, mean_episode_step = 71.523, total_loss = -540.46, entropy_loss = -8.6386, pg_loss = -608.45, baseline_loss = 76.629, learner_queue_size = 32, _tick = 181, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:18:47,067[0m][[34mroot[0m][[32mINFO[0m] - Step 468480 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1446.8, step = 468480, mean_episode_return = 0.5746, mean_episode_step = 77.864, total_loss = 291.1, entropy_loss = -8.6319, pg_loss = 191.61, baseline_loss = 108.12, learner_queue_size = 32, _tick = 182, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:18:52,072[0m][[34mroot[0m][[32mINFO[0m] - Step 471040 @ 511.5 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 1451.8, step = 471040, mean_episode_return = 0.36791, mean_episode_step = 63.393, total_loss = 279.0, entropy_loss = -8.6585, pg_loss = 149.79, baseline_loss = 137.87, learner_queue_size = 32, _tick = 183, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:18:57,077[0m][[34mroot[0m][[32mINFO[0m] - Step 471040 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1456.8, step = 471040, mean_episode_return = 0.36791, mean_episode_step = 63.393, total_loss = 279.0, entropy_loss = -8.6585, pg_loss = 149.79, baseline_loss = 137.87, learner_queue_size = 32, _tick = 183, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:19:02,082[0m][[34mroot[0m][[32mINFO[0m] - Step 473600 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1461.8, step = 473600, mean_episode_return = 0.42703, mean_episode_step = 69.851, total_loss = 538.38, entropy_loss = -8.6356, pg_loss = 427.47, baseline_loss = 119.54, learner_queue_size = 32, _tick = 184, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:19:07,087[0m][[34mroot[0m][[32mINFO[0m] - Step 476160 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1466.9, step = 476160, mean_episode_return = 0.33923, mean_episode_step = 73.407, total_loss = 124.58, entropy_loss = -8.6282, pg_loss = 42.154, baseline_loss = 91.051, learner_queue_size = 32, _tick = 185, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:19:12,092[0m][[34mroot[0m][[32mINFO[0m] - Step 476160 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1471.9, step = 476160, mean_episode_return = 0.33923, mean_episode_step = 73.407, total_loss = 124.58, entropy_loss = -8.6282, pg_loss = 42.154, baseline_loss = 91.051, learner_queue_size = 32, _tick = 185, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:19:17,097[0m][[34mroot[0m][[32mINFO[0m] - Step 478720 @ 511.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1476.9, step = 478720, mean_episode_return = 0.59317, mean_episode_step = 75.839, total_loss = -290.54, entropy_loss = -8.625, pg_loss = -378.28, baseline_loss = 96.372, learner_queue_size = 32, _tick = 186, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:19:22,103[0m][[34mroot[0m][[32mINFO[0m] - Step 481280 @ 511.4 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 1481.9, step = 481280, mean_episode_return = 0.66767, mean_episode_step = 87.363, total_loss = 385.06, entropy_loss = -8.6126, pg_loss = 278.21, baseline_loss = 115.46, learner_queue_size = 32, _tick = 187, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:19:27,108[0m][[34mroot[0m][[32mINFO[0m] - Step 481280 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1486.9, step = 481280, mean_episode_return = 0.66767, mean_episode_step = 87.363, total_loss = 385.06, entropy_loss = -8.6126, pg_loss = 278.21, baseline_loss = 115.46, learner_queue_size = 32, _tick = 187, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:19:32,113[0m][[34mroot[0m][[32mINFO[0m] - Step 483840 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1491.9, step = 483840, mean_episode_return = 0.33847, mean_episode_step = 67.638, total_loss = 24.937, entropy_loss = -8.6551, pg_loss = -75.301, baseline_loss = 108.89, learner_queue_size = 32, _tick = 188, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:19:37,119[0m][[34mroot[0m][[32mINFO[0m] - Step 486400 @ 511.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1496.9, step = 486400, mean_episode_return = 0.34742, mean_episode_step = 68.816, total_loss = -382.91, entropy_loss = -8.6394, pg_loss = -450.85, baseline_loss = 76.577, learner_queue_size = 32, _tick = 189, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:19:42,124[0m][[34mroot[0m][[32mINFO[0m] - Step 486400 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1501.9, step = 486400, mean_episode_return = 0.34742, mean_episode_step = 68.816, total_loss = -382.91, entropy_loss = -8.6394, pg_loss = -450.85, baseline_loss = 76.577, learner_queue_size = 32, _tick = 189, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:19:47,129[0m][[34mroot[0m][[32mINFO[0m] - Step 488960 @ 511.5 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 1506.9, step = 488960, mean_episode_return = 0.60671, mean_episode_step = 71.829, total_loss = 1214.6, entropy_loss = -8.6243, pg_loss = 1068.4, baseline_loss = 154.9, learner_queue_size = 32, _tick = 190, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:19:52,134[0m][[34mroot[0m][[32mINFO[0m] - Step 491520 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 1511.9, step = 491520, mean_episode_return = 0.68745, mean_episode_step = 83.431, total_loss = 1154.2, entropy_loss = -8.6022, pg_loss = 1032.1, baseline_loss = 130.71, learner_queue_size = 32, _tick = 191, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:19:57,139[0m][[34mroot[0m][[32mINFO[0m] - Step 491520 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1516.9, step = 491520, mean_episode_return = 0.68745, mean_episode_step = 83.431, total_loss = 1154.2, entropy_loss = -8.6022, pg_loss = 1032.1, baseline_loss = 130.71, learner_queue_size = 32, _tick = 191, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:20:02,144[0m][[34mroot[0m][[32mINFO[0m] - Step 494080 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 1521.9, step = 494080, mean_episode_return = 0.47853, mean_episode_step = 75.502, total_loss = -21.787, entropy_loss = -8.6287, pg_loss = -118.61, baseline_loss = 105.45, learner_queue_size = 32, _tick = 192, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:20:07,149[0m][[34mroot[0m][[32mINFO[0m] - Step 496640 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1526.9, step = 496640, mean_episode_return = 0.56183, mean_episode_step = 84.887, total_loss = 419.44, entropy_loss = -8.595, pg_loss = 312.17, baseline_loss = 115.87, learner_queue_size = 32, _tick = 193, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:20:12,154[0m][[34mroot[0m][[32mINFO[0m] - Step 496640 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1531.9, step = 496640, mean_episode_return = 0.56183, mean_episode_step = 84.887, total_loss = 419.44, entropy_loss = -8.595, pg_loss = 312.17, baseline_loss = 115.87, learner_queue_size = 32, _tick = 193, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:20:17,159[0m][[34mroot[0m][[32mINFO[0m] - Step 499200 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1536.9, step = 499200, mean_episode_return = 0.55774, mean_episode_step = 80.743, total_loss = 648.5, entropy_loss = -8.6051, pg_loss = 537.76, baseline_loss = 119.35, learner_queue_size = 32, _tick = 194, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:20:22,164[0m][[34mroot[0m][[32mINFO[0m] - Step 501760 @ 511.5 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (train_seconds = 1541.9, step = 501760, mean_episode_return = 0.65553, mean_episode_step = 72.624, total_loss = 1054.4, entropy_loss = -8.6306, pg_loss = 893.36, baseline_loss = 169.65, learner_queue_size = 32, _tick = 195, _time = 1.7374e+09)[0m
[[36m2025-01-20 13:20:22,165[0m][[34mroot[0m][[32mINFO[0m] - Learning finished after 501760 steps.[0m
[[36m2025-01-20 13:20:22,166[0m][[34mroot[0m][[32mINFO[0m] - Saving checkpoint to /opt/minihack/checkpoint.tar[0m
[[36m2025-01-20 13:26:01,003[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,005[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,007[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,009[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,010[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
<MiniHackCombatSkill instance>
[[36m2025-01-20 13:26:01,010[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,011[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,012[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,013[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,013[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,014[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,014[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,007[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,016[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,007[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,017[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,020[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,020[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,021[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,024[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,024[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,026[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,026[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,027[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,027[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,028[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,029[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,029[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,030[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,031[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,031[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,031[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,031[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,032[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,032[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,032[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,033[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,034[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,034[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,033[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,034[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,033[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,036[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,036[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,037[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,038[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,038[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,038[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,039[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,040[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,040[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,040[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,041[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,042[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,042[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,042[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,043[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,043[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,043[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,044[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,044[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,044[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,045[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,045[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,046[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,046[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,046[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,047[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,047[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,048[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,048[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,040[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,019[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,050[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,050[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,051[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,024[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,052[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,052[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,053[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,053[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,053[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,053[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,054[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,036[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,054[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,048[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,055[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,055[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,057[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,058[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,059[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,037[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,058[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,060[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,061[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
<MiniHackCombatSkill instance>
[[36m2025-01-20 13:26:01,063[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,062[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,063[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,064[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,065[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,063[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,067[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,068[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,070[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,072[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,073[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,026[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,042[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,079[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,083[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,083[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,070[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,087[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,090[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,090[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,091[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,093[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,094[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,090[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,096[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,098[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,099[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,101[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,102[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,104[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,108[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,112[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,113[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,116[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,116[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,117[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,118[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,122[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,122[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,123[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,124[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,116[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,117[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,125[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,121[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,119[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,121[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,123[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,130[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,130[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,130[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,131[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,134[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,134[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,136[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,137[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,137[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,138[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,140[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,141[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,141[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,141[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,142[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,143[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,139[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,144[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,147[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,148[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,149[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,151[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,152[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,153[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,154[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,156[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,158[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,161[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,161[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,161[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,162[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,165[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,164[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,168[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,169[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,172[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,175[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,175[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,177[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,178[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,178[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,180[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,175[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,182[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,180[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,184[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,186[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,189[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,189[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,192[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,194[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,195[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,197[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,198[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,196[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,197[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,200[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,207[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,205[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,210[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,210[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,210[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,203[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,211[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,216[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,218[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-20 13:26:01,220[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
