[[36m2025-01-18 17:09:43,273[0m][[34mroot[0m][[32mINFO[0m] - name: null
wandb: false
project: minihack
entity: entity_name
group: default
mock: false
single_ttyrec: true
num_seeds: 0
write_profiler_trace: false
relative_reward: false
fn_penalty_step: constant
penalty_time: 0.0
penalty_step: -0.001
reward_lose: 0
reward_win: 1
save_tty: false
character: null
mode: train
env: MiniHack-Combat-Skill-v0
obs_keys: glyphs,chars,colors,specials,blstats,message
num_actors: 256
total_steps: 500000
batch_size: 32
unroll_length: 80
num_learner_threads: 1
num_inference_threads: 1
disable_cuda: false
learner_device: cuda:0
actor_device: cuda:0
max_learner_queue_size: null
model: baseline
use_lstm: true
hidden_dim: 256
embedding_dim: 64
glyph_type: all_cat
equalize_input_dim: false
equalize_factor: 2
layers: 5
crop_model: cnn
crop_dim: 9
use_index_select: true
entropy_cost: 0.001
baseline_cost: 0.5
discounting: 0.999
reward_clipping: none
normalize_reward: true
learning_rate: 0.0002
grad_norm_clipping: 40
alpha: 0.99
momentum: 0
epsilon: 1.0e-06
state_counter: none
no_extrinsic: false
int:
  twoheaded: true
  input: full
  intrinsic_weight: 0.1
  discounting: 0.99
  baseline_cost: 0.5
  episodic: true
  reward_clipping: none
  normalize_reward: true
ride:
  count_norm: true
  forward_cost: 1
  inverse_cost: 0.1
  hidden_dim: 128
rnd:
  forward_cost: 0.01
msg:
  model: none
  hidden_dim: 64
  embedding_dim: 32
[0m
[[36m2025-01-18 17:09:43,329[0m][[34mroot[0m][[32mINFO[0m] - Symlinked log directory: /opt/minihack/latest[0m
[[36m2025-01-18 17:09:43,329[0m][[34mroot[0m][[32mINFO[0m] - Found archive directory: /opt/minihack/archives[0m
[[36m2025-01-18 17:09:43,334[0m][[34mroot[0m][[32mINFO[0m] - Logging results to /opt/minihack[0m
[[36m2025-01-18 17:09:43,383[0m][[34mpalaas/out[0m][[32mINFO[0m] - Found log directory: /opt/minihack[0m
[[36m2025-01-18 17:09:43,383[0m][[34mpalaas/out[0m][[32mINFO[0m] - Saving arguments to /opt/minihack/meta.json[0m
[[36m2025-01-18 17:09:43,384[0m][[34mpalaas/out[0m][[32mINFO[0m] - Saving messages to /opt/minihack/out.log[0m
[[36m2025-01-18 17:09:43,384[0m][[34mpalaas/out[0m][[32mINFO[0m] - Saving logs data to /opt/minihack/logs.csv[0m
[[36m2025-01-18 17:09:43,384[0m][[34mpalaas/out[0m][[32mINFO[0m] - Saving logs' fields to /opt/minihack/fields.csv[0m
[[36m2025-01-18 17:09:43,385[0m][[34mroot[0m][[32mINFO[0m] - Not using CUDA.[0m
[[36m2025-01-18 17:09:43,399[0m][[34mroot[0m][[32mINFO[0m] - Using model baseline[0m
[[36m2025-01-18 17:09:43,399[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,480[0m][[34mroot[0m][[32mINFO[0m] - Number of model parameters: 4264078[0m
[[36m2025-01-18 17:09:43,480[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,544[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,544[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,544[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,552[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,552[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,553[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,553[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,557[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,558[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,561[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,562[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,563[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,565[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,566[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,552[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,568[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,569[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,578[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,581[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,583[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,585[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,587[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
First Environment waiting for connection to unix:/tmp/poly..opt.minihack.0 ... connection established.
[[36m2025-01-18 17:09:43,589[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,596[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,643[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,657[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,665[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,691[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,698[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,698[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,701[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,698[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,700[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,707[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,710[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,712[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,714[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,715[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,717[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,717[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,719[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,721[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,725[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,727[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,732[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,733[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,735[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,735[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,739[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,742[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:43,740[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 17:09:48,543[0m][[34mroot[0m][[32mINFO[0m] - Step 0 @ 0.0 SPS. Inference batcher size: 3. Learner queue size: 14. Other stats: (train_seconds = 5.0)[0m
[[36m2025-01-18 17:09:53,551[0m][[34mroot[0m][[32mINFO[0m] - Step 0 @ 0.0 SPS. Inference batcher size: 114. Learner queue size: 0. Other stats: (train_seconds = 10.0)[0m
[[36m2025-01-18 17:09:58,556[0m][[34mroot[0m][[32mINFO[0m] - Step 0 @ 0.0 SPS. Inference batcher size: 45. Learner queue size: 1. Other stats: (train_seconds = 15.0)[0m
[[36m2025-01-18 17:10:03,054[0m][[34mpalaas/out[0m][[32mINFO[0m] - Updated log fields: ['_tick', '_time', 'train_seconds', 'step', 'mean_episode_return', 'mean_episode_step', 'total_loss', 'entropy_loss', 'pg_loss', 'baseline_loss', 'learner_queue_size'][0m
[[36m2025-01-18 17:10:03,562[0m][[34mroot[0m][[32mINFO[0m] - Saving checkpoint to /opt/minihack/checkpoint_0.tar[0m
[[36m2025-01-18 17:10:03,597[0m][[34mroot[0m][[32mINFO[0m] - Step 2560 @ 511.4 SPS. Inference batcher size: 123. Learner queue size: 3. Other stats: (train_seconds = 20.0, step = 2560, mean_episode_return = -0.04019, mean_episode_step = 57.615, total_loss = -1296.9, entropy_loss = -11.371, pg_loss = -1637.2, baseline_loss = 351.62, learner_queue_size = 3, _tick = 0, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:10:08,602[0m][[34mroot[0m][[32mINFO[0m] - Step 2560 @ 0.0 SPS. Inference batcher size: 69. Learner queue size: 5. Other stats: (train_seconds = 25.1, step = 2560, mean_episode_return = -0.04019, mean_episode_step = 57.615, total_loss = -1296.9, entropy_loss = -11.371, pg_loss = -1637.2, baseline_loss = 351.62, learner_queue_size = 3, _tick = 0, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:10:13,609[0m][[34mroot[0m][[32mINFO[0m] - Step 2560 @ 0.0 SPS. Inference batcher size: 85. Learner queue size: 8. Other stats: (train_seconds = 30.1, step = 2560, mean_episode_return = -0.04019, mean_episode_step = 57.615, total_loss = -1296.9, entropy_loss = -11.371, pg_loss = -1637.2, baseline_loss = 351.62, learner_queue_size = 3, _tick = 0, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:10:18,614[0m][[34mroot[0m][[32mINFO[0m] - Step 2560 @ 0.0 SPS. Inference batcher size: 125. Learner queue size: 14. Other stats: (train_seconds = 35.1, step = 2560, mean_episode_return = -0.04019, mean_episode_step = 57.615, total_loss = -1296.9, entropy_loss = -11.371, pg_loss = -1637.2, baseline_loss = 351.62, learner_queue_size = 3, _tick = 0, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:10:23,618[0m][[34mroot[0m][[32mINFO[0m] - Step 2560 @ 0.0 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (train_seconds = 40.1, step = 2560, mean_episode_return = -0.04019, mean_episode_step = 57.615, total_loss = -1296.9, entropy_loss = -11.371, pg_loss = -1637.2, baseline_loss = 351.62, learner_queue_size = 3, _tick = 0, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:10:28,624[0m][[34mroot[0m][[32mINFO[0m] - Step 2560 @ 0.0 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 45.1, step = 2560, mean_episode_return = -0.04019, mean_episode_step = 57.615, total_loss = -1296.9, entropy_loss = -11.371, pg_loss = -1637.2, baseline_loss = 351.62, learner_queue_size = 3, _tick = 0, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:10:33,629[0m][[34mroot[0m][[32mINFO[0m] - Step 2560 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 50.1, step = 2560, mean_episode_return = -0.04019, mean_episode_step = 57.615, total_loss = -1296.9, entropy_loss = -11.371, pg_loss = -1637.2, baseline_loss = 351.62, learner_queue_size = 3, _tick = 0, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:10:38,635[0m][[34mroot[0m][[32mINFO[0m] - Step 5120 @ 511.5 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (train_seconds = 55.1, step = 5120, mean_episode_return = -0.050652, mean_episode_step = 40.494, total_loss = 1410.7, entropy_loss = -11.342, pg_loss = 1289.4, baseline_loss = 132.61, learner_queue_size = 32, _tick = 1, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:10:43,640[0m][[34mroot[0m][[32mINFO[0m] - Step 7680 @ 511.4 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 60.1, step = 7680, mean_episode_return = -0.0335, mean_episode_step = 30.781, total_loss = -858.63, entropy_loss = -11.371, pg_loss = -853.53, baseline_loss = 6.2686, learner_queue_size = 32, _tick = 2, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:10:48,645[0m][[34mroot[0m][[32mINFO[0m] - Step 7680 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 65.1, step = 7680, mean_episode_return = -0.0335, mean_episode_step = 30.781, total_loss = -858.63, entropy_loss = -11.371, pg_loss = -853.53, baseline_loss = 6.2686, learner_queue_size = 32, _tick = 2, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:10:53,651[0m][[34mroot[0m][[32mINFO[0m] - Step 10240 @ 511.4 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 70.1, step = 10240, mean_episode_return = -0.033842, mean_episode_step = 33.298, total_loss = 1034.6, entropy_loss = -11.325, pg_loss = 928.38, baseline_loss = 117.54, learner_queue_size = 32, _tick = 3, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:10:58,656[0m][[34mroot[0m][[32mINFO[0m] - Step 10240 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 75.1, step = 10240, mean_episode_return = -0.033842, mean_episode_step = 33.298, total_loss = 1034.6, entropy_loss = -11.325, pg_loss = 928.38, baseline_loss = 117.54, learner_queue_size = 32, _tick = 3, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:11:03,662[0m][[34mroot[0m][[32mINFO[0m] - Step 12800 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 80.1, step = 12800, mean_episode_return = -0.051789, mean_episode_step = 42.746, total_loss = 161.33, entropy_loss = -11.35, pg_loss = 109.59, baseline_loss = 63.089, learner_queue_size = 32, _tick = 4, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:11:08,667[0m][[34mroot[0m][[32mINFO[0m] - Step 12800 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 85.1, step = 12800, mean_episode_return = -0.051789, mean_episode_step = 42.746, total_loss = 161.33, entropy_loss = -11.35, pg_loss = 109.59, baseline_loss = 63.089, learner_queue_size = 32, _tick = 4, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:11:13,672[0m][[34mroot[0m][[32mINFO[0m] - Step 15360 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 90.1, step = 15360, mean_episode_return = -0.053211, mean_episode_step = 51.979, total_loss = -271.44, entropy_loss = -11.304, pg_loss = -317.95, baseline_loss = 57.812, learner_queue_size = 32, _tick = 5, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:11:18,677[0m][[34mroot[0m][[32mINFO[0m] - Step 17920 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 95.1, step = 17920, mean_episode_return = -0.017095, mean_episode_step = 56.192, total_loss = 156.47, entropy_loss = -11.301, pg_loss = 115.52, baseline_loss = 52.246, learner_queue_size = 32, _tick = 6, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:11:23,682[0m][[34mroot[0m][[32mINFO[0m] - Step 17920 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 100.1, step = 17920, mean_episode_return = -0.017095, mean_episode_step = 56.192, total_loss = 156.47, entropy_loss = -11.301, pg_loss = 115.52, baseline_loss = 52.246, learner_queue_size = 32, _tick = 6, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:11:28,687[0m][[34mroot[0m][[32mINFO[0m] - Step 20480 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 105.1, step = 20480, mean_episode_return = -0.070391, mean_episode_step = 56.617, total_loss = -47.061, entropy_loss = -11.313, pg_loss = -97.656, baseline_loss = 61.908, learner_queue_size = 32, _tick = 7, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:11:33,692[0m][[34mroot[0m][[32mINFO[0m] - Step 20480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 110.2, step = 20480, mean_episode_return = -0.070391, mean_episode_step = 56.617, total_loss = -47.061, entropy_loss = -11.313, pg_loss = -97.656, baseline_loss = 61.908, learner_queue_size = 32, _tick = 7, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:11:38,698[0m][[34mroot[0m][[32mINFO[0m] - Step 23040 @ 511.5 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (train_seconds = 115.2, step = 23040, mean_episode_return = -0.076571, mean_episode_step = 55.888, total_loss = -119.56, entropy_loss = -11.292, pg_loss = -112.66, baseline_loss = 4.3964, learner_queue_size = 32, _tick = 8, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:11:43,704[0m][[34mroot[0m][[32mINFO[0m] - Step 25600 @ 511.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 120.2, step = 25600, mean_episode_return = -0.058083, mean_episode_step = 53.176, total_loss = 110.74, entropy_loss = -11.169, pg_loss = 111.77, baseline_loss = 10.139, learner_queue_size = 32, _tick = 9, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:11:48,709[0m][[34mroot[0m][[32mINFO[0m] - Step 25600 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 125.2, step = 25600, mean_episode_return = -0.058083, mean_episode_step = 53.176, total_loss = 110.74, entropy_loss = -11.169, pg_loss = 111.77, baseline_loss = 10.139, learner_queue_size = 32, _tick = 9, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:11:53,715[0m][[34mroot[0m][[32mINFO[0m] - Step 28160 @ 511.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (train_seconds = 130.2, step = 28160, mean_episode_return = -0.069056, mean_episode_step = 59.596, total_loss = 6.555, entropy_loss = -11.158, pg_loss = -25.249, baseline_loss = 42.962, learner_queue_size = 32, _tick = 10, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:11:58,720[0m][[34mroot[0m][[32mINFO[0m] - Step 28160 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 135.2, step = 28160, mean_episode_return = -0.069056, mean_episode_step = 59.596, total_loss = 6.555, entropy_loss = -11.158, pg_loss = -25.249, baseline_loss = 42.962, learner_queue_size = 32, _tick = 10, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:12:03,725[0m][[34mroot[0m][[32mINFO[0m] - Step 30720 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 140.2, step = 30720, mean_episode_return = -0.05703, mean_episode_step = 54.868, total_loss = -47.796, entropy_loss = -11.101, pg_loss = -40.11, baseline_loss = 3.4151, learner_queue_size = 32, _tick = 11, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:12:08,739[0m][[34mroot[0m][[32mINFO[0m] - Step 30720 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 145.2, step = 30720, mean_episode_return = -0.05703, mean_episode_step = 54.868, total_loss = -47.796, entropy_loss = -11.101, pg_loss = -40.11, baseline_loss = 3.4151, learner_queue_size = 32, _tick = 11, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:12:13,744[0m][[34mroot[0m][[32mINFO[0m] - Step 33280 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 150.2, step = 33280, mean_episode_return = -0.042458, mean_episode_step = 64.67, total_loss = 273.17, entropy_loss = -10.939, pg_loss = 247.06, baseline_loss = 37.045, learner_queue_size = 32, _tick = 12, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:12:18,749[0m][[34mroot[0m][[32mINFO[0m] - Step 35840 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 155.2, step = 35840, mean_episode_return = -0.061, mean_episode_step = 45.04, total_loss = -91.497, entropy_loss = -10.921, pg_loss = -82.772, baseline_loss = 2.1958, learner_queue_size = 32, _tick = 13, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:12:23,754[0m][[34mroot[0m][[32mINFO[0m] - Step 35840 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 160.2, step = 35840, mean_episode_return = -0.061, mean_episode_step = 45.04, total_loss = -91.497, entropy_loss = -10.921, pg_loss = -82.772, baseline_loss = 2.1958, learner_queue_size = 32, _tick = 13, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:12:28,759[0m][[34mroot[0m][[32mINFO[0m] - Step 38400 @ 511.5 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 165.2, step = 38400, mean_episode_return = -0.0041316, mean_episode_step = 50.314, total_loss = 852.04, entropy_loss = -10.72, pg_loss = 704.12, baseline_loss = 158.64, learner_queue_size = 32, _tick = 14, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:12:33,764[0m][[34mroot[0m][[32mINFO[0m] - Step 38400 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 170.2, step = 38400, mean_episode_return = -0.0041316, mean_episode_step = 50.314, total_loss = 852.04, entropy_loss = -10.72, pg_loss = 704.12, baseline_loss = 158.64, learner_queue_size = 32, _tick = 14, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:12:38,769[0m][[34mroot[0m][[32mINFO[0m] - Step 40960 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 175.2, step = 40960, mean_episode_return = 0.019057, mean_episode_step = 63.427, total_loss = 956.4, entropy_loss = -10.73, pg_loss = 764.13, baseline_loss = 203.0, learner_queue_size = 32, _tick = 15, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:12:43,774[0m][[34mroot[0m][[32mINFO[0m] - Step 40960 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 180.2, step = 40960, mean_episode_return = 0.019057, mean_episode_step = 63.427, total_loss = 956.4, entropy_loss = -10.73, pg_loss = 764.13, baseline_loss = 203.0, learner_queue_size = 32, _tick = 15, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:12:48,779[0m][[34mroot[0m][[32mINFO[0m] - Step 43520 @ 511.5 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (train_seconds = 185.2, step = 43520, mean_episode_return = 0.0038605, mean_episode_step = 56.07, total_loss = 86.896, entropy_loss = -10.735, pg_loss = 33.929, baseline_loss = 63.702, learner_queue_size = 32, _tick = 16, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:12:53,784[0m][[34mroot[0m][[32mINFO[0m] - Step 46080 @ 511.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 190.2, step = 46080, mean_episode_return = -0.059717, mean_episode_step = 58.542, total_loss = 183.11, entropy_loss = -10.582, pg_loss = 57.661, baseline_loss = 136.03, learner_queue_size = 32, _tick = 17, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:12:58,789[0m][[34mroot[0m][[32mINFO[0m] - Step 46080 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 195.3, step = 46080, mean_episode_return = -0.059717, mean_episode_step = 58.542, total_loss = 183.11, entropy_loss = -10.582, pg_loss = 57.661, baseline_loss = 136.03, learner_queue_size = 32, _tick = 17, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:13:03,794[0m][[34mroot[0m][[32mINFO[0m] - Step 48640 @ 511.5 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (train_seconds = 200.3, step = 48640, mean_episode_return = 6.166e-09, mean_episode_step = 43.775, total_loss = 368.15, entropy_loss = -10.586, pg_loss = 217.76, baseline_loss = 160.98, learner_queue_size = 32, _tick = 18, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:13:08,799[0m][[34mroot[0m][[32mINFO[0m] - Step 48640 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 205.3, step = 48640, mean_episode_return = 6.166e-09, mean_episode_step = 43.775, total_loss = 368.15, entropy_loss = -10.586, pg_loss = 217.76, baseline_loss = 160.98, learner_queue_size = 32, _tick = 18, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:13:13,804[0m][[34mroot[0m][[32mINFO[0m] - Step 51200 @ 511.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 210.3, step = 51200, mean_episode_return = -0.027362, mean_episode_step = 44.638, total_loss = -703.59, entropy_loss = -10.663, pg_loss = -699.05, baseline_loss = 6.1251, learner_queue_size = 32, _tick = 19, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:13:18,810[0m][[34mroot[0m][[32mINFO[0m] - Step 51200 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 215.3, step = 51200, mean_episode_return = -0.027362, mean_episode_step = 44.638, total_loss = -703.59, entropy_loss = -10.663, pg_loss = -699.05, baseline_loss = 6.1251, learner_queue_size = 32, _tick = 19, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:13:23,814[0m][[34mroot[0m][[32mINFO[0m] - Step 53760 @ 511.5 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (train_seconds = 220.3, step = 53760, mean_episode_return = -0.038467, mean_episode_step = 48.334, total_loss = 223.99, entropy_loss = -10.643, pg_loss = 129.72, baseline_loss = 104.91, learner_queue_size = 32, _tick = 20, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:13:28,819[0m][[34mroot[0m][[32mINFO[0m] - Step 56320 @ 511.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 225.3, step = 56320, mean_episode_return = 0.0078364, mean_episode_step = 47.119, total_loss = -0.48108, entropy_loss = -10.625, pg_loss = -81.642, baseline_loss = 91.787, learner_queue_size = 32, _tick = 21, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:13:33,824[0m][[34mroot[0m][[32mINFO[0m] - Step 56320 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 230.3, step = 56320, mean_episode_return = 0.0078364, mean_episode_step = 47.119, total_loss = -0.48108, entropy_loss = -10.625, pg_loss = -81.642, baseline_loss = 91.787, learner_queue_size = 32, _tick = 21, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:13:38,830[0m][[34mroot[0m][[32mINFO[0m] - Step 58880 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 235.3, step = 58880, mean_episode_return = -0.019684, mean_episode_step = 40.835, total_loss = 373.93, entropy_loss = -10.56, pg_loss = 272.6, baseline_loss = 111.89, learner_queue_size = 32, _tick = 22, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:13:43,835[0m][[34mroot[0m][[32mINFO[0m] - Step 58880 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 240.3, step = 58880, mean_episode_return = -0.019684, mean_episode_step = 40.835, total_loss = 373.93, entropy_loss = -10.56, pg_loss = 272.6, baseline_loss = 111.89, learner_queue_size = 32, _tick = 22, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:13:48,841[0m][[34mroot[0m][[32mINFO[0m] - Step 61440 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 245.3, step = 61440, mean_episode_return = -0.0082272, mean_episode_step = 48.772, total_loss = 222.44, entropy_loss = -10.587, pg_loss = 85.814, baseline_loss = 147.21, learner_queue_size = 32, _tick = 23, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:13:53,846[0m][[34mroot[0m][[32mINFO[0m] - Step 61440 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 250.3, step = 61440, mean_episode_return = -0.0082272, mean_episode_step = 48.772, total_loss = 222.44, entropy_loss = -10.587, pg_loss = 85.814, baseline_loss = 147.21, learner_queue_size = 32, _tick = 23, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:13:58,851[0m][[34mroot[0m][[32mINFO[0m] - Step 64000 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 255.3, step = 64000, mean_episode_return = 0.038471, mean_episode_step = 59.264, total_loss = 492.24, entropy_loss = -10.577, pg_loss = 284.55, baseline_loss = 218.27, learner_queue_size = 32, _tick = 24, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:14:03,856[0m][[34mroot[0m][[32mINFO[0m] - Step 64000 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 260.3, step = 64000, mean_episode_return = 0.038471, mean_episode_step = 59.264, total_loss = 492.24, entropy_loss = -10.577, pg_loss = 284.55, baseline_loss = 218.27, learner_queue_size = 32, _tick = 24, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:14:08,862[0m][[34mroot[0m][[32mINFO[0m] - Step 66560 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 265.3, step = 66560, mean_episode_return = -0.013152, mean_episode_step = 35.423, total_loss = -867.51, entropy_loss = -10.589, pg_loss = -951.94, baseline_loss = 95.016, learner_queue_size = 32, _tick = 25, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:14:13,868[0m][[34mroot[0m][[32mINFO[0m] - Step 69120 @ 511.4 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 270.3, step = 69120, mean_episode_return = -0.0057937, mean_episode_step = 44.35, total_loss = -153.88, entropy_loss = -10.541, pg_loss = -243.4, baseline_loss = 100.06, learner_queue_size = 32, _tick = 26, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:14:18,873[0m][[34mroot[0m][[32mINFO[0m] - Step 69120 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 275.3, step = 69120, mean_episode_return = -0.0057937, mean_episode_step = 44.35, total_loss = -153.88, entropy_loss = -10.541, pg_loss = -243.4, baseline_loss = 100.06, learner_queue_size = 32, _tick = 26, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:14:23,879[0m][[34mroot[0m][[32mINFO[0m] - Step 71680 @ 511.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (train_seconds = 280.3, step = 71680, mean_episode_return = -0.016981, mean_episode_step = 47.02, total_loss = -201.7, entropy_loss = -10.485, pg_loss = -215.19, baseline_loss = 23.977, learner_queue_size = 32, _tick = 27, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:14:28,884[0m][[34mroot[0m][[32mINFO[0m] - Step 71680 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 285.3, step = 71680, mean_episode_return = -0.016981, mean_episode_step = 47.02, total_loss = -201.7, entropy_loss = -10.485, pg_loss = -215.19, baseline_loss = 23.977, learner_queue_size = 32, _tick = 27, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:14:33,889[0m][[34mroot[0m][[32mINFO[0m] - Step 74240 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 290.4, step = 74240, mean_episode_return = -0.0051311, mean_episode_step = 39.903, total_loss = 420.97, entropy_loss = -10.393, pg_loss = 344.17, baseline_loss = 87.195, learner_queue_size = 32, _tick = 28, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:14:38,894[0m][[34mroot[0m][[32mINFO[0m] - Step 74240 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 295.4, step = 74240, mean_episode_return = -0.0051311, mean_episode_step = 39.903, total_loss = 420.97, entropy_loss = -10.393, pg_loss = 344.17, baseline_loss = 87.195, learner_queue_size = 32, _tick = 28, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:14:43,899[0m][[34mroot[0m][[32mINFO[0m] - Step 76800 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 300.4, step = 76800, mean_episode_return = 0.031547, mean_episode_step = 42.841, total_loss = 1014.7, entropy_loss = -10.351, pg_loss = 817.62, baseline_loss = 207.39, learner_queue_size = 32, _tick = 29, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:14:48,906[0m][[34mroot[0m][[32mINFO[0m] - Step 79360 @ 511.3 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 305.4, step = 79360, mean_episode_return = 0.02873, mean_episode_step = 47.319, total_loss = 117.15, entropy_loss = -10.367, pg_loss = -31.443, baseline_loss = 158.96, learner_queue_size = 32, _tick = 30, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:14:53,913[0m][[34mroot[0m][[32mINFO[0m] - Step 79360 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 310.4, step = 79360, mean_episode_return = 0.02873, mean_episode_step = 47.319, total_loss = 117.15, entropy_loss = -10.367, pg_loss = -31.443, baseline_loss = 158.96, learner_queue_size = 32, _tick = 30, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:14:58,918[0m][[34mroot[0m][[32mINFO[0m] - Step 81920 @ 511.5 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (train_seconds = 315.4, step = 81920, mean_episode_return = 0.052554, mean_episode_step = 43.704, total_loss = 433.64, entropy_loss = -10.373, pg_loss = 255.61, baseline_loss = 188.4, learner_queue_size = 32, _tick = 31, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:15:03,923[0m][[34mroot[0m][[32mINFO[0m] - Step 81920 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 320.4, step = 81920, mean_episode_return = 0.052554, mean_episode_step = 43.704, total_loss = 433.64, entropy_loss = -10.373, pg_loss = 255.61, baseline_loss = 188.4, learner_queue_size = 32, _tick = 31, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:15:08,928[0m][[34mroot[0m][[32mINFO[0m] - Step 84480 @ 511.5 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 325.4, step = 84480, mean_episode_return = -0.017, mean_episode_step = 37.108, total_loss = -358.6, entropy_loss = -10.366, pg_loss = -443.61, baseline_loss = 95.374, learner_queue_size = 32, _tick = 32, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:15:13,933[0m][[34mroot[0m][[32mINFO[0m] - Step 84480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 330.4, step = 84480, mean_episode_return = -0.017, mean_episode_step = 37.108, total_loss = -358.6, entropy_loss = -10.366, pg_loss = -443.61, baseline_loss = 95.374, learner_queue_size = 32, _tick = 32, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:15:18,939[0m][[34mroot[0m][[32mINFO[0m] - Step 87040 @ 511.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 335.4, step = 87040, mean_episode_return = 0.021882, mean_episode_step = 44.246, total_loss = 147.56, entropy_loss = -10.347, pg_loss = 10.985, baseline_loss = 146.92, learner_queue_size = 32, _tick = 33, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:15:23,943[0m][[34mroot[0m][[32mINFO[0m] - Step 89600 @ 511.5 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (train_seconds = 340.4, step = 89600, mean_episode_return = -0.014952, mean_episode_step = 51.429, total_loss = -394.31, entropy_loss = -10.231, pg_loss = -441.66, baseline_loss = 57.588, learner_queue_size = 32, _tick = 34, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:15:28,948[0m][[34mroot[0m][[32mINFO[0m] - Step 89600 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 345.4, step = 89600, mean_episode_return = -0.014952, mean_episode_step = 51.429, total_loss = -394.31, entropy_loss = -10.231, pg_loss = -441.66, baseline_loss = 57.588, learner_queue_size = 32, _tick = 34, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:15:33,953[0m][[34mroot[0m][[32mINFO[0m] - Step 92160 @ 511.5 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (train_seconds = 350.4, step = 92160, mean_episode_return = 0.0241, mean_episode_step = 48.801, total_loss = 530.57, entropy_loss = -10.181, pg_loss = 364.81, baseline_loss = 175.94, learner_queue_size = 32, _tick = 35, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:15:38,958[0m][[34mroot[0m][[32mINFO[0m] - Step 92160 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 355.4, step = 92160, mean_episode_return = 0.0241, mean_episode_step = 48.801, total_loss = 530.57, entropy_loss = -10.181, pg_loss = 364.81, baseline_loss = 175.94, learner_queue_size = 32, _tick = 35, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:15:43,965[0m][[34mroot[0m][[32mINFO[0m] - Step 94720 @ 511.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (train_seconds = 360.4, step = 94720, mean_episode_return = 0.1018, mean_episode_step = 44.638, total_loss = 287.14, entropy_loss = -10.17, pg_loss = 108.24, baseline_loss = 189.07, learner_queue_size = 32, _tick = 36, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:15:48,970[0m][[34mroot[0m][[32mINFO[0m] - Step 97280 @ 511.4 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (train_seconds = 365.4, step = 97280, mean_episode_return = 0.060815, mean_episode_step = 41.593, total_loss = 1001.4, entropy_loss = -9.993, pg_loss = 768.45, baseline_loss = 242.94, learner_queue_size = 32, _tick = 37, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:15:53,976[0m][[34mroot[0m][[32mINFO[0m] - Step 97280 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 370.4, step = 97280, mean_episode_return = 0.060815, mean_episode_step = 41.593, total_loss = 1001.4, entropy_loss = -9.993, pg_loss = 768.45, baseline_loss = 242.94, learner_queue_size = 32, _tick = 37, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:15:58,982[0m][[34mroot[0m][[32mINFO[0m] - Step 99840 @ 511.4 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (train_seconds = 375.4, step = 99840, mean_episode_return = 0.018683, mean_episode_step = 48.196, total_loss = 48.774, entropy_loss = -10.003, pg_loss = -131.27, baseline_loss = 190.05, learner_queue_size = 32, _tick = 38, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:16:03,986[0m][[34mroot[0m][[32mINFO[0m] - Step 99840 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 380.4, step = 99840, mean_episode_return = 0.018683, mean_episode_step = 48.196, total_loss = 48.774, entropy_loss = -10.003, pg_loss = -131.27, baseline_loss = 190.05, learner_queue_size = 32, _tick = 38, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:16:08,991[0m][[34mroot[0m][[32mINFO[0m] - Step 102400 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 385.5, step = 102400, mean_episode_return = -0.013569, mean_episode_step = 48.357, total_loss = 217.02, entropy_loss = -9.9928, pg_loss = 101.61, baseline_loss = 125.4, learner_queue_size = 32, _tick = 39, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:16:13,996[0m][[34mroot[0m][[32mINFO[0m] - Step 102400 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 390.5, step = 102400, mean_episode_return = -0.013569, mean_episode_step = 48.357, total_loss = 217.02, entropy_loss = -9.9928, pg_loss = 101.61, baseline_loss = 125.4, learner_queue_size = 32, _tick = 39, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:16:19,002[0m][[34mroot[0m][[32mINFO[0m] - Step 104960 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 395.5, step = 104960, mean_episode_return = 0.057185, mean_episode_step = 52.034, total_loss = -523.1, entropy_loss = -10.167, pg_loss = -639.19, baseline_loss = 126.26, learner_queue_size = 32, _tick = 40, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:16:24,006[0m][[34mroot[0m][[32mINFO[0m] - Step 107520 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 400.5, step = 107520, mean_episode_return = 0.048065, mean_episode_step = 50.817, total_loss = 345.86, entropy_loss = -10.193, pg_loss = 185.6, baseline_loss = 170.46, learner_queue_size = 32, _tick = 41, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:16:29,011[0m][[34mroot[0m][[32mINFO[0m] - Step 107520 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 405.5, step = 107520, mean_episode_return = 0.048065, mean_episode_step = 50.817, total_loss = 345.86, entropy_loss = -10.193, pg_loss = 185.6, baseline_loss = 170.46, learner_queue_size = 32, _tick = 41, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:16:34,018[0m][[34mroot[0m][[32mINFO[0m] - Step 110080 @ 511.4 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (train_seconds = 410.5, step = 110080, mean_episode_return = 0.016493, mean_episode_step = 36.507, total_loss = -0.24192, entropy_loss = -10.204, pg_loss = -127.3, baseline_loss = 137.26, learner_queue_size = 32, _tick = 42, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:16:39,023[0m][[34mroot[0m][[32mINFO[0m] - Step 110080 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 415.5, step = 110080, mean_episode_return = 0.016493, mean_episode_step = 36.507, total_loss = -0.24192, entropy_loss = -10.204, pg_loss = -127.3, baseline_loss = 137.26, learner_queue_size = 32, _tick = 42, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:16:44,028[0m][[34mroot[0m][[32mINFO[0m] - Step 112640 @ 511.5 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 420.5, step = 112640, mean_episode_return = 0.02035, mean_episode_step = 54.398, total_loss = 440.12, entropy_loss = -10.081, pg_loss = 284.22, baseline_loss = 165.98, learner_queue_size = 32, _tick = 43, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:16:49,033[0m][[34mroot[0m][[32mINFO[0m] - Step 112640 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 425.5, step = 112640, mean_episode_return = 0.02035, mean_episode_step = 54.398, total_loss = 440.12, entropy_loss = -10.081, pg_loss = 284.22, baseline_loss = 165.98, learner_queue_size = 32, _tick = 43, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:16:54,038[0m][[34mroot[0m][[32mINFO[0m] - Step 115200 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 430.5, step = 115200, mean_episode_return = 0.036527, mean_episode_step = 64.2, total_loss = -287.53, entropy_loss = -10.043, pg_loss = -384.41, baseline_loss = 106.92, learner_queue_size = 32, _tick = 44, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:16:59,044[0m][[34mroot[0m][[32mINFO[0m] - Step 115200 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 435.5, step = 115200, mean_episode_return = 0.036527, mean_episode_step = 64.2, total_loss = -287.53, entropy_loss = -10.043, pg_loss = -384.41, baseline_loss = 106.92, learner_queue_size = 32, _tick = 44, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:17:04,049[0m][[34mroot[0m][[32mINFO[0m] - Step 117760 @ 511.5 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (train_seconds = 440.5, step = 117760, mean_episode_return = 0.015648, mean_episode_step = 36.836, total_loss = -132.05, entropy_loss = -10.017, pg_loss = -233.97, baseline_loss = 111.94, learner_queue_size = 32, _tick = 45, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:17:09,055[0m][[34mroot[0m][[32mINFO[0m] - Step 120320 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 445.5, step = 120320, mean_episode_return = 0.013967, mean_episode_step = 53.832, total_loss = 1670.9, entropy_loss = -9.9713, pg_loss = 1358.1, baseline_loss = 322.74, learner_queue_size = 32, _tick = 46, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:17:14,059[0m][[34mroot[0m][[32mINFO[0m] - Step 120320 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 450.5, step = 120320, mean_episode_return = 0.013967, mean_episode_step = 53.832, total_loss = 1670.9, entropy_loss = -9.9713, pg_loss = 1358.1, baseline_loss = 322.74, learner_queue_size = 32, _tick = 46, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:17:19,066[0m][[34mroot[0m][[32mINFO[0m] - Step 122880 @ 511.4 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 455.5, step = 122880, mean_episode_return = 0.092395, mean_episode_step = 56.934, total_loss = 1167.6, entropy_loss = -9.9545, pg_loss = 854.06, baseline_loss = 323.53, learner_queue_size = 32, _tick = 47, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:17:24,073[0m][[34mroot[0m][[32mINFO[0m] - Step 122880 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 460.5, step = 122880, mean_episode_return = 0.092395, mean_episode_step = 56.934, total_loss = 1167.6, entropy_loss = -9.9545, pg_loss = 854.06, baseline_loss = 323.53, learner_queue_size = 32, _tick = 47, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:17:29,078[0m][[34mroot[0m][[32mINFO[0m] - Saving checkpoint to /opt/minihack/checkpoint_0.25.tar[0m
[[36m2025-01-18 17:17:29,127[0m][[34mroot[0m][[32mINFO[0m] - Step 125440 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 465.5, step = 125440, mean_episode_return = 0.020509, mean_episode_step = 49.039, total_loss = -599.63, entropy_loss = -9.8717, pg_loss = -696.07, baseline_loss = 106.3, learner_queue_size = 32, _tick = 48, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:17:34,128[0m][[34mroot[0m][[32mINFO[0m] - Step 125440 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 470.6, step = 125440, mean_episode_return = 0.020509, mean_episode_step = 49.039, total_loss = -599.63, entropy_loss = -9.8717, pg_loss = -696.07, baseline_loss = 106.3, learner_queue_size = 32, _tick = 48, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:17:39,134[0m][[34mroot[0m][[32mINFO[0m] - Step 128000 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 475.6, step = 128000, mean_episode_return = 0.06802, mean_episode_step = 46.034, total_loss = 655.2, entropy_loss = -9.878, pg_loss = 461.58, baseline_loss = 203.5, learner_queue_size = 32, _tick = 49, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:17:44,140[0m][[34mroot[0m][[32mINFO[0m] - Step 128000 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 480.6, step = 128000, mean_episode_return = 0.06802, mean_episode_step = 46.034, total_loss = 655.2, entropy_loss = -9.878, pg_loss = 461.58, baseline_loss = 203.5, learner_queue_size = 32, _tick = 49, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:17:49,146[0m][[34mroot[0m][[32mINFO[0m] - Step 130560 @ 511.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (train_seconds = 485.6, step = 130560, mean_episode_return = 0.053046, mean_episode_step = 63.662, total_loss = -278.49, entropy_loss = -9.8558, pg_loss = -404.97, baseline_loss = 136.34, learner_queue_size = 32, _tick = 50, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:17:54,152[0m][[34mroot[0m][[32mINFO[0m] - Step 133120 @ 511.5 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (train_seconds = 490.6, step = 133120, mean_episode_return = 0.049952, mean_episode_step = 55.179, total_loss = 234.72, entropy_loss = -9.7965, pg_loss = 69.076, baseline_loss = 175.44, learner_queue_size = 32, _tick = 51, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:17:59,157[0m][[34mroot[0m][[32mINFO[0m] - Step 133120 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 495.6, step = 133120, mean_episode_return = 0.049952, mean_episode_step = 55.179, total_loss = 234.72, entropy_loss = -9.7965, pg_loss = 69.076, baseline_loss = 175.44, learner_queue_size = 32, _tick = 51, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:18:04,162[0m][[34mroot[0m][[32mINFO[0m] - Step 135680 @ 511.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 500.6, step = 135680, mean_episode_return = 0.048617, mean_episode_step = 57.209, total_loss = 342.89, entropy_loss = -9.7553, pg_loss = 181.94, baseline_loss = 170.7, learner_queue_size = 32, _tick = 52, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:18:09,167[0m][[34mroot[0m][[32mINFO[0m] - Step 135680 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 505.6, step = 135680, mean_episode_return = 0.048617, mean_episode_step = 57.209, total_loss = 342.89, entropy_loss = -9.7553, pg_loss = 181.94, baseline_loss = 170.7, learner_queue_size = 32, _tick = 52, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:18:14,173[0m][[34mroot[0m][[32mINFO[0m] - Step 138240 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 510.6, step = 138240, mean_episode_return = 0.026292, mean_episode_step = 51.171, total_loss = -152.87, entropy_loss = -9.7813, pg_loss = -315.04, baseline_loss = 171.95, learner_queue_size = 32, _tick = 53, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:18:19,177[0m][[34mroot[0m][[32mINFO[0m] - Step 138240 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 515.6, step = 138240, mean_episode_return = 0.026292, mean_episode_step = 51.171, total_loss = -152.87, entropy_loss = -9.7813, pg_loss = -315.04, baseline_loss = 171.95, learner_queue_size = 32, _tick = 53, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:18:24,183[0m][[34mroot[0m][[32mINFO[0m] - Step 140800 @ 511.4 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 520.6, step = 140800, mean_episode_return = 0.0070741, mean_episode_step = 55.153, total_loss = 452.52, entropy_loss = -9.6935, pg_loss = 268.25, baseline_loss = 193.97, learner_queue_size = 32, _tick = 54, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:18:29,188[0m][[34mroot[0m][[32mINFO[0m] - Step 140800 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 525.6, step = 140800, mean_episode_return = 0.0070741, mean_episode_step = 55.153, total_loss = 452.52, entropy_loss = -9.6935, pg_loss = 268.25, baseline_loss = 193.97, learner_queue_size = 32, _tick = 54, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:18:34,193[0m][[34mroot[0m][[32mINFO[0m] - Step 143360 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 530.7, step = 143360, mean_episode_return = 0.033419, mean_episode_step = 51.826, total_loss = 106.61, entropy_loss = -9.5734, pg_loss = -23.506, baseline_loss = 139.69, learner_queue_size = 32, _tick = 55, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:18:39,198[0m][[34mroot[0m][[32mINFO[0m] - Step 145920 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 535.7, step = 145920, mean_episode_return = 0.036696, mean_episode_step = 51.327, total_loss = -42.09, entropy_loss = -9.4171, pg_loss = -136.16, baseline_loss = 103.49, learner_queue_size = 32, _tick = 56, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:18:44,203[0m][[34mroot[0m][[32mINFO[0m] - Step 145920 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 540.7, step = 145920, mean_episode_return = 0.036696, mean_episode_step = 51.327, total_loss = -42.09, entropy_loss = -9.4171, pg_loss = -136.16, baseline_loss = 103.49, learner_queue_size = 32, _tick = 56, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:18:49,209[0m][[34mroot[0m][[32mINFO[0m] - Step 148480 @ 511.4 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 545.7, step = 148480, mean_episode_return = 0.14437, mean_episode_step = 65.318, total_loss = -107.72, entropy_loss = -9.3193, pg_loss = -162.58, baseline_loss = 64.178, learner_queue_size = 32, _tick = 57, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:18:54,214[0m][[34mroot[0m][[32mINFO[0m] - Step 148480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 550.7, step = 148480, mean_episode_return = 0.14437, mean_episode_step = 65.318, total_loss = -107.72, entropy_loss = -9.3193, pg_loss = -162.58, baseline_loss = 64.178, learner_queue_size = 32, _tick = 57, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:18:59,219[0m][[34mroot[0m][[32mINFO[0m] - Step 151040 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 555.7, step = 151040, mean_episode_return = 0.0209, mean_episode_step = 49.02, total_loss = -180.69, entropy_loss = -9.1906, pg_loss = -230.73, baseline_loss = 59.23, learner_queue_size = 32, _tick = 58, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:19:04,225[0m][[34mroot[0m][[32mINFO[0m] - Step 151040 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 560.7, step = 151040, mean_episode_return = 0.0209, mean_episode_step = 49.02, total_loss = -180.69, entropy_loss = -9.1906, pg_loss = -230.73, baseline_loss = 59.23, learner_queue_size = 32, _tick = 58, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:19:09,229[0m][[34mroot[0m][[32mINFO[0m] - Step 153600 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 565.7, step = 153600, mean_episode_return = 0.085645, mean_episode_step = 56.118, total_loss = -156.75, entropy_loss = -9.2243, pg_loss = -226.25, baseline_loss = 78.715, learner_queue_size = 32, _tick = 59, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:19:14,235[0m][[34mroot[0m][[32mINFO[0m] - Step 153600 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 570.7, step = 153600, mean_episode_return = 0.085645, mean_episode_step = 56.118, total_loss = -156.75, entropy_loss = -9.2243, pg_loss = -226.25, baseline_loss = 78.715, learner_queue_size = 32, _tick = 59, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:19:19,241[0m][[34mroot[0m][[32mINFO[0m] - Step 156160 @ 511.4 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 575.7, step = 156160, mean_episode_return = 0.12279, mean_episode_step = 68.036, total_loss = 119.12, entropy_loss = -9.118, pg_loss = 62.436, baseline_loss = 65.797, learner_queue_size = 32, _tick = 60, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:19:24,246[0m][[34mroot[0m][[32mINFO[0m] - Step 158720 @ 511.4 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (train_seconds = 580.7, step = 158720, mean_episode_return = 0.099577, mean_episode_step = 51.943, total_loss = 284.73, entropy_loss = -9.0387, pg_loss = 176.82, baseline_loss = 116.95, learner_queue_size = 32, _tick = 61, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:19:29,251[0m][[34mroot[0m][[32mINFO[0m] - Step 158720 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 585.7, step = 158720, mean_episode_return = 0.099577, mean_episode_step = 51.943, total_loss = 284.73, entropy_loss = -9.0387, pg_loss = 176.82, baseline_loss = 116.95, learner_queue_size = 32, _tick = 61, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:19:34,258[0m][[34mroot[0m][[32mINFO[0m] - Step 161280 @ 511.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (train_seconds = 590.7, step = 161280, mean_episode_return = 0.068737, mean_episode_step = 50.901, total_loss = 952.46, entropy_loss = -8.9728, pg_loss = 734.56, baseline_loss = 226.87, learner_queue_size = 32, _tick = 62, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:19:39,263[0m][[34mroot[0m][[32mINFO[0m] - Step 161280 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 595.7, step = 161280, mean_episode_return = 0.068737, mean_episode_step = 50.901, total_loss = 952.46, entropy_loss = -8.9728, pg_loss = 734.56, baseline_loss = 226.87, learner_queue_size = 32, _tick = 62, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:19:44,268[0m][[34mroot[0m][[32mINFO[0m] - Saving checkpoint to /opt/minihack/checkpoint.tar[0m
[[36m2025-01-18 17:19:44,307[0m][[34mroot[0m][[32mINFO[0m] - Step 163840 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 600.7, step = 163840, mean_episode_return = 0.10535, mean_episode_step = 68.641, total_loss = -96.917, entropy_loss = -8.962, pg_loss = -187.65, baseline_loss = 99.698, learner_queue_size = 32, _tick = 63, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:19:49,312[0m][[34mroot[0m][[32mINFO[0m] - Step 163840 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 605.8, step = 163840, mean_episode_return = 0.10535, mean_episode_step = 68.641, total_loss = -96.917, entropy_loss = -8.962, pg_loss = -187.65, baseline_loss = 99.698, learner_queue_size = 32, _tick = 63, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:19:54,317[0m][[34mroot[0m][[32mINFO[0m] - Step 166400 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 610.8, step = 166400, mean_episode_return = 0.059184, mean_episode_step = 61.547, total_loss = 54.76, entropy_loss = -8.8643, pg_loss = -64.038, baseline_loss = 127.66, learner_queue_size = 32, _tick = 64, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:19:59,322[0m][[34mroot[0m][[32mINFO[0m] - Step 166400 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 615.8, step = 166400, mean_episode_return = 0.059184, mean_episode_step = 61.547, total_loss = 54.76, entropy_loss = -8.8643, pg_loss = -64.038, baseline_loss = 127.66, learner_queue_size = 32, _tick = 64, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:20:04,327[0m][[34mroot[0m][[32mINFO[0m] - Step 168960 @ 511.5 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (train_seconds = 620.8, step = 168960, mean_episode_return = 0.054133, mean_episode_step = 58.141, total_loss = -40.144, entropy_loss = -8.6864, pg_loss = -115.01, baseline_loss = 83.55, learner_queue_size = 32, _tick = 65, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:20:09,333[0m][[34mroot[0m][[32mINFO[0m] - Step 168960 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 625.8, step = 168960, mean_episode_return = 0.054133, mean_episode_step = 58.141, total_loss = -40.144, entropy_loss = -8.6864, pg_loss = -115.01, baseline_loss = 83.55, learner_queue_size = 32, _tick = 65, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:20:14,337[0m][[34mroot[0m][[32mINFO[0m] - Step 171520 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 630.8, step = 171520, mean_episode_return = 0.075233, mean_episode_step = 68.946, total_loss = -22.448, entropy_loss = -8.6391, pg_loss = -102.13, baseline_loss = 88.324, learner_queue_size = 32, _tick = 66, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:20:19,343[0m][[34mroot[0m][[32mINFO[0m] - Step 174080 @ 511.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (train_seconds = 635.8, step = 174080, mean_episode_return = 0.079983, mean_episode_step = 49.947, total_loss = 81.44, entropy_loss = -8.6661, pg_loss = -10.15, baseline_loss = 100.26, learner_queue_size = 32, _tick = 67, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:20:24,347[0m][[34mroot[0m][[32mINFO[0m] - Step 174080 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 640.8, step = 174080, mean_episode_return = 0.079983, mean_episode_step = 49.947, total_loss = 81.44, entropy_loss = -8.6661, pg_loss = -10.15, baseline_loss = 100.26, learner_queue_size = 32, _tick = 67, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:20:29,353[0m][[34mroot[0m][[32mINFO[0m] - Step 176640 @ 511.4 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (train_seconds = 645.8, step = 176640, mean_episode_return = 0.06625, mean_episode_step = 57.808, total_loss = 326.52, entropy_loss = -8.5901, pg_loss = 186.96, baseline_loss = 148.15, learner_queue_size = 32, _tick = 68, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:20:34,358[0m][[34mroot[0m][[32mINFO[0m] - Step 176640 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 650.8, step = 176640, mean_episode_return = 0.06625, mean_episode_step = 57.808, total_loss = 326.52, entropy_loss = -8.5901, pg_loss = 186.96, baseline_loss = 148.15, learner_queue_size = 32, _tick = 68, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:20:39,364[0m][[34mroot[0m][[32mINFO[0m] - Step 179200 @ 511.5 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (train_seconds = 655.8, step = 179200, mean_episode_return = 0.0095098, mean_episode_step = 53.626, total_loss = -36.908, entropy_loss = -8.5277, pg_loss = -134.1, baseline_loss = 105.72, learner_queue_size = 32, _tick = 69, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:20:44,370[0m][[34mroot[0m][[32mINFO[0m] - Step 179200 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 660.8, step = 179200, mean_episode_return = 0.0095098, mean_episode_step = 53.626, total_loss = -36.908, entropy_loss = -8.5277, pg_loss = -134.1, baseline_loss = 105.72, learner_queue_size = 32, _tick = 69, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:20:49,378[0m][[34mroot[0m][[32mINFO[0m] - Step 181760 @ 511.2 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (train_seconds = 665.8, step = 181760, mean_episode_return = 0.018154, mean_episode_step = 60.093, total_loss = 129.0, entropy_loss = -8.4345, pg_loss = 16.257, baseline_loss = 121.18, learner_queue_size = 32, _tick = 70, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:20:54,383[0m][[34mroot[0m][[32mINFO[0m] - Step 181760 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 670.8, step = 181760, mean_episode_return = 0.018154, mean_episode_step = 60.093, total_loss = 129.0, entropy_loss = -8.4345, pg_loss = 16.257, baseline_loss = 121.18, learner_queue_size = 32, _tick = 70, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:20:59,388[0m][[34mroot[0m][[32mINFO[0m] - Step 184320 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 675.8, step = 184320, mean_episode_return = 0.15802, mean_episode_step = 67.333, total_loss = 749.16, entropy_loss = -8.311, pg_loss = 585.25, baseline_loss = 172.22, learner_queue_size = 32, _tick = 71, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:21:04,393[0m][[34mroot[0m][[32mINFO[0m] - Step 186880 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 680.9, step = 186880, mean_episode_return = 0.1702, mean_episode_step = 74.534, total_loss = -16.282, entropy_loss = -8.3621, pg_loss = -134.13, baseline_loss = 126.21, learner_queue_size = 32, _tick = 72, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:21:09,399[0m][[34mroot[0m][[32mINFO[0m] - Step 186880 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 685.9, step = 186880, mean_episode_return = 0.1702, mean_episode_step = 74.534, total_loss = -16.282, entropy_loss = -8.3621, pg_loss = -134.13, baseline_loss = 126.21, learner_queue_size = 32, _tick = 72, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:21:14,406[0m][[34mroot[0m][[32mINFO[0m] - Step 189440 @ 511.3 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (train_seconds = 690.9, step = 189440, mean_episode_return = 0.15197, mean_episode_step = 63.66, total_loss = 1061.5, entropy_loss = -8.4308, pg_loss = 787.94, baseline_loss = 281.98, learner_queue_size = 32, _tick = 73, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:21:19,411[0m][[34mroot[0m][[32mINFO[0m] - Step 189440 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 695.9, step = 189440, mean_episode_return = 0.15197, mean_episode_step = 63.66, total_loss = 1061.5, entropy_loss = -8.4308, pg_loss = 787.94, baseline_loss = 281.98, learner_queue_size = 32, _tick = 73, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:21:24,416[0m][[34mroot[0m][[32mINFO[0m] - Step 192000 @ 511.5 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (train_seconds = 700.9, step = 192000, mean_episode_return = 0.060842, mean_episode_step = 65.623, total_loss = -515.0, entropy_loss = -8.4046, pg_loss = -601.26, baseline_loss = 94.665, learner_queue_size = 32, _tick = 74, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:21:29,421[0m][[34mroot[0m][[32mINFO[0m] - Step 194560 @ 511.5 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 705.9, step = 194560, mean_episode_return = 0.18206, mean_episode_step = 65.767, total_loss = 1171.5, entropy_loss = -8.4282, pg_loss = 917.6, baseline_loss = 262.3, learner_queue_size = 32, _tick = 75, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:21:34,426[0m][[34mroot[0m][[32mINFO[0m] - Step 194560 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 710.9, step = 194560, mean_episode_return = 0.18206, mean_episode_step = 65.767, total_loss = 1171.5, entropy_loss = -8.4282, pg_loss = 917.6, baseline_loss = 262.3, learner_queue_size = 32, _tick = 75, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:21:39,431[0m][[34mroot[0m][[32mINFO[0m] - Step 197120 @ 511.5 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (train_seconds = 715.9, step = 197120, mean_episode_return = 0.10509, mean_episode_step = 48.089, total_loss = 614.22, entropy_loss = -8.4541, pg_loss = 394.35, baseline_loss = 228.32, learner_queue_size = 32, _tick = 76, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:21:44,437[0m][[34mroot[0m][[32mINFO[0m] - Step 197120 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 720.9, step = 197120, mean_episode_return = 0.10509, mean_episode_step = 48.089, total_loss = 614.22, entropy_loss = -8.4541, pg_loss = 394.35, baseline_loss = 228.32, learner_queue_size = 32, _tick = 76, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:21:49,441[0m][[34mroot[0m][[32mINFO[0m] - Step 199680 @ 511.5 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 725.9, step = 199680, mean_episode_return = 0.1617, mean_episode_step = 59.921, total_loss = 269.05, entropy_loss = -8.3728, pg_loss = 64.07, baseline_loss = 213.35, learner_queue_size = 32, _tick = 77, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:21:54,446[0m][[34mroot[0m][[32mINFO[0m] - Step 199680 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 730.9, step = 199680, mean_episode_return = 0.1617, mean_episode_step = 59.921, total_loss = 269.05, entropy_loss = -8.3728, pg_loss = 64.07, baseline_loss = 213.35, learner_queue_size = 32, _tick = 77, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:21:59,451[0m][[34mroot[0m][[32mINFO[0m] - Step 202240 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 735.9, step = 202240, mean_episode_return = 0.15484, mean_episode_step = 66.385, total_loss = -517.67, entropy_loss = -8.4191, pg_loss = -646.24, baseline_loss = 136.99, learner_queue_size = 32, _tick = 78, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:22:04,456[0m][[34mroot[0m][[32mINFO[0m] - Step 202240 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 740.9, step = 202240, mean_episode_return = 0.15484, mean_episode_step = 66.385, total_loss = -517.67, entropy_loss = -8.4191, pg_loss = -646.24, baseline_loss = 136.99, learner_queue_size = 32, _tick = 78, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:22:09,464[0m][[34mroot[0m][[32mINFO[0m] - Step 204800 @ 511.2 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 745.9, step = 204800, mean_episode_return = 0.037333, mean_episode_step = 73.5, total_loss = 56.702, entropy_loss = -8.421, pg_loss = -94.199, baseline_loss = 159.32, learner_queue_size = 32, _tick = 79, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:22:14,470[0m][[34mroot[0m][[32mINFO[0m] - Step 207360 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 750.9, step = 207360, mean_episode_return = 0.19427, mean_episode_step = 74.53, total_loss = 31.34, entropy_loss = -8.4531, pg_loss = -62.722, baseline_loss = 102.52, learner_queue_size = 32, _tick = 80, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:22:19,475[0m][[34mroot[0m][[32mINFO[0m] - Step 207360 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 755.9, step = 207360, mean_episode_return = 0.19427, mean_episode_step = 74.53, total_loss = 31.34, entropy_loss = -8.4531, pg_loss = -62.722, baseline_loss = 102.52, learner_queue_size = 32, _tick = 80, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:22:24,482[0m][[34mroot[0m][[32mINFO[0m] - Step 209920 @ 511.4 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 760.9, step = 209920, mean_episode_return = 0.18821, mean_episode_step = 72.186, total_loss = 905.49, entropy_loss = -8.3267, pg_loss = 679.57, baseline_loss = 234.25, learner_queue_size = 32, _tick = 81, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:22:29,487[0m][[34mroot[0m][[32mINFO[0m] - Step 209920 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 765.9, step = 209920, mean_episode_return = 0.18821, mean_episode_step = 72.186, total_loss = 905.49, entropy_loss = -8.3267, pg_loss = 679.57, baseline_loss = 234.25, learner_queue_size = 32, _tick = 81, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:22:34,493[0m][[34mroot[0m][[32mINFO[0m] - Step 212480 @ 511.4 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 771.0, step = 212480, mean_episode_return = 0.097, mean_episode_step = 59.094, total_loss = -332.14, entropy_loss = -8.3254, pg_loss = -421.09, baseline_loss = 97.271, learner_queue_size = 32, _tick = 82, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:22:39,497[0m][[34mroot[0m][[32mINFO[0m] - Step 212480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 776.0, step = 212480, mean_episode_return = 0.097, mean_episode_step = 59.094, total_loss = -332.14, entropy_loss = -8.3254, pg_loss = -421.09, baseline_loss = 97.271, learner_queue_size = 32, _tick = 82, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:22:44,502[0m][[34mroot[0m][[32mINFO[0m] - Step 215040 @ 511.4 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 781.0, step = 215040, mean_episode_return = 0.12485, mean_episode_step = 67.135, total_loss = -227.99, entropy_loss = -8.3149, pg_loss = -317.19, baseline_loss = 97.508, learner_queue_size = 32, _tick = 83, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:22:49,507[0m][[34mroot[0m][[32mINFO[0m] - Step 217600 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 786.0, step = 217600, mean_episode_return = 0.047959, mean_episode_step = 56.222, total_loss = -609.34, entropy_loss = -8.3511, pg_loss = -661.98, baseline_loss = 60.996, learner_queue_size = 32, _tick = 84, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:22:54,512[0m][[34mroot[0m][[32mINFO[0m] - Step 217600 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 791.0, step = 217600, mean_episode_return = 0.047959, mean_episode_step = 56.222, total_loss = -609.34, entropy_loss = -8.3511, pg_loss = -661.98, baseline_loss = 60.996, learner_queue_size = 32, _tick = 84, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:22:59,517[0m][[34mroot[0m][[32mINFO[0m] - Step 220160 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 796.0, step = 220160, mean_episode_return = 0.10615, mean_episode_step = 74.18, total_loss = -271.0, entropy_loss = -8.3754, pg_loss = -333.46, baseline_loss = 70.833, learner_queue_size = 32, _tick = 85, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:23:04,522[0m][[34mroot[0m][[32mINFO[0m] - Step 220160 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 801.0, step = 220160, mean_episode_return = 0.10615, mean_episode_step = 74.18, total_loss = -271.0, entropy_loss = -8.3754, pg_loss = -333.46, baseline_loss = 70.833, learner_queue_size = 32, _tick = 85, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:23:09,527[0m][[34mroot[0m][[32mINFO[0m] - Step 222720 @ 511.5 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (train_seconds = 806.0, step = 222720, mean_episode_return = 0.18613, mean_episode_step = 69.168, total_loss = 350.53, entropy_loss = -8.4222, pg_loss = 233.9, baseline_loss = 125.06, learner_queue_size = 32, _tick = 86, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:23:14,532[0m][[34mroot[0m][[32mINFO[0m] - Step 222720 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 811.0, step = 222720, mean_episode_return = 0.18613, mean_episode_step = 69.168, total_loss = 350.53, entropy_loss = -8.4222, pg_loss = 233.9, baseline_loss = 125.06, learner_queue_size = 32, _tick = 86, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:23:19,538[0m][[34mroot[0m][[32mINFO[0m] - Step 225280 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 816.0, step = 225280, mean_episode_return = 0.062429, mean_episode_step = 57.214, total_loss = -81.271, entropy_loss = -8.3728, pg_loss = -176.74, baseline_loss = 103.84, learner_queue_size = 32, _tick = 87, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:23:24,542[0m][[34mroot[0m][[32mINFO[0m] - Step 225280 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 821.0, step = 225280, mean_episode_return = 0.062429, mean_episode_step = 57.214, total_loss = -81.271, entropy_loss = -8.3728, pg_loss = -176.74, baseline_loss = 103.84, learner_queue_size = 32, _tick = 87, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:23:29,547[0m][[34mroot[0m][[32mINFO[0m] - Step 227840 @ 511.5 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (train_seconds = 826.0, step = 227840, mean_episode_return = 0.11651, mean_episode_step = 62.719, total_loss = 1720.5, entropy_loss = -8.3355, pg_loss = 1407.2, baseline_loss = 321.63, learner_queue_size = 32, _tick = 88, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:23:34,553[0m][[34mroot[0m][[32mINFO[0m] - Step 227840 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 831.0, step = 227840, mean_episode_return = 0.11651, mean_episode_step = 62.719, total_loss = 1720.5, entropy_loss = -8.3355, pg_loss = 1407.2, baseline_loss = 321.63, learner_queue_size = 32, _tick = 88, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:23:39,559[0m][[34mroot[0m][[32mINFO[0m] - Step 230400 @ 511.4 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (train_seconds = 836.0, step = 230400, mean_episode_return = 0.2125, mean_episode_step = 65.454, total_loss = 1064.3, entropy_loss = -8.3354, pg_loss = 825.31, baseline_loss = 247.28, learner_queue_size = 32, _tick = 89, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:23:44,565[0m][[34mroot[0m][[32mINFO[0m] - Step 232960 @ 511.4 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 841.0, step = 232960, mean_episode_return = 0.075686, mean_episode_step = 66.425, total_loss = 884.71, entropy_loss = -8.3169, pg_loss = 670.24, baseline_loss = 222.78, learner_queue_size = 32, _tick = 90, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:23:49,570[0m][[34mroot[0m][[32mINFO[0m] - Step 232960 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 846.0, step = 232960, mean_episode_return = 0.075686, mean_episode_step = 66.425, total_loss = 884.71, entropy_loss = -8.3169, pg_loss = 670.24, baseline_loss = 222.78, learner_queue_size = 32, _tick = 90, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:23:54,575[0m][[34mroot[0m][[32mINFO[0m] - Step 235520 @ 511.5 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (train_seconds = 851.0, step = 235520, mean_episode_return = 0.14208, mean_episode_step = 60.862, total_loss = -596.72, entropy_loss = -8.3031, pg_loss = -683.68, baseline_loss = 95.265, learner_queue_size = 32, _tick = 91, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:23:59,581[0m][[34mroot[0m][[32mINFO[0m] - Step 235520 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 856.0, step = 235520, mean_episode_return = 0.14208, mean_episode_step = 60.862, total_loss = -596.72, entropy_loss = -8.3031, pg_loss = -683.68, baseline_loss = 95.265, learner_queue_size = 32, _tick = 91, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:24:04,586[0m][[34mroot[0m][[32mINFO[0m] - Step 238080 @ 511.4 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 861.0, step = 238080, mean_episode_return = 0.19179, mean_episode_step = 70.802, total_loss = 1587.9, entropy_loss = -8.3104, pg_loss = 1230.5, baseline_loss = 365.69, learner_queue_size = 32, _tick = 92, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:24:09,591[0m][[34mroot[0m][[32mINFO[0m] - Step 238080 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 866.1, step = 238080, mean_episode_return = 0.19179, mean_episode_step = 70.802, total_loss = 1587.9, entropy_loss = -8.3104, pg_loss = 1230.5, baseline_loss = 365.69, learner_queue_size = 32, _tick = 92, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:24:14,597[0m][[34mroot[0m][[32mINFO[0m] - Step 240640 @ 511.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 871.1, step = 240640, mean_episode_return = 0.13755, mean_episode_step = 64.561, total_loss = 191.82, entropy_loss = -8.4107, pg_loss = 58.447, baseline_loss = 141.79, learner_queue_size = 32, _tick = 93, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:24:19,603[0m][[34mroot[0m][[32mINFO[0m] - Step 240640 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 876.1, step = 240640, mean_episode_return = 0.13755, mean_episode_step = 64.561, total_loss = 191.82, entropy_loss = -8.4107, pg_loss = 58.447, baseline_loss = 141.79, learner_queue_size = 32, _tick = 93, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:24:24,608[0m][[34mroot[0m][[32mINFO[0m] - Step 243200 @ 511.5 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (train_seconds = 881.1, step = 243200, mean_episode_return = 0.11678, mean_episode_step = 62.655, total_loss = 227.17, entropy_loss = -8.4881, pg_loss = 98.309, baseline_loss = 137.34, learner_queue_size = 32, _tick = 94, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:24:29,613[0m][[34mroot[0m][[32mINFO[0m] - Step 243200 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 886.1, step = 243200, mean_episode_return = 0.11678, mean_episode_step = 62.655, total_loss = 227.17, entropy_loss = -8.4881, pg_loss = 98.309, baseline_loss = 137.34, learner_queue_size = 32, _tick = 94, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:24:34,618[0m][[34mroot[0m][[32mINFO[0m] - Step 243200 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 891.1, step = 243200, mean_episode_return = 0.11678, mean_episode_step = 62.655, total_loss = 227.17, entropy_loss = -8.4881, pg_loss = 98.309, baseline_loss = 137.34, learner_queue_size = 32, _tick = 94, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:24:39,624[0m][[34mroot[0m][[32mINFO[0m] - Step 245760 @ 511.4 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 896.1, step = 245760, mean_episode_return = 0.22683, mean_episode_step = 76.748, total_loss = -134.92, entropy_loss = -8.4154, pg_loss = -245.15, baseline_loss = 118.64, learner_queue_size = 32, _tick = 95, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:24:44,630[0m][[34mroot[0m][[32mINFO[0m] - Step 245760 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 901.1, step = 245760, mean_episode_return = 0.22683, mean_episode_step = 76.748, total_loss = -134.92, entropy_loss = -8.4154, pg_loss = -245.15, baseline_loss = 118.64, learner_queue_size = 32, _tick = 95, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:24:49,729[0m][[34mroot[0m][[32mINFO[0m] - Step 248320 @ 502.0 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 906.2, step = 248320, mean_episode_return = 0.14763, mean_episode_step = 66.947, total_loss = -458.57, entropy_loss = -8.4275, pg_loss = -541.57, baseline_loss = 91.427, learner_queue_size = 32, _tick = 96, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:24:54,734[0m][[34mroot[0m][[32mINFO[0m] - Step 248320 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 911.2, step = 248320, mean_episode_return = 0.14763, mean_episode_step = 66.947, total_loss = -458.57, entropy_loss = -8.4275, pg_loss = -541.57, baseline_loss = 91.427, learner_queue_size = 32, _tick = 96, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:24:59,739[0m][[34mroot[0m][[32mINFO[0m] - Saving checkpoint to /opt/minihack/checkpoint_0.5.tar[0m
[[36m2025-01-18 17:24:59,796[0m][[34mroot[0m][[32mINFO[0m] - Step 250880 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 916.2, step = 250880, mean_episode_return = 0.15347, mean_episode_step = 62.664, total_loss = 285.46, entropy_loss = -8.4285, pg_loss = 131.16, baseline_loss = 162.73, learner_queue_size = 32, _tick = 97, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:25:04,801[0m][[34mroot[0m][[32mINFO[0m] - Step 250880 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 921.3, step = 250880, mean_episode_return = 0.15347, mean_episode_step = 62.664, total_loss = 285.46, entropy_loss = -8.4285, pg_loss = 131.16, baseline_loss = 162.73, learner_queue_size = 32, _tick = 97, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:25:09,877[0m][[34mroot[0m][[32mINFO[0m] - Step 253440 @ 504.4 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (train_seconds = 926.3, step = 253440, mean_episode_return = 0.11129, mean_episode_step = 75.543, total_loss = 225.61, entropy_loss = -8.3879, pg_loss = 48.578, baseline_loss = 185.42, learner_queue_size = 32, _tick = 98, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:25:14,882[0m][[34mroot[0m][[32mINFO[0m] - Step 253440 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 931.3, step = 253440, mean_episode_return = 0.11129, mean_episode_step = 75.543, total_loss = 225.61, entropy_loss = -8.3879, pg_loss = 48.578, baseline_loss = 185.42, learner_queue_size = 32, _tick = 98, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:25:19,888[0m][[34mroot[0m][[32mINFO[0m] - Step 256000 @ 511.4 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (train_seconds = 936.3, step = 256000, mean_episode_return = 0.095565, mean_episode_step = 57.668, total_loss = 143.78, entropy_loss = -8.4326, pg_loss = 15.871, baseline_loss = 136.34, learner_queue_size = 32, _tick = 99, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:25:24,893[0m][[34mroot[0m][[32mINFO[0m] - Step 256000 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 941.4, step = 256000, mean_episode_return = 0.095565, mean_episode_step = 57.668, total_loss = 143.78, entropy_loss = -8.4326, pg_loss = 15.871, baseline_loss = 136.34, learner_queue_size = 32, _tick = 99, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:25:29,898[0m][[34mroot[0m][[32mINFO[0m] - Step 258560 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 946.4, step = 258560, mean_episode_return = 0.13124, mean_episode_step = 44.731, total_loss = 308.98, entropy_loss = -8.5622, pg_loss = 151.17, baseline_loss = 166.37, learner_queue_size = 32, _tick = 100, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:25:34,903[0m][[34mroot[0m][[32mINFO[0m] - Step 258560 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 951.4, step = 258560, mean_episode_return = 0.13124, mean_episode_step = 44.731, total_loss = 308.98, entropy_loss = -8.5622, pg_loss = 151.17, baseline_loss = 166.37, learner_queue_size = 32, _tick = 100, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:25:39,908[0m][[34mroot[0m][[32mINFO[0m] - Step 261120 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 956.4, step = 261120, mean_episode_return = 0.17875, mean_episode_step = 67.327, total_loss = 64.848, entropy_loss = -8.6544, pg_loss = -9.9743, baseline_loss = 83.477, learner_queue_size = 32, _tick = 101, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:25:44,914[0m][[34mroot[0m][[32mINFO[0m] - Step 261120 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 961.4, step = 261120, mean_episode_return = 0.17875, mean_episode_step = 67.327, total_loss = 64.848, entropy_loss = -8.6544, pg_loss = -9.9743, baseline_loss = 83.477, learner_queue_size = 32, _tick = 101, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:25:49,918[0m][[34mroot[0m][[32mINFO[0m] - Step 263680 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 966.4, step = 263680, mean_episode_return = 0.086108, mean_episode_step = 64.562, total_loss = 684.27, entropy_loss = -8.6188, pg_loss = 537.37, baseline_loss = 155.52, learner_queue_size = 32, _tick = 102, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:25:54,923[0m][[34mroot[0m][[32mINFO[0m] - Step 263680 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 971.4, step = 263680, mean_episode_return = 0.086108, mean_episode_step = 64.562, total_loss = 684.27, entropy_loss = -8.6188, pg_loss = 537.37, baseline_loss = 155.52, learner_queue_size = 32, _tick = 102, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:25:59,929[0m][[34mroot[0m][[32mINFO[0m] - Step 266240 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 976.4, step = 266240, mean_episode_return = 0.079029, mean_episode_step = 73.197, total_loss = -286.62, entropy_loss = -8.6036, pg_loss = -330.14, baseline_loss = 52.126, learner_queue_size = 32, _tick = 103, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:26:04,933[0m][[34mroot[0m][[32mINFO[0m] - Step 266240 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 981.4, step = 266240, mean_episode_return = 0.079029, mean_episode_step = 73.197, total_loss = -286.62, entropy_loss = -8.6036, pg_loss = -330.14, baseline_loss = 52.126, learner_queue_size = 32, _tick = 103, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:26:09,940[0m][[34mroot[0m][[32mINFO[0m] - Step 268800 @ 511.3 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 986.4, step = 268800, mean_episode_return = 0.076446, mean_episode_step = 48.902, total_loss = -23.87, entropy_loss = -8.6326, pg_loss = -126.57, baseline_loss = 111.33, learner_queue_size = 32, _tick = 104, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:26:14,946[0m][[34mroot[0m][[32mINFO[0m] - Step 268800 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 991.4, step = 268800, mean_episode_return = 0.076446, mean_episode_step = 48.902, total_loss = -23.87, entropy_loss = -8.6326, pg_loss = -126.57, baseline_loss = 111.33, learner_queue_size = 32, _tick = 104, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:26:19,950[0m][[34mroot[0m][[32mINFO[0m] - Step 271360 @ 511.5 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 996.4, step = 271360, mean_episode_return = 0.16298, mean_episode_step = 75.707, total_loss = -309.53, entropy_loss = -8.5484, pg_loss = -388.81, baseline_loss = 87.826, learner_queue_size = 32, _tick = 105, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:26:24,955[0m][[34mroot[0m][[32mINFO[0m] - Step 271360 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1001.4, step = 271360, mean_episode_return = 0.16298, mean_episode_step = 75.707, total_loss = -309.53, entropy_loss = -8.5484, pg_loss = -388.81, baseline_loss = 87.826, learner_queue_size = 32, _tick = 105, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:26:29,961[0m][[34mroot[0m][[32mINFO[0m] - Step 273920 @ 511.5 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (train_seconds = 1006.4, step = 273920, mean_episode_return = 0.23156, mean_episode_step = 70.653, total_loss = 238.53, entropy_loss = -8.522, pg_loss = 116.37, baseline_loss = 130.68, learner_queue_size = 32, _tick = 106, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:26:34,965[0m][[34mroot[0m][[32mINFO[0m] - Step 273920 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1011.4, step = 273920, mean_episode_return = 0.23156, mean_episode_step = 70.653, total_loss = 238.53, entropy_loss = -8.522, pg_loss = 116.37, baseline_loss = 130.68, learner_queue_size = 32, _tick = 106, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:26:39,970[0m][[34mroot[0m][[32mINFO[0m] - Step 276480 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1016.4, step = 276480, mean_episode_return = 0.10763, mean_episode_step = 73.638, total_loss = 555.63, entropy_loss = -8.5794, pg_loss = 364.98, baseline_loss = 199.24, learner_queue_size = 32, _tick = 107, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:26:44,975[0m][[34mroot[0m][[32mINFO[0m] - Step 276480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1021.4, step = 276480, mean_episode_return = 0.10763, mean_episode_step = 73.638, total_loss = 555.63, entropy_loss = -8.5794, pg_loss = 364.98, baseline_loss = 199.24, learner_queue_size = 32, _tick = 107, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:26:49,981[0m][[34mroot[0m][[32mINFO[0m] - Step 279040 @ 511.4 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (train_seconds = 1026.4, step = 279040, mean_episode_return = 0.094478, mean_episode_step = 66.844, total_loss = 3.2742, entropy_loss = -8.5918, pg_loss = -135.83, baseline_loss = 147.7, learner_queue_size = 32, _tick = 108, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:26:54,986[0m][[34mroot[0m][[32mINFO[0m] - Step 279040 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1031.4, step = 279040, mean_episode_return = 0.094478, mean_episode_step = 66.844, total_loss = 3.2742, entropy_loss = -8.5918, pg_loss = -135.83, baseline_loss = 147.7, learner_queue_size = 32, _tick = 108, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:26:59,991[0m][[34mroot[0m][[32mINFO[0m] - Step 281600 @ 511.5 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (train_seconds = 1036.5, step = 281600, mean_episode_return = 0.17583, mean_episode_step = 73.818, total_loss = 373.21, entropy_loss = -8.6061, pg_loss = 197.35, baseline_loss = 184.47, learner_queue_size = 32, _tick = 109, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:27:04,997[0m][[34mroot[0m][[32mINFO[0m] - Step 281600 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1041.5, step = 281600, mean_episode_return = 0.17583, mean_episode_step = 73.818, total_loss = 373.21, entropy_loss = -8.6061, pg_loss = 197.35, baseline_loss = 184.47, learner_queue_size = 32, _tick = 109, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:27:10,002[0m][[34mroot[0m][[32mINFO[0m] - Step 284160 @ 511.4 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 1046.5, step = 284160, mean_episode_return = 0.17651, mean_episode_step = 77.796, total_loss = -722.71, entropy_loss = -8.6046, pg_loss = -783.99, baseline_loss = 69.887, learner_queue_size = 32, _tick = 110, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:27:15,008[0m][[34mroot[0m][[32mINFO[0m] - Step 284160 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1051.5, step = 284160, mean_episode_return = 0.17651, mean_episode_step = 77.796, total_loss = -722.71, entropy_loss = -8.6046, pg_loss = -783.99, baseline_loss = 69.887, learner_queue_size = 32, _tick = 110, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:27:20,014[0m][[34mroot[0m][[32mINFO[0m] - Step 286720 @ 511.4 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 1056.5, step = 286720, mean_episode_return = 0.091102, mean_episode_step = 54.154, total_loss = -413.02, entropy_loss = -8.6289, pg_loss = -497.86, baseline_loss = 93.474, learner_queue_size = 32, _tick = 111, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:27:25,019[0m][[34mroot[0m][[32mINFO[0m] - Step 286720 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1061.5, step = 286720, mean_episode_return = 0.091102, mean_episode_step = 54.154, total_loss = -413.02, entropy_loss = -8.6289, pg_loss = -497.86, baseline_loss = 93.474, learner_queue_size = 32, _tick = 111, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:27:30,025[0m][[34mroot[0m][[32mINFO[0m] - Step 289280 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1066.5, step = 289280, mean_episode_return = 0.096486, mean_episode_step = 63.877, total_loss = 487.19, entropy_loss = -8.6051, pg_loss = 312.58, baseline_loss = 183.21, learner_queue_size = 32, _tick = 112, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:27:35,031[0m][[34mroot[0m][[32mINFO[0m] - Step 289280 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1071.5, step = 289280, mean_episode_return = 0.096486, mean_episode_step = 63.877, total_loss = 487.19, entropy_loss = -8.6051, pg_loss = 312.58, baseline_loss = 183.21, learner_queue_size = 32, _tick = 112, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:27:40,036[0m][[34mroot[0m][[32mINFO[0m] - Step 291840 @ 511.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1076.5, step = 291840, mean_episode_return = 0.17803, mean_episode_step = 71.831, total_loss = -359.29, entropy_loss = -8.5921, pg_loss = -436.97, baseline_loss = 86.278, learner_queue_size = 32, _tick = 113, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:27:45,041[0m][[34mroot[0m][[32mINFO[0m] - Step 291840 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1081.5, step = 291840, mean_episode_return = 0.17803, mean_episode_step = 71.831, total_loss = -359.29, entropy_loss = -8.5921, pg_loss = -436.97, baseline_loss = 86.278, learner_queue_size = 32, _tick = 113, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:27:50,046[0m][[34mroot[0m][[32mINFO[0m] - Step 294400 @ 511.5 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 1086.5, step = 294400, mean_episode_return = 0.15783, mean_episode_step = 60.561, total_loss = -55.117, entropy_loss = -8.6042, pg_loss = -199.63, baseline_loss = 153.11, learner_queue_size = 32, _tick = 114, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:27:55,051[0m][[34mroot[0m][[32mINFO[0m] - Step 294400 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1091.5, step = 294400, mean_episode_return = 0.15783, mean_episode_step = 60.561, total_loss = -55.117, entropy_loss = -8.6042, pg_loss = -199.63, baseline_loss = 153.11, learner_queue_size = 32, _tick = 114, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:28:00,056[0m][[34mroot[0m][[32mINFO[0m] - Step 296960 @ 511.5 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (train_seconds = 1096.5, step = 296960, mean_episode_return = 0.12121, mean_episode_step = 64.149, total_loss = 213.58, entropy_loss = -8.5922, pg_loss = 63.918, baseline_loss = 158.26, learner_queue_size = 32, _tick = 115, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:28:05,061[0m][[34mroot[0m][[32mINFO[0m] - Step 296960 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1101.5, step = 296960, mean_episode_return = 0.12121, mean_episode_step = 64.149, total_loss = 213.58, entropy_loss = -8.5922, pg_loss = 63.918, baseline_loss = 158.26, learner_queue_size = 32, _tick = 115, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:28:10,066[0m][[34mroot[0m][[32mINFO[0m] - Step 299520 @ 511.5 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 1106.5, step = 299520, mean_episode_return = 0.16388, mean_episode_step = 69.841, total_loss = 286.13, entropy_loss = -8.5712, pg_loss = 141.74, baseline_loss = 152.95, learner_queue_size = 32, _tick = 116, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:28:15,072[0m][[34mroot[0m][[32mINFO[0m] - Step 302080 @ 511.4 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (train_seconds = 1111.5, step = 302080, mean_episode_return = 0.0645, mean_episode_step = 81.53, total_loss = 484.14, entropy_loss = -8.5221, pg_loss = 331.42, baseline_loss = 161.24, learner_queue_size = 32, _tick = 117, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:28:20,077[0m][[34mroot[0m][[32mINFO[0m] - Step 302080 @ 0.0 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 1116.5, step = 302080, mean_episode_return = 0.0645, mean_episode_step = 81.53, total_loss = 484.14, entropy_loss = -8.5221, pg_loss = 331.42, baseline_loss = 161.24, learner_queue_size = 32, _tick = 117, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:28:25,083[0m][[34mroot[0m][[32mINFO[0m] - Step 304640 @ 511.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1121.5, step = 304640, mean_episode_return = 0.10619, mean_episode_step = 69.152, total_loss = -280.86, entropy_loss = -8.5508, pg_loss = -407.61, baseline_loss = 135.31, learner_queue_size = 32, _tick = 118, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:28:30,089[0m][[34mroot[0m][[32mINFO[0m] - Step 304640 @ 0.0 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 1126.5, step = 304640, mean_episode_return = 0.10619, mean_episode_step = 69.152, total_loss = -280.86, entropy_loss = -8.5508, pg_loss = -407.61, baseline_loss = 135.31, learner_queue_size = 32, _tick = 118, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:28:35,094[0m][[34mroot[0m][[32mINFO[0m] - Step 304640 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1131.6, step = 304640, mean_episode_return = 0.10619, mean_episode_step = 69.152, total_loss = -280.86, entropy_loss = -8.5508, pg_loss = -407.61, baseline_loss = 135.31, learner_queue_size = 32, _tick = 118, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:28:40,100[0m][[34mroot[0m][[32mINFO[0m] - Step 307200 @ 511.4 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 1136.6, step = 307200, mean_episode_return = 0.19504, mean_episode_step = 66.136, total_loss = 431.47, entropy_loss = -8.5542, pg_loss = 238.46, baseline_loss = 201.57, learner_queue_size = 32, _tick = 119, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:28:45,105[0m][[34mroot[0m][[32mINFO[0m] - Step 307200 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1141.6, step = 307200, mean_episode_return = 0.19504, mean_episode_step = 66.136, total_loss = 431.47, entropy_loss = -8.5542, pg_loss = 238.46, baseline_loss = 201.57, learner_queue_size = 32, _tick = 119, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:28:50,111[0m][[34mroot[0m][[32mINFO[0m] - Step 309760 @ 511.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1146.6, step = 309760, mean_episode_return = 0.247, mean_episode_step = 61.412, total_loss = 232.8, entropy_loss = -8.5923, pg_loss = 74.807, baseline_loss = 166.58, learner_queue_size = 32, _tick = 120, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:28:55,116[0m][[34mroot[0m][[32mINFO[0m] - Step 309760 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1151.6, step = 309760, mean_episode_return = 0.247, mean_episode_step = 61.412, total_loss = 232.8, entropy_loss = -8.5923, pg_loss = 74.807, baseline_loss = 166.58, learner_queue_size = 32, _tick = 120, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:29:00,122[0m][[34mroot[0m][[32mINFO[0m] - Step 312320 @ 511.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 1156.6, step = 312320, mean_episode_return = 0.076283, mean_episode_step = 58.438, total_loss = 677.51, entropy_loss = -8.6374, pg_loss = 491.77, baseline_loss = 194.38, learner_queue_size = 32, _tick = 121, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:29:05,128[0m][[34mroot[0m][[32mINFO[0m] - Step 312320 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1161.6, step = 312320, mean_episode_return = 0.076283, mean_episode_step = 58.438, total_loss = 677.51, entropy_loss = -8.6374, pg_loss = 491.77, baseline_loss = 194.38, learner_queue_size = 32, _tick = 121, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:29:10,133[0m][[34mroot[0m][[32mINFO[0m] - Step 314880 @ 511.4 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1166.6, step = 314880, mean_episode_return = 0.052391, mean_episode_step = 69.096, total_loss = -335.15, entropy_loss = -8.6417, pg_loss = -435.01, baseline_loss = 108.51, learner_queue_size = 32, _tick = 122, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:29:15,138[0m][[34mroot[0m][[32mINFO[0m] - Step 314880 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1171.6, step = 314880, mean_episode_return = 0.052391, mean_episode_step = 69.096, total_loss = -335.15, entropy_loss = -8.6417, pg_loss = -435.01, baseline_loss = 108.51, learner_queue_size = 32, _tick = 122, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:29:20,143[0m][[34mroot[0m][[32mINFO[0m] - Step 317440 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 1176.6, step = 317440, mean_episode_return = 0.16362, mean_episode_step = 73.878, total_loss = -51.227, entropy_loss = -8.6412, pg_loss = -152.72, baseline_loss = 110.14, learner_queue_size = 32, _tick = 123, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:29:25,149[0m][[34mroot[0m][[32mINFO[0m] - Step 317440 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1181.6, step = 317440, mean_episode_return = 0.16362, mean_episode_step = 73.878, total_loss = -51.227, entropy_loss = -8.6412, pg_loss = -152.72, baseline_loss = 110.14, learner_queue_size = 32, _tick = 123, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:29:30,154[0m][[34mroot[0m][[32mINFO[0m] - Step 320000 @ 511.4 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1186.6, step = 320000, mean_episode_return = 0.02178, mean_episode_step = 60.662, total_loss = -317.64, entropy_loss = -8.6691, pg_loss = -402.39, baseline_loss = 93.423, learner_queue_size = 32, _tick = 124, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:29:35,159[0m][[34mroot[0m][[32mINFO[0m] - Step 320000 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1191.6, step = 320000, mean_episode_return = 0.02178, mean_episode_step = 60.662, total_loss = -317.64, entropy_loss = -8.6691, pg_loss = -402.39, baseline_loss = 93.423, learner_queue_size = 32, _tick = 124, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:29:40,164[0m][[34mroot[0m][[32mINFO[0m] - Step 322560 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1196.6, step = 322560, mean_episode_return = 0.17038, mean_episode_step = 69.604, total_loss = 622.78, entropy_loss = -8.7257, pg_loss = 511.68, baseline_loss = 119.82, learner_queue_size = 32, _tick = 125, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:29:45,170[0m][[34mroot[0m][[32mINFO[0m] - Saving checkpoint to /opt/minihack/checkpoint.tar[0m
[[36m2025-01-18 17:29:45,516[0m][[34mroot[0m][[32mINFO[0m] - Step 322560 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1201.6, step = 322560, mean_episode_return = 0.17038, mean_episode_step = 69.604, total_loss = 622.78, entropy_loss = -8.7257, pg_loss = 511.68, baseline_loss = 119.82, learner_queue_size = 32, _tick = 125, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:29:50,523[0m][[34mroot[0m][[32mINFO[0m] - Step 325120 @ 478.2 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1207.0, step = 325120, mean_episode_return = 0.13198, mean_episode_step = 60.157, total_loss = 115.47, entropy_loss = -8.6864, pg_loss = -21.429, baseline_loss = 145.59, learner_queue_size = 32, _tick = 126, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:29:55,528[0m][[34mroot[0m][[32mINFO[0m] - Step 325120 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1212.0, step = 325120, mean_episode_return = 0.13198, mean_episode_step = 60.157, total_loss = 115.47, entropy_loss = -8.6864, pg_loss = -21.429, baseline_loss = 145.59, learner_queue_size = 32, _tick = 126, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:30:00,534[0m][[34mroot[0m][[32mINFO[0m] - Step 327680 @ 511.4 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1217.0, step = 327680, mean_episode_return = 0.11662, mean_episode_step = 60.911, total_loss = 1110.1, entropy_loss = -8.754, pg_loss = 928.91, baseline_loss = 189.95, learner_queue_size = 32, _tick = 127, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:30:05,539[0m][[34mroot[0m][[32mINFO[0m] - Step 330240 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1222.0, step = 330240, mean_episode_return = 0.037105, mean_episode_step = 64.316, total_loss = -149.33, entropy_loss = -8.7531, pg_loss = -239.52, baseline_loss = 98.947, learner_queue_size = 32, _tick = 128, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:30:10,544[0m][[34mroot[0m][[32mINFO[0m] - Step 330240 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1227.0, step = 330240, mean_episode_return = 0.037105, mean_episode_step = 64.316, total_loss = -149.33, entropy_loss = -8.7531, pg_loss = -239.52, baseline_loss = 98.947, learner_queue_size = 32, _tick = 128, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:30:15,549[0m][[34mroot[0m][[32mINFO[0m] - Step 332800 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1232.0, step = 332800, mean_episode_return = 0.14991, mean_episode_step = 91.093, total_loss = -476.09, entropy_loss = -8.735, pg_loss = -510.82, baseline_loss = 43.46, learner_queue_size = 32, _tick = 129, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:30:20,554[0m][[34mroot[0m][[32mINFO[0m] - Step 332800 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1237.0, step = 332800, mean_episode_return = 0.14991, mean_episode_step = 91.093, total_loss = -476.09, entropy_loss = -8.735, pg_loss = -510.82, baseline_loss = 43.46, learner_queue_size = 32, _tick = 129, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:30:25,559[0m][[34mroot[0m][[32mINFO[0m] - Step 335360 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1242.0, step = 335360, mean_episode_return = 0.169, mean_episode_step = 63.843, total_loss = 680.25, entropy_loss = -8.7602, pg_loss = 545.33, baseline_loss = 143.69, learner_queue_size = 32, _tick = 130, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:30:30,564[0m][[34mroot[0m][[32mINFO[0m] - Step 335360 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1247.0, step = 335360, mean_episode_return = 0.169, mean_episode_step = 63.843, total_loss = 680.25, entropy_loss = -8.7602, pg_loss = 545.33, baseline_loss = 143.69, learner_queue_size = 32, _tick = 130, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:30:35,569[0m][[34mroot[0m][[32mINFO[0m] - Step 337920 @ 511.5 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 1252.0, step = 337920, mean_episode_return = 0.21468, mean_episode_step = 80.824, total_loss = -151.95, entropy_loss = -8.7514, pg_loss = -247.01, baseline_loss = 103.81, learner_queue_size = 32, _tick = 131, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:30:40,574[0m][[34mroot[0m][[32mINFO[0m] - Step 340480 @ 511.5 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (train_seconds = 1257.0, step = 340480, mean_episode_return = 0.21437, mean_episode_step = 62.623, total_loss = 468.85, entropy_loss = -8.7444, pg_loss = 309.96, baseline_loss = 167.64, learner_queue_size = 32, _tick = 132, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:30:45,579[0m][[34mroot[0m][[32mINFO[0m] - Step 340480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1262.0, step = 340480, mean_episode_return = 0.21437, mean_episode_step = 62.623, total_loss = 468.85, entropy_loss = -8.7444, pg_loss = 309.96, baseline_loss = 167.64, learner_queue_size = 32, _tick = 132, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:30:50,585[0m][[34mroot[0m][[32mINFO[0m] - Step 343040 @ 511.4 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 1267.0, step = 343040, mean_episode_return = 0.16459, mean_episode_step = 51.923, total_loss = 273.99, entropy_loss = -8.765, pg_loss = 107.85, baseline_loss = 174.91, learner_queue_size = 32, _tick = 133, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:30:55,590[0m][[34mroot[0m][[32mINFO[0m] - Step 343040 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1272.1, step = 343040, mean_episode_return = 0.16459, mean_episode_step = 51.923, total_loss = 273.99, entropy_loss = -8.765, pg_loss = 107.85, baseline_loss = 174.91, learner_queue_size = 32, _tick = 133, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:31:00,595[0m][[34mroot[0m][[32mINFO[0m] - Step 345600 @ 511.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 1277.1, step = 345600, mean_episode_return = 0.12714, mean_episode_step = 60.959, total_loss = 532.55, entropy_loss = -8.7206, pg_loss = 350.57, baseline_loss = 190.69, learner_queue_size = 32, _tick = 134, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:31:05,601[0m][[34mroot[0m][[32mINFO[0m] - Step 345600 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1282.1, step = 345600, mean_episode_return = 0.12714, mean_episode_step = 60.959, total_loss = 532.55, entropy_loss = -8.7206, pg_loss = 350.57, baseline_loss = 190.69, learner_queue_size = 32, _tick = 134, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:31:10,606[0m][[34mroot[0m][[32mINFO[0m] - Step 348160 @ 511.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1287.1, step = 348160, mean_episode_return = 0.10216, mean_episode_step = 79.482, total_loss = -28.219, entropy_loss = -8.714, pg_loss = -132.41, baseline_loss = 112.9, learner_queue_size = 32, _tick = 135, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:31:15,613[0m][[34mroot[0m][[32mINFO[0m] - Step 350720 @ 511.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1292.1, step = 350720, mean_episode_return = 0.1734, mean_episode_step = 68.839, total_loss = 128.9, entropy_loss = -8.7386, pg_loss = -24.321, baseline_loss = 161.96, learner_queue_size = 32, _tick = 136, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:31:20,618[0m][[34mroot[0m][[32mINFO[0m] - Step 350720 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1297.1, step = 350720, mean_episode_return = 0.1734, mean_episode_step = 68.839, total_loss = 128.9, entropy_loss = -8.7386, pg_loss = -24.321, baseline_loss = 161.96, learner_queue_size = 32, _tick = 136, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:31:25,624[0m][[34mroot[0m][[32mINFO[0m] - Step 353280 @ 511.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1302.1, step = 353280, mean_episode_return = 0.10298, mean_episode_step = 57.8, total_loss = 474.59, entropy_loss = -8.7198, pg_loss = 291.77, baseline_loss = 191.54, learner_queue_size = 32, _tick = 137, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:31:30,629[0m][[34mroot[0m][[32mINFO[0m] - Step 353280 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1307.1, step = 353280, mean_episode_return = 0.10298, mean_episode_step = 57.8, total_loss = 474.59, entropy_loss = -8.7198, pg_loss = 291.77, baseline_loss = 191.54, learner_queue_size = 32, _tick = 137, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:31:35,635[0m][[34mroot[0m][[32mINFO[0m] - Step 355840 @ 511.5 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 1312.1, step = 355840, mean_episode_return = 0.19889, mean_episode_step = 64.351, total_loss = 734.43, entropy_loss = -8.7408, pg_loss = 503.43, baseline_loss = 239.74, learner_queue_size = 32, _tick = 138, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:31:40,639[0m][[34mroot[0m][[32mINFO[0m] - Step 355840 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1317.1, step = 355840, mean_episode_return = 0.19889, mean_episode_step = 64.351, total_loss = 734.43, entropy_loss = -8.7408, pg_loss = 503.43, baseline_loss = 239.74, learner_queue_size = 32, _tick = 138, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:31:45,644[0m][[34mroot[0m][[32mINFO[0m] - Step 358400 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 1322.1, step = 358400, mean_episode_return = 0.098216, mean_episode_step = 69.128, total_loss = -516.33, entropy_loss = -8.73, pg_loss = -603.26, baseline_loss = 95.661, learner_queue_size = 32, _tick = 139, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:31:50,649[0m][[34mroot[0m][[32mINFO[0m] - Step 360960 @ 511.5 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 1327.1, step = 360960, mean_episode_return = 0.21073, mean_episode_step = 61.954, total_loss = 708.26, entropy_loss = -8.7405, pg_loss = 479.74, baseline_loss = 237.26, learner_queue_size = 32, _tick = 140, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:31:55,655[0m][[34mroot[0m][[32mINFO[0m] - Step 360960 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1332.1, step = 360960, mean_episode_return = 0.21073, mean_episode_step = 61.954, total_loss = 708.26, entropy_loss = -8.7405, pg_loss = 479.74, baseline_loss = 237.26, learner_queue_size = 32, _tick = 140, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:32:00,661[0m][[34mroot[0m][[32mINFO[0m] - Step 363520 @ 511.4 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 1337.1, step = 363520, mean_episode_return = 0.16492, mean_episode_step = 71.428, total_loss = 340.11, entropy_loss = -8.7235, pg_loss = 172.0, baseline_loss = 176.84, learner_queue_size = 32, _tick = 141, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:32:05,665[0m][[34mroot[0m][[32mINFO[0m] - Step 363520 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1342.1, step = 363520, mean_episode_return = 0.16492, mean_episode_step = 71.428, total_loss = 340.11, entropy_loss = -8.7235, pg_loss = 172.0, baseline_loss = 176.84, learner_queue_size = 32, _tick = 141, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:32:10,670[0m][[34mroot[0m][[32mINFO[0m] - Step 366080 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1347.1, step = 366080, mean_episode_return = 0.11891, mean_episode_step = 72.387, total_loss = 338.24, entropy_loss = -8.7157, pg_loss = 156.66, baseline_loss = 190.29, learner_queue_size = 32, _tick = 142, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:32:15,675[0m][[34mroot[0m][[32mINFO[0m] - Step 366080 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1352.1, step = 366080, mean_episode_return = 0.11891, mean_episode_step = 72.387, total_loss = 338.24, entropy_loss = -8.7157, pg_loss = 156.66, baseline_loss = 190.29, learner_queue_size = 32, _tick = 142, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:32:20,681[0m][[34mroot[0m][[32mINFO[0m] - Step 368640 @ 511.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 1357.1, step = 368640, mean_episode_return = 0.19016, mean_episode_step = 64.08, total_loss = 6.5265, entropy_loss = -8.7214, pg_loss = -158.73, baseline_loss = 173.98, learner_queue_size = 32, _tick = 143, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:32:25,686[0m][[34mroot[0m][[32mINFO[0m] - Step 371200 @ 511.4 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1362.1, step = 371200, mean_episode_return = 0.11254, mean_episode_step = 66.343, total_loss = 1130.6, entropy_loss = -8.6852, pg_loss = 892.96, baseline_loss = 246.31, learner_queue_size = 32, _tick = 144, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:32:30,692[0m][[34mroot[0m][[32mINFO[0m] - Step 371200 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1367.2, step = 371200, mean_episode_return = 0.11254, mean_episode_step = 66.343, total_loss = 1130.6, entropy_loss = -8.6852, pg_loss = 892.96, baseline_loss = 246.31, learner_queue_size = 32, _tick = 144, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:32:35,698[0m][[34mroot[0m][[32mINFO[0m] - Step 373760 @ 511.3 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (train_seconds = 1372.2, step = 373760, mean_episode_return = 0.12627, mean_episode_step = 62.912, total_loss = -242.4, entropy_loss = -8.7029, pg_loss = -369.58, baseline_loss = 135.88, learner_queue_size = 32, _tick = 145, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:32:40,703[0m][[34mroot[0m][[32mINFO[0m] - Step 373760 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1377.2, step = 373760, mean_episode_return = 0.12627, mean_episode_step = 62.912, total_loss = -242.4, entropy_loss = -8.7029, pg_loss = -369.58, baseline_loss = 135.88, learner_queue_size = 32, _tick = 145, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:32:45,708[0m][[34mroot[0m][[32mINFO[0m] - Saving checkpoint to /opt/minihack/checkpoint_0.75.tar[0m
[[36m2025-01-18 17:32:45,761[0m][[34mroot[0m][[32mINFO[0m] - Step 376320 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 1382.2, step = 376320, mean_episode_return = 0.12874, mean_episode_step = 70.713, total_loss = 997.67, entropy_loss = -8.7106, pg_loss = 735.74, baseline_loss = 270.63, learner_queue_size = 32, _tick = 146, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:32:50,766[0m][[34mroot[0m][[32mINFO[0m] - Step 376320 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1387.2, step = 376320, mean_episode_return = 0.12874, mean_episode_step = 70.713, total_loss = 997.67, entropy_loss = -8.7106, pg_loss = 735.74, baseline_loss = 270.63, learner_queue_size = 32, _tick = 146, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:32:55,771[0m][[34mroot[0m][[32mINFO[0m] - Step 378880 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1392.2, step = 378880, mean_episode_return = 0.23809, mean_episode_step = 67.52, total_loss = 58.05, entropy_loss = -8.7028, pg_loss = -83.336, baseline_loss = 150.09, learner_queue_size = 32, _tick = 147, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:33:00,776[0m][[34mroot[0m][[32mINFO[0m] - Step 381440 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1397.2, step = 381440, mean_episode_return = 0.15796, mean_episode_step = 61.884, total_loss = -35.255, entropy_loss = -8.6964, pg_loss = -206.01, baseline_loss = 179.45, learner_queue_size = 32, _tick = 148, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:33:05,781[0m][[34mroot[0m][[32mINFO[0m] - Step 381440 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1402.2, step = 381440, mean_episode_return = 0.15796, mean_episode_step = 61.884, total_loss = -35.255, entropy_loss = -8.6964, pg_loss = -206.01, baseline_loss = 179.45, learner_queue_size = 32, _tick = 148, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:33:10,787[0m][[34mroot[0m][[32mINFO[0m] - Step 384000 @ 511.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 1407.2, step = 384000, mean_episode_return = 0.1989, mean_episode_step = 85.122, total_loss = 531.88, entropy_loss = -8.665, pg_loss = 348.2, baseline_loss = 192.35, learner_queue_size = 32, _tick = 149, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:33:15,792[0m][[34mroot[0m][[32mINFO[0m] - Step 384000 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1412.3, step = 384000, mean_episode_return = 0.1989, mean_episode_step = 85.122, total_loss = 531.88, entropy_loss = -8.665, pg_loss = 348.2, baseline_loss = 192.35, learner_queue_size = 32, _tick = 149, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:33:20,797[0m][[34mroot[0m][[32mINFO[0m] - Step 386560 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1417.3, step = 386560, mean_episode_return = 0.10014, mean_episode_step = 61.233, total_loss = -493.84, entropy_loss = -8.6808, pg_loss = -591.89, baseline_loss = 106.74, learner_queue_size = 32, _tick = 150, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:33:25,803[0m][[34mroot[0m][[32mINFO[0m] - Step 386560 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1422.3, step = 386560, mean_episode_return = 0.10014, mean_episode_step = 61.233, total_loss = -493.84, entropy_loss = -8.6808, pg_loss = -591.89, baseline_loss = 106.74, learner_queue_size = 32, _tick = 150, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:33:30,809[0m][[34mroot[0m][[32mINFO[0m] - Step 389120 @ 511.4 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 1427.3, step = 389120, mean_episode_return = 0.13208, mean_episode_step = 76.075, total_loss = -338.67, entropy_loss = -8.6852, pg_loss = -451.32, baseline_loss = 121.34, learner_queue_size = 32, _tick = 151, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:33:35,815[0m][[34mroot[0m][[32mINFO[0m] - Step 389120 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1432.3, step = 389120, mean_episode_return = 0.13208, mean_episode_step = 76.075, total_loss = -338.67, entropy_loss = -8.6852, pg_loss = -451.32, baseline_loss = 121.34, learner_queue_size = 32, _tick = 151, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:33:40,820[0m][[34mroot[0m][[32mINFO[0m] - Step 391680 @ 511.4 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (train_seconds = 1437.3, step = 391680, mean_episode_return = 0.23322, mean_episode_step = 75.846, total_loss = -972.29, entropy_loss = -8.6717, pg_loss = -1008.2, baseline_loss = 44.602, learner_queue_size = 32, _tick = 152, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:33:45,827[0m][[34mroot[0m][[32mINFO[0m] - Step 394240 @ 511.3 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (train_seconds = 1442.3, step = 394240, mean_episode_return = 0.13057, mean_episode_step = 71.795, total_loss = -862.15, entropy_loss = -8.6744, pg_loss = -925.22, baseline_loss = 71.751, learner_queue_size = 32, _tick = 153, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:33:50,832[0m][[34mroot[0m][[32mINFO[0m] - Step 394240 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1447.3, step = 394240, mean_episode_return = 0.13057, mean_episode_step = 71.795, total_loss = -862.15, entropy_loss = -8.6744, pg_loss = -925.22, baseline_loss = 71.751, learner_queue_size = 32, _tick = 153, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:33:55,837[0m][[34mroot[0m][[32mINFO[0m] - Step 396800 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 1452.3, step = 396800, mean_episode_return = 0.14991, mean_episode_step = 72.199, total_loss = -37.102, entropy_loss = -8.665, pg_loss = -169.47, baseline_loss = 141.03, learner_queue_size = 32, _tick = 154, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:34:00,842[0m][[34mroot[0m][[32mINFO[0m] - Step 396800 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1457.3, step = 396800, mean_episode_return = 0.14991, mean_episode_step = 72.199, total_loss = -37.102, entropy_loss = -8.665, pg_loss = -169.47, baseline_loss = 141.03, learner_queue_size = 32, _tick = 154, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:34:05,847[0m][[34mroot[0m][[32mINFO[0m] - Step 399360 @ 511.5 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (train_seconds = 1462.3, step = 399360, mean_episode_return = 0.23735, mean_episode_step = 78.725, total_loss = 421.07, entropy_loss = -8.6575, pg_loss = 258.71, baseline_loss = 171.02, learner_queue_size = 32, _tick = 155, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:34:10,852[0m][[34mroot[0m][[32mINFO[0m] - Step 399360 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1467.3, step = 399360, mean_episode_return = 0.23735, mean_episode_step = 78.725, total_loss = 421.07, entropy_loss = -8.6575, pg_loss = 258.71, baseline_loss = 171.02, learner_queue_size = 32, _tick = 155, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:34:15,857[0m][[34mroot[0m][[32mINFO[0m] - Step 401920 @ 511.5 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 1472.3, step = 401920, mean_episode_return = 0.2581, mean_episode_step = 80.976, total_loss = 239.9, entropy_loss = -8.6689, pg_loss = 53.976, baseline_loss = 194.6, learner_queue_size = 32, _tick = 156, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:34:20,863[0m][[34mroot[0m][[32mINFO[0m] - Step 404480 @ 511.4 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 1477.3, step = 404480, mean_episode_return = 0.1408, mean_episode_step = 63.984, total_loss = 801.84, entropy_loss = -8.6711, pg_loss = 572.36, baseline_loss = 238.15, learner_queue_size = 32, _tick = 157, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:34:25,868[0m][[34mroot[0m][[32mINFO[0m] - Step 404480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1482.3, step = 404480, mean_episode_return = 0.1408, mean_episode_step = 63.984, total_loss = 801.84, entropy_loss = -8.6711, pg_loss = 572.36, baseline_loss = 238.15, learner_queue_size = 32, _tick = 157, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:34:30,874[0m][[34mroot[0m][[32mINFO[0m] - Step 407040 @ 511.4 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (train_seconds = 1487.3, step = 407040, mean_episode_return = 0.10095, mean_episode_step = 76.043, total_loss = 241.96, entropy_loss = -8.6612, pg_loss = 91.837, baseline_loss = 158.78, learner_queue_size = 32, _tick = 158, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:34:35,879[0m][[34mroot[0m][[32mINFO[0m] - Step 407040 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1492.3, step = 407040, mean_episode_return = 0.10095, mean_episode_step = 76.043, total_loss = 241.96, entropy_loss = -8.6612, pg_loss = 91.837, baseline_loss = 158.78, learner_queue_size = 32, _tick = 158, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:34:40,884[0m][[34mroot[0m][[32mINFO[0m] - Step 409600 @ 511.5 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 1497.3, step = 409600, mean_episode_return = 0.14326, mean_episode_step = 65.264, total_loss = 282.23, entropy_loss = -8.6833, pg_loss = 95.715, baseline_loss = 195.2, learner_queue_size = 32, _tick = 159, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:34:45,889[0m][[34mroot[0m][[32mINFO[0m] - Step 409600 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1502.4, step = 409600, mean_episode_return = 0.14326, mean_episode_step = 65.264, total_loss = 282.23, entropy_loss = -8.6833, pg_loss = 95.715, baseline_loss = 195.2, learner_queue_size = 32, _tick = 159, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:34:50,894[0m][[34mroot[0m][[32mINFO[0m] - Step 412160 @ 511.5 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 1507.4, step = 412160, mean_episode_return = 0.12007, mean_episode_step = 54.269, total_loss = -484.41, entropy_loss = -8.6999, pg_loss = -595.23, baseline_loss = 119.53, learner_queue_size = 32, _tick = 160, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:34:55,899[0m][[34mroot[0m][[32mINFO[0m] - Step 414720 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1512.4, step = 414720, mean_episode_return = 0.1502, mean_episode_step = 67.673, total_loss = 730.18, entropy_loss = -8.6854, pg_loss = 511.79, baseline_loss = 227.07, learner_queue_size = 32, _tick = 161, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:35:00,904[0m][[34mroot[0m][[32mINFO[0m] - Step 414720 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1517.4, step = 414720, mean_episode_return = 0.1502, mean_episode_step = 67.673, total_loss = 730.18, entropy_loss = -8.6854, pg_loss = 511.79, baseline_loss = 227.07, learner_queue_size = 32, _tick = 161, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:35:05,911[0m][[34mroot[0m][[32mINFO[0m] - Step 417280 @ 511.3 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 1522.4, step = 417280, mean_episode_return = 0.20763, mean_episode_step = 67.286, total_loss = -275.56, entropy_loss = -8.6841, pg_loss = -393.89, baseline_loss = 127.02, learner_queue_size = 32, _tick = 162, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:35:10,916[0m][[34mroot[0m][[32mINFO[0m] - Step 417280 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1527.4, step = 417280, mean_episode_return = 0.20763, mean_episode_step = 67.286, total_loss = -275.56, entropy_loss = -8.6841, pg_loss = -393.89, baseline_loss = 127.02, learner_queue_size = 32, _tick = 162, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:35:15,922[0m][[34mroot[0m][[32mINFO[0m] - Step 419840 @ 511.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1532.4, step = 419840, mean_episode_return = 0.17069, mean_episode_step = 71.295, total_loss = 1487.3, entropy_loss = -8.6824, pg_loss = 1157.4, baseline_loss = 338.56, learner_queue_size = 32, _tick = 163, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:35:20,927[0m][[34mroot[0m][[32mINFO[0m] - Step 419840 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1537.4, step = 419840, mean_episode_return = 0.17069, mean_episode_step = 71.295, total_loss = 1487.3, entropy_loss = -8.6824, pg_loss = 1157.4, baseline_loss = 338.56, learner_queue_size = 32, _tick = 163, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:35:25,932[0m][[34mroot[0m][[32mINFO[0m] - Step 422400 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1542.4, step = 422400, mean_episode_return = 0.24845, mean_episode_step = 59.721, total_loss = -187.15, entropy_loss = -8.6912, pg_loss = -350.62, baseline_loss = 172.17, learner_queue_size = 32, _tick = 164, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:35:30,940[0m][[34mroot[0m][[32mINFO[0m] - Step 424960 @ 511.2 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1547.4, step = 424960, mean_episode_return = 0.15343, mean_episode_step = 90.767, total_loss = 690.29, entropy_loss = -8.6677, pg_loss = 459.9, baseline_loss = 239.05, learner_queue_size = 32, _tick = 165, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:35:35,945[0m][[34mroot[0m][[32mINFO[0m] - Step 424960 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1552.4, step = 424960, mean_episode_return = 0.15343, mean_episode_step = 90.767, total_loss = 690.29, entropy_loss = -8.6677, pg_loss = 459.9, baseline_loss = 239.05, learner_queue_size = 32, _tick = 165, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:35:40,951[0m][[34mroot[0m][[32mINFO[0m] - Step 427520 @ 511.4 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (train_seconds = 1557.4, step = 427520, mean_episode_return = 0.16868, mean_episode_step = 73.197, total_loss = 1499.6, entropy_loss = -8.6872, pg_loss = 1207.4, baseline_loss = 300.82, learner_queue_size = 32, _tick = 166, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:35:45,957[0m][[34mroot[0m][[32mINFO[0m] - Step 427520 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1562.4, step = 427520, mean_episode_return = 0.16868, mean_episode_step = 73.197, total_loss = 1499.6, entropy_loss = -8.6872, pg_loss = 1207.4, baseline_loss = 300.82, learner_queue_size = 32, _tick = 166, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:35:50,962[0m][[34mroot[0m][[32mINFO[0m] - Step 430080 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1567.4, step = 430080, mean_episode_return = 0.088525, mean_episode_step = 60.991, total_loss = 60.466, entropy_loss = -8.7016, pg_loss = -103.24, baseline_loss = 172.41, learner_queue_size = 32, _tick = 167, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:35:55,967[0m][[34mroot[0m][[32mINFO[0m] - Step 430080 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1572.4, step = 430080, mean_episode_return = 0.088525, mean_episode_step = 60.991, total_loss = 60.466, entropy_loss = -8.7016, pg_loss = -103.24, baseline_loss = 172.41, learner_queue_size = 32, _tick = 167, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:36:00,972[0m][[34mroot[0m][[32mINFO[0m] - Step 432640 @ 511.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 1577.4, step = 432640, mean_episode_return = 0.12697, mean_episode_step = 72.308, total_loss = 493.68, entropy_loss = -8.6944, pg_loss = 293.53, baseline_loss = 208.84, learner_queue_size = 32, _tick = 168, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:36:05,977[0m][[34mroot[0m][[32mINFO[0m] - Step 435200 @ 511.5 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (train_seconds = 1582.4, step = 435200, mean_episode_return = 0.18652, mean_episode_step = 70.15, total_loss = -823.71, entropy_loss = -8.6996, pg_loss = -924.22, baseline_loss = 109.21, learner_queue_size = 32, _tick = 169, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:36:10,982[0m][[34mroot[0m][[32mINFO[0m] - Step 435200 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1587.4, step = 435200, mean_episode_return = 0.18652, mean_episode_step = 70.15, total_loss = -823.71, entropy_loss = -8.6996, pg_loss = -924.22, baseline_loss = 109.21, learner_queue_size = 32, _tick = 169, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:36:15,987[0m][[34mroot[0m][[32mINFO[0m] - Step 437760 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1592.4, step = 437760, mean_episode_return = 0.10973, mean_episode_step = 76.988, total_loss = 502.77, entropy_loss = -8.6891, pg_loss = 305.48, baseline_loss = 205.98, learner_queue_size = 32, _tick = 170, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:36:20,992[0m][[34mroot[0m][[32mINFO[0m] - Step 437760 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1597.5, step = 437760, mean_episode_return = 0.10973, mean_episode_step = 76.988, total_loss = 502.77, entropy_loss = -8.6891, pg_loss = 305.48, baseline_loss = 205.98, learner_queue_size = 32, _tick = 170, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:36:25,997[0m][[34mroot[0m][[32mINFO[0m] - Step 440320 @ 511.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 1602.5, step = 440320, mean_episode_return = 0.12957, mean_episode_step = 66.358, total_loss = 362.86, entropy_loss = -8.6927, pg_loss = 165.1, baseline_loss = 206.45, learner_queue_size = 32, _tick = 171, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:36:31,002[0m][[34mroot[0m][[32mINFO[0m] - Step 440320 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1607.5, step = 440320, mean_episode_return = 0.12957, mean_episode_step = 66.358, total_loss = 362.86, entropy_loss = -8.6927, pg_loss = 165.1, baseline_loss = 206.45, learner_queue_size = 32, _tick = 171, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:36:36,008[0m][[34mroot[0m][[32mINFO[0m] - Step 442880 @ 511.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1612.5, step = 442880, mean_episode_return = 0.10904, mean_episode_step = 58.88, total_loss = 297.1, entropy_loss = -8.7024, pg_loss = 69.863, baseline_loss = 235.94, learner_queue_size = 32, _tick = 172, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:36:41,011[0m][[34mroot[0m][[32mINFO[0m] - Step 442880 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1617.5, step = 442880, mean_episode_return = 0.10904, mean_episode_step = 58.88, total_loss = 297.1, entropy_loss = -8.7024, pg_loss = 69.863, baseline_loss = 235.94, learner_queue_size = 32, _tick = 172, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:36:46,016[0m][[34mroot[0m][[32mINFO[0m] - Step 445440 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 1622.5, step = 445440, mean_episode_return = 0.093743, mean_episode_step = 66.032, total_loss = 571.48, entropy_loss = -8.6847, pg_loss = 354.27, baseline_loss = 225.89, learner_queue_size = 32, _tick = 173, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:36:51,022[0m][[34mroot[0m][[32mINFO[0m] - Step 448000 @ 511.4 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1627.5, step = 448000, mean_episode_return = 0.18561, mean_episode_step = 65.364, total_loss = -256.97, entropy_loss = -8.7023, pg_loss = -416.99, baseline_loss = 168.73, learner_queue_size = 32, _tick = 174, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:36:56,027[0m][[34mroot[0m][[32mINFO[0m] - Step 448000 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1632.5, step = 448000, mean_episode_return = 0.18561, mean_episode_step = 65.364, total_loss = -256.97, entropy_loss = -8.7023, pg_loss = -416.99, baseline_loss = 168.73, learner_queue_size = 32, _tick = 174, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:37:01,032[0m][[34mroot[0m][[32mINFO[0m] - Step 450560 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 1637.5, step = 450560, mean_episode_return = 0.23931, mean_episode_step = 78.46, total_loss = -69.04, entropy_loss = -8.6909, pg_loss = -237.69, baseline_loss = 177.34, learner_queue_size = 32, _tick = 175, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:37:06,037[0m][[34mroot[0m][[32mINFO[0m] - Step 450560 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1642.5, step = 450560, mean_episode_return = 0.23931, mean_episode_step = 78.46, total_loss = -69.04, entropy_loss = -8.6909, pg_loss = -237.69, baseline_loss = 177.34, learner_queue_size = 32, _tick = 175, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:37:11,042[0m][[34mroot[0m][[32mINFO[0m] - Step 453120 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1647.5, step = 453120, mean_episode_return = 0.13616, mean_episode_step = 83.548, total_loss = -537.47, entropy_loss = -8.6883, pg_loss = -650.79, baseline_loss = 122.01, learner_queue_size = 32, _tick = 176, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:37:16,048[0m][[34mroot[0m][[32mINFO[0m] - Step 453120 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1652.5, step = 453120, mean_episode_return = 0.13616, mean_episode_step = 83.548, total_loss = -537.47, entropy_loss = -8.6883, pg_loss = -650.79, baseline_loss = 122.01, learner_queue_size = 32, _tick = 176, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:37:21,052[0m][[34mroot[0m][[32mINFO[0m] - Step 455680 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1657.5, step = 455680, mean_episode_return = 0.24908, mean_episode_step = 72.354, total_loss = -318.26, entropy_loss = -8.6946, pg_loss = -478.65, baseline_loss = 169.08, learner_queue_size = 32, _tick = 177, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:37:26,057[0m][[34mroot[0m][[32mINFO[0m] - Step 458240 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 1662.5, step = 458240, mean_episode_return = 0.19778, mean_episode_step = 67.52, total_loss = -360.1, entropy_loss = -8.6837, pg_loss = -469.01, baseline_loss = 117.59, learner_queue_size = 32, _tick = 178, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:37:31,063[0m][[34mroot[0m][[32mINFO[0m] - Step 458240 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1667.5, step = 458240, mean_episode_return = 0.19778, mean_episode_step = 67.52, total_loss = -360.1, entropy_loss = -8.6837, pg_loss = -469.01, baseline_loss = 117.59, learner_queue_size = 32, _tick = 178, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:37:36,068[0m][[34mroot[0m][[32mINFO[0m] - Step 460800 @ 511.5 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (train_seconds = 1672.5, step = 460800, mean_episode_return = 0.20313, mean_episode_step = 64.52, total_loss = -215.32, entropy_loss = -8.6813, pg_loss = -344.25, baseline_loss = 137.61, learner_queue_size = 32, _tick = 179, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:37:41,073[0m][[34mroot[0m][[32mINFO[0m] - Step 460800 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1677.5, step = 460800, mean_episode_return = 0.20313, mean_episode_step = 64.52, total_loss = -215.32, entropy_loss = -8.6813, pg_loss = -344.25, baseline_loss = 137.61, learner_queue_size = 32, _tick = 179, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:37:46,078[0m][[34mroot[0m][[32mINFO[0m] - Step 463360 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1682.5, step = 463360, mean_episode_return = 0.2092, mean_episode_step = 71.623, total_loss = 6.4463, entropy_loss = -8.6821, pg_loss = -168.61, baseline_loss = 183.74, learner_queue_size = 32, _tick = 180, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:37:51,083[0m][[34mroot[0m][[32mINFO[0m] - Step 463360 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1687.5, step = 463360, mean_episode_return = 0.2092, mean_episode_step = 71.623, total_loss = 6.4463, entropy_loss = -8.6821, pg_loss = -168.61, baseline_loss = 183.74, learner_queue_size = 32, _tick = 180, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:37:56,089[0m][[34mroot[0m][[32mINFO[0m] - Step 465920 @ 511.4 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 1692.5, step = 465920, mean_episode_return = 0.126, mean_episode_step = 61.024, total_loss = 1015.7, entropy_loss = -8.6837, pg_loss = 746.61, baseline_loss = 277.74, learner_queue_size = 32, _tick = 181, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:38:01,093[0m][[34mroot[0m][[32mINFO[0m] - Step 468480 @ 511.5 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 1697.6, step = 468480, mean_episode_return = 0.10466, mean_episode_step = 63.268, total_loss = 571.0, entropy_loss = -8.68, pg_loss = 348.01, baseline_loss = 231.67, learner_queue_size = 32, _tick = 182, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:38:06,098[0m][[34mroot[0m][[32mINFO[0m] - Step 468480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1702.6, step = 468480, mean_episode_return = 0.10466, mean_episode_step = 63.268, total_loss = 571.0, entropy_loss = -8.68, pg_loss = 348.01, baseline_loss = 231.67, learner_queue_size = 32, _tick = 182, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:38:11,106[0m][[34mroot[0m][[32mINFO[0m] - Step 471040 @ 511.2 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 1707.6, step = 471040, mean_episode_return = 0.098528, mean_episode_step = 48.157, total_loss = -549.7, entropy_loss = -8.696, pg_loss = -691.61, baseline_loss = 150.61, learner_queue_size = 32, _tick = 183, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:38:16,111[0m][[34mroot[0m][[32mINFO[0m] - Step 471040 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1712.6, step = 471040, mean_episode_return = 0.098528, mean_episode_step = 48.157, total_loss = -549.7, entropy_loss = -8.696, pg_loss = -691.61, baseline_loss = 150.61, learner_queue_size = 32, _tick = 183, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:38:21,116[0m][[34mroot[0m][[32mINFO[0m] - Step 473600 @ 511.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 1717.6, step = 473600, mean_episode_return = 0.23003, mean_episode_step = 75.541, total_loss = 591.1, entropy_loss = -8.6749, pg_loss = 382.93, baseline_loss = 216.84, learner_queue_size = 32, _tick = 184, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:38:26,121[0m][[34mroot[0m][[32mINFO[0m] - Step 473600 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1722.6, step = 473600, mean_episode_return = 0.23003, mean_episode_step = 75.541, total_loss = 591.1, entropy_loss = -8.6749, pg_loss = 382.93, baseline_loss = 216.84, learner_queue_size = 32, _tick = 184, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:38:31,126[0m][[34mroot[0m][[32mINFO[0m] - Step 476160 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 1727.6, step = 476160, mean_episode_return = 0.12206, mean_episode_step = 66.475, total_loss = 397.31, entropy_loss = -8.6744, pg_loss = 205.03, baseline_loss = 200.95, learner_queue_size = 32, _tick = 185, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:38:36,131[0m][[34mroot[0m][[32mINFO[0m] - Step 478720 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 1732.6, step = 478720, mean_episode_return = 0.21129, mean_episode_step = 66.527, total_loss = 651.33, entropy_loss = -8.6748, pg_loss = 425.89, baseline_loss = 234.11, learner_queue_size = 32, _tick = 186, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:38:41,137[0m][[34mroot[0m][[32mINFO[0m] - Step 478720 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1737.6, step = 478720, mean_episode_return = 0.21129, mean_episode_step = 66.527, total_loss = 651.33, entropy_loss = -8.6748, pg_loss = 425.89, baseline_loss = 234.11, learner_queue_size = 32, _tick = 186, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:38:46,142[0m][[34mroot[0m][[32mINFO[0m] - Step 481280 @ 511.5 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (train_seconds = 1742.6, step = 481280, mean_episode_return = 0.14128, mean_episode_step = 58.05, total_loss = 924.03, entropy_loss = -8.6924, pg_loss = 667.46, baseline_loss = 265.26, learner_queue_size = 32, _tick = 187, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:38:51,147[0m][[34mroot[0m][[32mINFO[0m] - Step 481280 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1747.6, step = 481280, mean_episode_return = 0.14128, mean_episode_step = 58.05, total_loss = 924.03, entropy_loss = -8.6924, pg_loss = 667.46, baseline_loss = 265.26, learner_queue_size = 32, _tick = 187, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:38:56,152[0m][[34mroot[0m][[32mINFO[0m] - Step 483840 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1752.6, step = 483840, mean_episode_return = 0.084727, mean_episode_step = 54.092, total_loss = 333.9, entropy_loss = -8.6939, pg_loss = 120.44, baseline_loss = 222.15, learner_queue_size = 32, _tick = 188, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:39:01,157[0m][[34mroot[0m][[32mINFO[0m] - Step 483840 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1757.6, step = 483840, mean_episode_return = 0.084727, mean_episode_step = 54.092, total_loss = 333.9, entropy_loss = -8.6939, pg_loss = 120.44, baseline_loss = 222.15, learner_queue_size = 32, _tick = 188, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:39:06,162[0m][[34mroot[0m][[32mINFO[0m] - Step 486400 @ 511.5 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (train_seconds = 1762.6, step = 486400, mean_episode_return = 0.1559, mean_episode_step = 69.894, total_loss = 552.44, entropy_loss = -8.6817, pg_loss = 309.86, baseline_loss = 251.26, learner_queue_size = 32, _tick = 189, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:39:11,167[0m][[34mroot[0m][[32mINFO[0m] - Step 488960 @ 511.5 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (train_seconds = 1767.6, step = 488960, mean_episode_return = 0.20011, mean_episode_step = 73.027, total_loss = 671.06, entropy_loss = -8.6821, pg_loss = 428.22, baseline_loss = 251.52, learner_queue_size = 32, _tick = 190, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:39:16,172[0m][[34mroot[0m][[32mINFO[0m] - Step 488960 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1772.6, step = 488960, mean_episode_return = 0.20011, mean_episode_step = 73.027, total_loss = 671.06, entropy_loss = -8.6821, pg_loss = 428.22, baseline_loss = 251.52, learner_queue_size = 32, _tick = 190, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:39:21,185[0m][[34mroot[0m][[32mINFO[0m] - Step 491520 @ 510.7 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 1777.6, step = 491520, mean_episode_return = 0.016318, mean_episode_step = 58.407, total_loss = -15.888, entropy_loss = -8.6901, pg_loss = -198.71, baseline_loss = 191.51, learner_queue_size = 32, _tick = 191, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:39:26,190[0m][[34mroot[0m][[32mINFO[0m] - Step 491520 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1782.7, step = 491520, mean_episode_return = 0.016318, mean_episode_step = 58.407, total_loss = -15.888, entropy_loss = -8.6901, pg_loss = -198.71, baseline_loss = 191.51, learner_queue_size = 32, _tick = 191, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:39:31,196[0m][[34mroot[0m][[32mINFO[0m] - Step 494080 @ 511.4 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (train_seconds = 1787.7, step = 494080, mean_episode_return = 0.16353, mean_episode_step = 81.468, total_loss = -206.6, entropy_loss = -8.6782, pg_loss = -352.36, baseline_loss = 154.43, learner_queue_size = 32, _tick = 192, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:39:36,202[0m][[34mroot[0m][[32mINFO[0m] - Step 494080 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1792.7, step = 494080, mean_episode_return = 0.16353, mean_episode_step = 81.468, total_loss = -206.6, entropy_loss = -8.6782, pg_loss = -352.36, baseline_loss = 154.43, learner_queue_size = 32, _tick = 192, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:39:41,207[0m][[34mroot[0m][[32mINFO[0m] - Step 496640 @ 511.5 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (train_seconds = 1797.7, step = 496640, mean_episode_return = 0.12235, mean_episode_step = 72.771, total_loss = 140.56, entropy_loss = -8.6816, pg_loss = -38.385, baseline_loss = 187.63, learner_queue_size = 32, _tick = 193, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:39:46,212[0m][[34mroot[0m][[32mINFO[0m] - Saving checkpoint to /opt/minihack/checkpoint.tar[0m
[[36m2025-01-18 17:39:46,608[0m][[34mroot[0m][[32mINFO[0m] - Step 499200 @ 511.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 1802.7, step = 499200, mean_episode_return = 0.168, mean_episode_step = 79.58, total_loss = -543.02, entropy_loss = -8.6783, pg_loss = -659.51, baseline_loss = 125.17, learner_queue_size = 32, _tick = 194, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:39:51,613[0m][[34mroot[0m][[32mINFO[0m] - Step 499200 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1808.1, step = 499200, mean_episode_return = 0.168, mean_episode_step = 79.58, total_loss = -543.02, entropy_loss = -8.6783, pg_loss = -659.51, baseline_loss = 125.17, learner_queue_size = 32, _tick = 194, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:39:56,618[0m][[34mroot[0m][[32mINFO[0m] - Step 501760 @ 511.5 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 1813.1, step = 501760, mean_episode_return = 0.25498, mean_episode_step = 64.291, total_loss = 974.81, entropy_loss = -8.6864, pg_loss = 685.09, baseline_loss = 298.41, learner_queue_size = 32, _tick = 195, _time = 1.7372e+09)[0m
[[36m2025-01-18 17:39:56,619[0m][[34mroot[0m][[32mINFO[0m] - Learning finished after 501760 steps.[0m
[[36m2025-01-18 17:39:56,619[0m][[34mroot[0m][[32mINFO[0m] - Saving checkpoint to /opt/minihack/checkpoint.tar[0m
