[2025-01-17 21:02:08,943][root][INFO] - name: null
wandb: false
project: minihack
entity: entity_name
group: default
mock: false
single_ttyrec: true
num_seeds: 0
write_profiler_trace: false
relative_reward: false
fn_penalty_step: constant
penalty_time: 0.0
penalty_step: -0.001
reward_lose: 0
reward_win: 1
save_tty: false
character: null
mode: train
env: MiniHack-Combat-Skill-v0
obs_keys: glyphs,chars,colors,specials,blstats,message
num_actors: 256
total_steps: 500000
batch_size: 32
unroll_length: 80
num_learner_threads: 1
num_inference_threads: 1
disable_cuda: false
learner_device: cuda:0
actor_device: cuda:0
max_learner_queue_size: null
model: baseline
use_lstm: true
hidden_dim: 256
embedding_dim: 64
glyph_type: all_cat
equalize_input_dim: false
equalize_factor: 2
layers: 5
crop_model: cnn
crop_dim: 9
use_index_select: true
entropy_cost: 0.001
baseline_cost: 0.5
discounting: 0.999
reward_clipping: none
normalize_reward: true
learning_rate: 0.0002
grad_norm_clipping: 40
alpha: 0.99
momentum: 0
epsilon: 1.0e-06
state_counter: none
no_extrinsic: false
int:
  twoheaded: true
  input: full
  intrinsic_weight: 0.1
  discounting: 0.99
  baseline_cost: 0.5
  episodic: true
  reward_clipping: none
  normalize_reward: true
ride:
  count_norm: true
  forward_cost: 1
  inverse_cost: 0.1
  hidden_dim: 128
rnd:
  forward_cost: 0.01
msg:
  model: none
  hidden_dim: 64
  embedding_dim: 32

[2025-01-17 21:02:08,995][root][INFO] - Symlinked log directory: /opt/minihack/latest
[2025-01-17 21:02:08,996][root][INFO] - Found archive directory: /opt/minihack/archives
[2025-01-17 21:02:09,000][root][INFO] - Logging results to /opt/minihack
[2025-01-17 21:02:09,055][palaas/out][INFO] - Found log directory: /opt/minihack
[2025-01-17 21:02:09,055][palaas/out][INFO] - Saving arguments to /opt/minihack/meta.json
[2025-01-17 21:02:09,056][palaas/out][INFO] - Saving messages to /opt/minihack/out.log
[2025-01-17 21:02:09,056][palaas/out][INFO] - Saving logs data to /opt/minihack/logs.csv
[2025-01-17 21:02:09,056][palaas/out][INFO] - Saving logs' fields to /opt/minihack/fields.csv
[2025-01-17 21:02:09,057][root][INFO] - Not using CUDA.
[2025-01-17 21:02:09,070][root][INFO] - Using model baseline
[2025-01-17 21:02:09,071][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,099][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,162][root][INFO] - Number of model parameters: 4264078
[2025-01-17 21:02:09,163][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,184][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,241][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,243][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,243][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,243][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,245][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,248][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,248][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,248][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,249][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,249][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,250][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,247][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,244][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,252][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,252][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,254][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,255][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,255][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,261][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,261][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,261][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,261][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,262][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,263][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,253][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,265][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,265][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,265][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,265][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,265][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,261][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,269][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,271][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,268][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,272][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,266][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,268][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,276][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,274][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,281][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,281][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,282][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,285][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,287][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,286][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,273][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,285][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,288][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,286][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,287][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,290][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,292][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,293][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,294][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,289][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,296][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,291][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,293][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,294][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,295][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,302][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,298][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,309][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:09,307][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:02:14,240][root][INFO] - Step 0 @ 0.0 SPS. Inference batcher size: 43. Learner queue size: 0. Other stats: (train_seconds = 5.0)
[2025-01-17 21:02:22,781][root][INFO] - Step 0 @ 0.0 SPS. Inference batcher size: 71. Learner queue size: 32. Other stats: (train_seconds = 13.5)
[2025-01-17 21:02:27,784][root][INFO] - Step 0 @ 0.0 SPS. Inference batcher size: 60. Learner queue size: 32. Other stats: (train_seconds = 18.5)
[2025-01-17 21:02:32,819][root][INFO] - Step 0 @ 0.0 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (train_seconds = 23.6)
[2025-01-17 21:02:37,824][root][INFO] - Step 0 @ 0.0 SPS. Inference batcher size: 34. Learner queue size: 32. Other stats: (train_seconds = 28.6)
[2025-01-17 21:02:42,830][root][INFO] - Step 0 @ 0.0 SPS. Inference batcher size: 91. Learner queue size: 32. Other stats: (train_seconds = 33.6)
[2025-01-17 21:02:44,483][palaas/out][INFO] - Updated log fields: ['_tick', '_time', 'train_seconds', 'step', 'mean_episode_return', 'mean_episode_step', 'total_loss', 'entropy_loss', 'pg_loss', 'baseline_loss', 'learner_queue_size']
[2025-01-17 21:02:47,836][root][INFO] - Saving checkpoint to /opt/minihack/checkpoint_0.tar
[2025-01-17 21:02:47,893][root][INFO] - Step 2560 @ 511.4 SPS. Inference batcher size: 61. Learner queue size: 32. Other stats: (train_seconds = 38.6, step = 2560, mean_episode_return = -0.026687, mean_episode_step = 31.829, total_loss = -1578.0, entropy_loss = -11.371, pg_loss = -1803.2, baseline_loss = 236.53, learner_queue_size = 32, _tick = 0, _time = 1.7371e+09)
[2025-01-17 21:02:52,899][root][INFO] - Step 2560 @ 0.0 SPS. Inference batcher size: 96. Learner queue size: 32. Other stats: (train_seconds = 43.7, step = 2560, mean_episode_return = -0.026687, mean_episode_step = 31.829, total_loss = -1578.0, entropy_loss = -11.371, pg_loss = -1803.2, baseline_loss = 236.53, learner_queue_size = 32, _tick = 0, _time = 1.7371e+09)
[2025-01-17 21:02:57,904][root][INFO] - Step 2560 @ 0.0 SPS. Inference batcher size: 68. Learner queue size: 32. Other stats: (train_seconds = 48.7, step = 2560, mean_episode_return = -0.026687, mean_episode_step = 31.829, total_loss = -1578.0, entropy_loss = -11.371, pg_loss = -1803.2, baseline_loss = 236.53, learner_queue_size = 32, _tick = 0, _time = 1.7371e+09)
[2025-01-17 21:03:02,915][root][INFO] - Step 5120 @ 510.9 SPS. Inference batcher size: 111. Learner queue size: 32. Other stats: (train_seconds = 53.7, step = 5120, mean_episode_return = -0.037706, mean_episode_step = 36.22, total_loss = 530.27, entropy_loss = -11.366, pg_loss = 313.0, baseline_loss = 228.64, learner_queue_size = 32, _tick = 1, _time = 1.7371e+09)
[2025-01-17 21:03:07,920][root][INFO] - Step 5120 @ 0.0 SPS. Inference batcher size: 44. Learner queue size: 32. Other stats: (train_seconds = 58.7, step = 5120, mean_episode_return = -0.037706, mean_episode_step = 36.22, total_loss = 530.27, entropy_loss = -11.366, pg_loss = 313.0, baseline_loss = 228.64, learner_queue_size = 32, _tick = 1, _time = 1.7371e+09)
[2025-01-17 21:03:12,951][root][INFO] - Step 7680 @ 508.9 SPS. Inference batcher size: 48. Learner queue size: 32. Other stats: (train_seconds = 63.7, step = 7680, mean_episode_return = -0.053148, mean_episode_step = 56.984, total_loss = 868.68, entropy_loss = -11.368, pg_loss = 615.03, baseline_loss = 265.01, learner_queue_size = 32, _tick = 2, _time = 1.7371e+09)
[2025-01-17 21:03:17,956][root][INFO] - Step 7680 @ 0.0 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (train_seconds = 68.7, step = 7680, mean_episode_return = -0.053148, mean_episode_step = 56.984, total_loss = 868.68, entropy_loss = -11.368, pg_loss = 615.03, baseline_loss = 265.01, learner_queue_size = 32, _tick = 2, _time = 1.7371e+09)
[2025-01-17 21:03:22,961][root][INFO] - Step 7680 @ 0.0 SPS. Inference batcher size: 59. Learner queue size: 32. Other stats: (train_seconds = 73.7, step = 7680, mean_episode_return = -0.053148, mean_episode_step = 56.984, total_loss = 868.68, entropy_loss = -11.368, pg_loss = 615.03, baseline_loss = 265.01, learner_queue_size = 32, _tick = 2, _time = 1.7371e+09)
[2025-01-17 21:03:27,966][root][INFO] - Step 10240 @ 511.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 78.7, step = 10240, mean_episode_return = -0.036394, mean_episode_step = 57.641, total_loss = -1077.9, entropy_loss = -11.32, pg_loss = -1158.5, baseline_loss = 91.943, learner_queue_size = 32, _tick = 3, _time = 1.7371e+09)
[2025-01-17 21:03:32,971][root][INFO] - Step 10240 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 83.7, step = 10240, mean_episode_return = -0.036394, mean_episode_step = 57.641, total_loss = -1077.9, entropy_loss = -11.32, pg_loss = -1158.5, baseline_loss = 91.943, learner_queue_size = 32, _tick = 3, _time = 1.7371e+09)
[2025-01-17 21:03:37,976][root][INFO] - Step 12800 @ 511.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 88.7, step = 12800, mean_episode_return = -0.056174, mean_episode_step = 49.706, total_loss = -325.34, entropy_loss = -11.33, pg_loss = -316.38, baseline_loss = 2.3657, learner_queue_size = 32, _tick = 4, _time = 1.7371e+09)
[2025-01-17 21:03:42,982][root][INFO] - Step 12800 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 93.7, step = 12800, mean_episode_return = -0.056174, mean_episode_step = 49.706, total_loss = -325.34, entropy_loss = -11.33, pg_loss = -316.38, baseline_loss = 2.3657, learner_queue_size = 32, _tick = 4, _time = 1.7371e+09)
[2025-01-17 21:03:47,989][root][INFO] - Step 15360 @ 511.3 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (train_seconds = 98.8, step = 15360, mean_episode_return = 0.053083, mean_episode_step = 36.218, total_loss = 289.34, entropy_loss = -11.26, pg_loss = 238.29, baseline_loss = 62.319, learner_queue_size = 32, _tick = 5, _time = 1.7371e+09)
[2025-01-17 21:03:52,995][root][INFO] - Step 17920 @ 511.4 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 103.8, step = 17920, mean_episode_return = -0.01384, mean_episode_step = 45.311, total_loss = 645.63, entropy_loss = -11.294, pg_loss = 578.29, baseline_loss = 78.634, learner_queue_size = 32, _tick = 6, _time = 1.7371e+09)
[2025-01-17 21:03:58,000][root][INFO] - Step 17920 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 108.8, step = 17920, mean_episode_return = -0.01384, mean_episode_step = 45.311, total_loss = 645.63, entropy_loss = -11.294, pg_loss = 578.29, baseline_loss = 78.634, learner_queue_size = 32, _tick = 6, _time = 1.7371e+09)
[2025-01-17 21:04:03,005][root][INFO] - Step 20480 @ 511.5 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (train_seconds = 113.8, step = 20480, mean_episode_return = -0.08775, mean_episode_step = 58.722, total_loss = -41.304, entropy_loss = -11.291, pg_loss = -113.06, baseline_loss = 83.052, learner_queue_size = 32, _tick = 7, _time = 1.7371e+09)
[2025-01-17 21:04:08,015][root][INFO] - Step 20480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 118.8, step = 20480, mean_episode_return = -0.08775, mean_episode_step = 58.722, total_loss = -41.304, entropy_loss = -11.291, pg_loss = -113.06, baseline_loss = 83.052, learner_queue_size = 32, _tick = 7, _time = 1.7371e+09)
[2025-01-17 21:04:13,020][root][INFO] - Step 23040 @ 511.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 123.8, step = 23040, mean_episode_return = 0.0415, mean_episode_step = 69.068, total_loss = 114.12, entropy_loss = -11.273, pg_loss = 70.09, baseline_loss = 55.303, learner_queue_size = 32, _tick = 8, _time = 1.7371e+09)
[2025-01-17 21:04:18,025][root][INFO] - Step 25600 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 128.8, step = 25600, mean_episode_return = -0.029571, mean_episode_step = 62.304, total_loss = -328.26, entropy_loss = -11.218, pg_loss = -317.9, baseline_loss = 0.85438, learner_queue_size = 32, _tick = 9, _time = 1.7371e+09)
[2025-01-17 21:04:23,031][root][INFO] - Step 25600 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 133.8, step = 25600, mean_episode_return = -0.029571, mean_episode_step = 62.304, total_loss = -328.26, entropy_loss = -11.218, pg_loss = -317.9, baseline_loss = 0.85438, learner_queue_size = 32, _tick = 9, _time = 1.7371e+09)
[2025-01-17 21:04:28,037][root][INFO] - Step 28160 @ 511.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 138.8, step = 28160, mean_episode_return = -0.083067, mean_episode_step = 58.375, total_loss = 242.93, entropy_loss = -11.209, pg_loss = 230.95, baseline_loss = 23.192, learner_queue_size = 32, _tick = 10, _time = 1.7371e+09)
[2025-01-17 21:04:33,043][root][INFO] - Step 28160 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 143.8, step = 28160, mean_episode_return = -0.083067, mean_episode_step = 58.375, total_loss = 242.93, entropy_loss = -11.209, pg_loss = 230.95, baseline_loss = 23.192, learner_queue_size = 32, _tick = 10, _time = 1.7371e+09)
[2025-01-17 21:04:38,048][root][INFO] - Step 30720 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 148.8, step = 30720, mean_episode_return = -0.033423, mean_episode_step = 56.188, total_loss = -282.45, entropy_loss = -11.213, pg_loss = -272.91, baseline_loss = 1.6651, learner_queue_size = 32, _tick = 11, _time = 1.7371e+09)
[2025-01-17 21:04:43,054][root][INFO] - Step 33280 @ 511.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 153.8, step = 33280, mean_episode_return = -0.021935, mean_episode_step = 46.419, total_loss = 427.33, entropy_loss = -11.195, pg_loss = 398.4, baseline_loss = 40.119, learner_queue_size = 32, _tick = 12, _time = 1.7371e+09)
[2025-01-17 21:04:48,055][root][INFO] - Step 33280 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 158.8, step = 33280, mean_episode_return = -0.021935, mean_episode_step = 46.419, total_loss = 427.33, entropy_loss = -11.195, pg_loss = 398.4, baseline_loss = 40.119, learner_queue_size = 32, _tick = 12, _time = 1.7371e+09)
[2025-01-17 21:04:53,060][root][INFO] - Step 35840 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 163.8, step = 35840, mean_episode_return = -0.0088788, mean_episode_step = 58.561, total_loss = 222.42, entropy_loss = -11.205, pg_loss = 157.31, baseline_loss = 76.315, learner_queue_size = 32, _tick = 13, _time = 1.7371e+09)
[2025-01-17 21:04:58,066][root][INFO] - Step 38400 @ 511.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 168.8, step = 38400, mean_episode_return = 0.063483, mean_episode_step = 61.868, total_loss = 384.98, entropy_loss = -11.16, pg_loss = 245.32, baseline_loss = 150.82, learner_queue_size = 32, _tick = 14, _time = 1.7371e+09)
[2025-01-17 21:05:03,072][root][INFO] - Step 38400 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 173.8, step = 38400, mean_episode_return = 0.063483, mean_episode_step = 61.868, total_loss = 384.98, entropy_loss = -11.16, pg_loss = 245.32, baseline_loss = 150.82, learner_queue_size = 32, _tick = 14, _time = 1.7371e+09)
[2025-01-17 21:05:08,078][root][INFO] - Step 40960 @ 511.4 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 178.8, step = 40960, mean_episode_return = -0.074706, mean_episode_step = 58.393, total_loss = -635.0, entropy_loss = -11.157, pg_loss = -627.11, baseline_loss = 3.2677, learner_queue_size = 32, _tick = 15, _time = 1.7371e+09)
[2025-01-17 21:05:13,083][root][INFO] - Step 40960 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 183.8, step = 40960, mean_episode_return = -0.074706, mean_episode_step = 58.393, total_loss = -635.0, entropy_loss = -11.157, pg_loss = -627.11, baseline_loss = 3.2677, learner_queue_size = 32, _tick = 15, _time = 1.7371e+09)
[2025-01-17 21:05:18,088][root][INFO] - Step 43520 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 188.9, step = 43520, mean_episode_return = -0.031043, mean_episode_step = 54.763, total_loss = 517.7, entropy_loss = -11.158, pg_loss = 382.4, baseline_loss = 146.46, learner_queue_size = 32, _tick = 16, _time = 1.7371e+09)
[2025-01-17 21:05:23,093][root][INFO] - Step 46080 @ 511.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 193.9, step = 46080, mean_episode_return = -0.010189, mean_episode_step = 59.207, total_loss = 275.84, entropy_loss = -11.131, pg_loss = 112.46, baseline_loss = 174.51, learner_queue_size = 32, _tick = 17, _time = 1.7371e+09)
[2025-01-17 21:05:28,098][root][INFO] - Step 46080 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 198.9, step = 46080, mean_episode_return = -0.010189, mean_episode_step = 59.207, total_loss = 275.84, entropy_loss = -11.131, pg_loss = 112.46, baseline_loss = 174.51, learner_queue_size = 32, _tick = 17, _time = 1.7371e+09)
[2025-01-17 21:05:33,103][root][INFO] - Step 48640 @ 511.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 203.9, step = 48640, mean_episode_return = -0.084647, mean_episode_step = 59.542, total_loss = 123.25, entropy_loss = -11.056, pg_loss = -29.859, baseline_loss = 164.17, learner_queue_size = 32, _tick = 18, _time = 1.7371e+09)
[2025-01-17 21:05:38,109][root][INFO] - Step 51200 @ 511.4 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 208.9, step = 51200, mean_episode_return = 0.00021214, mean_episode_step = 62.049, total_loss = -316.32, entropy_loss = -10.928, pg_loss = -332.02, baseline_loss = 26.63, learner_queue_size = 32, _tick = 19, _time = 1.7371e+09)
[2025-01-17 21:05:43,114][root][INFO] - Step 51200 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 213.9, step = 51200, mean_episode_return = 0.00021214, mean_episode_step = 62.049, total_loss = -316.32, entropy_loss = -10.928, pg_loss = -332.02, baseline_loss = 26.63, learner_queue_size = 32, _tick = 19, _time = 1.7371e+09)
[2025-01-17 21:05:48,119][root][INFO] - Step 53760 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 218.9, step = 53760, mean_episode_return = -0.070333, mean_episode_step = 64.125, total_loss = 212.71, entropy_loss = -10.925, pg_loss = 155.74, baseline_loss = 67.899, learner_queue_size = 32, _tick = 20, _time = 1.7371e+09)
[2025-01-17 21:05:53,124][root][INFO] - Step 53760 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 223.9, step = 53760, mean_episode_return = -0.070333, mean_episode_step = 64.125, total_loss = 212.71, entropy_loss = -10.925, pg_loss = 155.74, baseline_loss = 67.899, learner_queue_size = 32, _tick = 20, _time = 1.7371e+09)
[2025-01-17 21:05:58,129][root][INFO] - Step 56320 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 228.9, step = 56320, mean_episode_return = 0.012216, mean_episode_step = 55.607, total_loss = -116.29, entropy_loss = -10.887, pg_loss = -172.0, baseline_loss = 66.597, learner_queue_size = 32, _tick = 21, _time = 1.7371e+09)
[2025-01-17 21:06:03,135][root][INFO] - Step 58880 @ 511.4 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 233.9, step = 58880, mean_episode_return = -0.0020512, mean_episode_step = 54.67, total_loss = 291.97, entropy_loss = -10.876, pg_loss = 185.73, baseline_loss = 117.12, learner_queue_size = 32, _tick = 22, _time = 1.7371e+09)
[2025-01-17 21:06:08,140][root][INFO] - Step 58880 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 238.9, step = 58880, mean_episode_return = -0.0020512, mean_episode_step = 54.67, total_loss = 291.97, entropy_loss = -10.876, pg_loss = 185.73, baseline_loss = 117.12, learner_queue_size = 32, _tick = 22, _time = 1.7371e+09)
[2025-01-17 21:06:13,145][root][INFO] - Step 61440 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 243.9, step = 61440, mean_episode_return = -0.030973, mean_episode_step = 48.039, total_loss = -220.29, entropy_loss = -10.787, pg_loss = -252.93, baseline_loss = 43.425, learner_queue_size = 32, _tick = 23, _time = 1.7371e+09)
[2025-01-17 21:06:18,150][root][INFO] - Step 61440 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 248.9, step = 61440, mean_episode_return = -0.030973, mean_episode_step = 48.039, total_loss = -220.29, entropy_loss = -10.787, pg_loss = -252.93, baseline_loss = 43.425, learner_queue_size = 32, _tick = 23, _time = 1.7371e+09)
[2025-01-17 21:06:23,155][root][INFO] - Step 64000 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 253.9, step = 64000, mean_episode_return = -0.012152, mean_episode_step = 55.064, total_loss = 515.28, entropy_loss = -10.781, pg_loss = 370.04, baseline_loss = 156.02, learner_queue_size = 32, _tick = 24, _time = 1.7371e+09)
[2025-01-17 21:06:28,161][root][INFO] - Step 66560 @ 511.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (train_seconds = 258.9, step = 66560, mean_episode_return = -0.0031025, mean_episode_step = 45.967, total_loss = 429.34, entropy_loss = -10.702, pg_loss = 236.29, baseline_loss = 203.75, learner_queue_size = 32, _tick = 25, _time = 1.7371e+09)
[2025-01-17 21:06:33,166][root][INFO] - Step 66560 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 263.9, step = 66560, mean_episode_return = -0.0031025, mean_episode_step = 45.967, total_loss = 429.34, entropy_loss = -10.702, pg_loss = 236.29, baseline_loss = 203.75, learner_queue_size = 32, _tick = 25, _time = 1.7371e+09)
[2025-01-17 21:06:38,172][root][INFO] - Step 69120 @ 511.4 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 268.9, step = 69120, mean_episode_return = -0.058537, mean_episode_step = 55.151, total_loss = -126.28, entropy_loss = -10.575, pg_loss = -205.33, baseline_loss = 89.622, learner_queue_size = 32, _tick = 26, _time = 1.7371e+09)
[2025-01-17 21:06:43,178][root][INFO] - Step 71680 @ 511.4 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 273.9, step = 71680, mean_episode_return = -0.019543, mean_episode_step = 51.56, total_loss = 89.611, entropy_loss = -10.5, pg_loss = -6.7693, baseline_loss = 106.88, learner_queue_size = 32, _tick = 27, _time = 1.7371e+09)
[2025-01-17 21:06:48,184][root][INFO] - Step 71680 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 278.9, step = 71680, mean_episode_return = -0.019543, mean_episode_step = 51.56, total_loss = 89.611, entropy_loss = -10.5, pg_loss = -6.7693, baseline_loss = 106.88, learner_queue_size = 32, _tick = 27, _time = 1.7371e+09)
[2025-01-17 21:06:53,189][root][INFO] - Step 74240 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 284.0, step = 74240, mean_episode_return = 0.046282, mean_episode_step = 47.479, total_loss = 389.31, entropy_loss = -10.506, pg_loss = 252.3, baseline_loss = 147.51, learner_queue_size = 32, _tick = 28, _time = 1.7371e+09)
[2025-01-17 21:06:58,194][root][INFO] - Step 74240 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 289.0, step = 74240, mean_episode_return = 0.046282, mean_episode_step = 47.479, total_loss = 389.31, entropy_loss = -10.506, pg_loss = 252.3, baseline_loss = 147.51, learner_queue_size = 32, _tick = 28, _time = 1.7371e+09)
[2025-01-17 21:07:03,202][root][INFO] - Step 76800 @ 511.2 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 294.0, step = 76800, mean_episode_return = 0.020488, mean_episode_step = 48.146, total_loss = -79.74, entropy_loss = -10.533, pg_loss = -186.6, baseline_loss = 117.4, learner_queue_size = 32, _tick = 29, _time = 1.7371e+09)
[2025-01-17 21:07:08,207][root][INFO] - Step 79360 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 299.0, step = 79360, mean_episode_return = 0.0054091, mean_episode_step = 41.571, total_loss = -64.584, entropy_loss = -10.381, pg_loss = -128.72, baseline_loss = 74.519, learner_queue_size = 32, _tick = 30, _time = 1.7371e+09)
[2025-01-17 21:07:13,212][root][INFO] - Step 79360 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 304.0, step = 79360, mean_episode_return = 0.0054091, mean_episode_step = 41.571, total_loss = -64.584, entropy_loss = -10.381, pg_loss = -128.72, baseline_loss = 74.519, learner_queue_size = 32, _tick = 30, _time = 1.7371e+09)
[2025-01-17 21:07:18,217][root][INFO] - Step 81920 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 309.0, step = 81920, mean_episode_return = -0.0064999, mean_episode_step = 56.6, total_loss = 618.92, entropy_loss = -10.184, pg_loss = 474.79, baseline_loss = 154.3, learner_queue_size = 32, _tick = 31, _time = 1.7371e+09)
[2025-01-17 21:07:23,222][root][INFO] - Step 81920 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 314.0, step = 81920, mean_episode_return = -0.0064999, mean_episode_step = 56.6, total_loss = 618.92, entropy_loss = -10.184, pg_loss = 474.79, baseline_loss = 154.3, learner_queue_size = 32, _tick = 31, _time = 1.7371e+09)
[2025-01-17 21:07:28,227][root][INFO] - Step 84480 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 319.0, step = 84480, mean_episode_return = 0.017705, mean_episode_step = 47.182, total_loss = -227.58, entropy_loss = -10.173, pg_loss = -288.51, baseline_loss = 71.105, learner_queue_size = 32, _tick = 32, _time = 1.7371e+09)
[2025-01-17 21:07:33,232][root][INFO] - Step 87040 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 324.0, step = 87040, mean_episode_return = 0.067256, mean_episode_step = 57.168, total_loss = 360.7, entropy_loss = -10.111, pg_loss = 251.5, baseline_loss = 119.31, learner_queue_size = 32, _tick = 33, _time = 1.7371e+09)
[2025-01-17 21:07:38,238][root][INFO] - Step 87040 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 329.0, step = 87040, mean_episode_return = 0.067256, mean_episode_step = 57.168, total_loss = 360.7, entropy_loss = -10.111, pg_loss = 251.5, baseline_loss = 119.31, learner_queue_size = 32, _tick = 33, _time = 1.7371e+09)
[2025-01-17 21:07:43,245][root][INFO] - Step 89600 @ 511.3 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 334.0, step = 89600, mean_episode_return = -0.0048809, mean_episode_step = 47.883, total_loss = -46.385, entropy_loss = -10.082, pg_loss = -136.03, baseline_loss = 99.727, learner_queue_size = 32, _tick = 34, _time = 1.7371e+09)
[2025-01-17 21:07:48,251][root][INFO] - Step 92160 @ 511.3 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 339.0, step = 92160, mean_episode_return = -0.031327, mean_episode_step = 54.55, total_loss = -88.374, entropy_loss = -9.9982, pg_loss = -148.11, baseline_loss = 69.737, learner_queue_size = 32, _tick = 35, _time = 1.7371e+09)
[2025-01-17 21:07:53,257][root][INFO] - Step 92160 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 344.0, step = 92160, mean_episode_return = -0.031327, mean_episode_step = 54.55, total_loss = -88.374, entropy_loss = -9.9982, pg_loss = -148.11, baseline_loss = 69.737, learner_queue_size = 32, _tick = 35, _time = 1.7371e+09)
[2025-01-17 21:07:58,262][root][INFO] - Step 94720 @ 511.5 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 349.0, step = 94720, mean_episode_return = 0.017935, mean_episode_step = 62.753, total_loss = 1580.8, entropy_loss = -10.03, pg_loss = 1277.9, baseline_loss = 312.97, learner_queue_size = 32, _tick = 36, _time = 1.7371e+09)
[2025-01-17 21:08:03,267][root][INFO] - Step 94720 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 354.0, step = 94720, mean_episode_return = 0.017935, mean_episode_step = 62.753, total_loss = 1580.8, entropy_loss = -10.03, pg_loss = 1277.9, baseline_loss = 312.97, learner_queue_size = 32, _tick = 36, _time = 1.7371e+09)
[2025-01-17 21:08:08,272][root][INFO] - Step 97280 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 359.0, step = 97280, mean_episode_return = 0.045806, mean_episode_step = 51.76, total_loss = 832.14, entropy_loss = -10.035, pg_loss = 587.39, baseline_loss = 254.79, learner_queue_size = 32, _tick = 37, _time = 1.7371e+09)
[2025-01-17 21:08:13,285][root][INFO] - Step 99840 @ 510.7 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 364.1, step = 99840, mean_episode_return = 0.081333, mean_episode_step = 63.611, total_loss = 69.586, entropy_loss = -10.015, pg_loss = -111.6, baseline_loss = 191.2, learner_queue_size = 32, _tick = 38, _time = 1.7371e+09)
[2025-01-17 21:08:18,290][root][INFO] - Step 99840 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 369.1, step = 99840, mean_episode_return = 0.081333, mean_episode_step = 63.611, total_loss = 69.586, entropy_loss = -10.015, pg_loss = -111.6, baseline_loss = 191.2, learner_queue_size = 32, _tick = 38, _time = 1.7371e+09)
[2025-01-17 21:08:23,295][root][INFO] - Step 102400 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 374.1, step = 102400, mean_episode_return = 0.055167, mean_episode_step = 43.926, total_loss = 846.79, entropy_loss = -9.9088, pg_loss = 598.93, baseline_loss = 257.77, learner_queue_size = 32, _tick = 39, _time = 1.7371e+09)
[2025-01-17 21:08:28,301][root][INFO] - Step 104960 @ 511.4 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 379.1, step = 104960, mean_episode_return = 0.097387, mean_episode_step = 54.856, total_loss = 386.05, entropy_loss = -9.8826, pg_loss = 200.58, baseline_loss = 195.35, learner_queue_size = 32, _tick = 40, _time = 1.7371e+09)
[2025-01-17 21:08:33,306][root][INFO] - Step 104960 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 384.1, step = 104960, mean_episode_return = 0.097387, mean_episode_step = 54.856, total_loss = 386.05, entropy_loss = -9.8826, pg_loss = 200.58, baseline_loss = 195.35, learner_queue_size = 32, _tick = 40, _time = 1.7371e+09)
[2025-01-17 21:08:38,312][root][INFO] - Step 107520 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 389.1, step = 107520, mean_episode_return = 0.03882, mean_episode_step = 58.474, total_loss = -463.74, entropy_loss = -9.926, pg_loss = -593.92, baseline_loss = 140.1, learner_queue_size = 32, _tick = 41, _time = 1.7371e+09)
[2025-01-17 21:08:43,318][root][INFO] - Step 107520 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 394.1, step = 107520, mean_episode_return = 0.03882, mean_episode_step = 58.474, total_loss = -463.74, entropy_loss = -9.926, pg_loss = -593.92, baseline_loss = 140.1, learner_queue_size = 32, _tick = 41, _time = 1.7371e+09)
[2025-01-17 21:08:48,323][root][INFO] - Step 110080 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 399.1, step = 110080, mean_episode_return = 0.032863, mean_episode_step = 50.463, total_loss = 242.77, entropy_loss = -9.9304, pg_loss = 87.74, baseline_loss = 164.96, learner_queue_size = 32, _tick = 42, _time = 1.7371e+09)
[2025-01-17 21:08:53,328][root][INFO] - Step 112640 @ 511.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 404.1, step = 112640, mean_episode_return = 0.075143, mean_episode_step = 64.407, total_loss = 58.788, entropy_loss = -9.7594, pg_loss = -72.609, baseline_loss = 141.16, learner_queue_size = 32, _tick = 43, _time = 1.7371e+09)
[2025-01-17 21:08:58,333][root][INFO] - Step 112640 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 409.1, step = 112640, mean_episode_return = 0.075143, mean_episode_step = 64.407, total_loss = 58.788, entropy_loss = -9.7594, pg_loss = -72.609, baseline_loss = 141.16, learner_queue_size = 32, _tick = 43, _time = 1.7371e+09)
[2025-01-17 21:09:03,338][root][INFO] - Step 115200 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 414.1, step = 115200, mean_episode_return = 0.050929, mean_episode_step = 50.042, total_loss = 896.34, entropy_loss = -9.754, pg_loss = 592.73, baseline_loss = 313.37, learner_queue_size = 32, _tick = 44, _time = 1.7371e+09)
[2025-01-17 21:09:08,343][root][INFO] - Step 115200 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 419.1, step = 115200, mean_episode_return = 0.050929, mean_episode_step = 50.042, total_loss = 896.34, entropy_loss = -9.754, pg_loss = 592.73, baseline_loss = 313.37, learner_queue_size = 32, _tick = 44, _time = 1.7371e+09)
[2025-01-17 21:09:13,348][root][INFO] - Step 117760 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 424.1, step = 117760, mean_episode_return = 0.04675, mean_episode_step = 56.561, total_loss = -706.34, entropy_loss = -9.6962, pg_loss = -775.9, baseline_loss = 79.248, learner_queue_size = 32, _tick = 45, _time = 1.7371e+09)
[2025-01-17 21:09:18,353][root][INFO] - Step 120320 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 429.1, step = 120320, mean_episode_return = 0.074915, mean_episode_step = 48.613, total_loss = 404.16, entropy_loss = -9.6881, pg_loss = 194.7, baseline_loss = 219.15, learner_queue_size = 32, _tick = 46, _time = 1.7371e+09)
[2025-01-17 21:09:23,358][root][INFO] - Step 120320 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 434.1, step = 120320, mean_episode_return = 0.074915, mean_episode_step = 48.613, total_loss = 404.16, entropy_loss = -9.6881, pg_loss = 194.7, baseline_loss = 219.15, learner_queue_size = 32, _tick = 46, _time = 1.7371e+09)
[2025-01-17 21:09:28,363][root][INFO] - Step 122880 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 439.1, step = 122880, mean_episode_return = 0.073936, mean_episode_step = 57.016, total_loss = -85.085, entropy_loss = -9.668, pg_loss = -282.92, baseline_loss = 207.5, learner_queue_size = 32, _tick = 47, _time = 1.7371e+09)
[2025-01-17 21:09:33,368][root][INFO] - Saving checkpoint to /opt/minihack/checkpoint_0.25.tar
[2025-01-17 21:09:33,422][root][INFO] - Step 125440 @ 511.5 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (train_seconds = 444.1, step = 125440, mean_episode_return = 0.059762, mean_episode_step = 54.111, total_loss = 783.72, entropy_loss = -9.6851, pg_loss = 542.26, baseline_loss = 251.14, learner_queue_size = 32, _tick = 48, _time = 1.7371e+09)
[2025-01-17 21:09:38,428][root][INFO] - Step 125440 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 449.2, step = 125440, mean_episode_return = 0.059762, mean_episode_step = 54.111, total_loss = 783.72, entropy_loss = -9.6851, pg_loss = 542.26, baseline_loss = 251.14, learner_queue_size = 32, _tick = 48, _time = 1.7371e+09)
[2025-01-17 21:09:43,434][root][INFO] - Step 128000 @ 511.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 454.2, step = 128000, mean_episode_return = 0.086978, mean_episode_step = 57.766, total_loss = 19.621, entropy_loss = -9.6052, pg_loss = -121.55, baseline_loss = 150.77, learner_queue_size = 32, _tick = 49, _time = 1.7371e+09)
[2025-01-17 21:09:48,439][root][INFO] - Step 130560 @ 511.4 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (train_seconds = 459.2, step = 130560, mean_episode_return = 0.058739, mean_episode_step = 55.318, total_loss = 191.77, entropy_loss = -9.4995, pg_loss = 12.443, baseline_loss = 188.82, learner_queue_size = 32, _tick = 50, _time = 1.7371e+09)
[2025-01-17 21:09:53,444][root][INFO] - Step 130560 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 464.2, step = 130560, mean_episode_return = 0.058739, mean_episode_step = 55.318, total_loss = 191.77, entropy_loss = -9.4995, pg_loss = 12.443, baseline_loss = 188.82, learner_queue_size = 32, _tick = 50, _time = 1.7371e+09)
[2025-01-17 21:09:58,450][root][INFO] - Step 133120 @ 511.4 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 469.2, step = 133120, mean_episode_return = 0.047375, mean_episode_step = 63.502, total_loss = 268.2, entropy_loss = -9.44, pg_loss = 117.84, baseline_loss = 159.8, learner_queue_size = 32, _tick = 51, _time = 1.7371e+09)
[2025-01-17 21:10:03,455][root][INFO] - Step 133120 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 474.2, step = 133120, mean_episode_return = 0.047375, mean_episode_step = 63.502, total_loss = 268.2, entropy_loss = -9.44, pg_loss = 117.84, baseline_loss = 159.8, learner_queue_size = 32, _tick = 51, _time = 1.7371e+09)
[2025-01-17 21:10:08,461][root][INFO] - Step 135680 @ 511.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 479.2, step = 135680, mean_episode_return = 0.0992, mean_episode_step = 63.229, total_loss = -0.64085, entropy_loss = -9.3433, pg_loss = -119.2, baseline_loss = 127.91, learner_queue_size = 32, _tick = 52, _time = 1.7371e+09)
[2025-01-17 21:10:13,466][root][INFO] - Step 138240 @ 511.4 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (train_seconds = 484.2, step = 138240, mean_episode_return = 0.16529, mean_episode_step = 51.921, total_loss = 86.88, entropy_loss = -9.2505, pg_loss = -11.704, baseline_loss = 107.83, learner_queue_size = 32, _tick = 53, _time = 1.7371e+09)
[2025-01-17 21:10:18,471][root][INFO] - Step 138240 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 489.2, step = 138240, mean_episode_return = 0.16529, mean_episode_step = 51.921, total_loss = 86.88, entropy_loss = -9.2505, pg_loss = -11.704, baseline_loss = 107.83, learner_queue_size = 32, _tick = 53, _time = 1.7371e+09)
[2025-01-17 21:10:23,476][root][INFO] - Step 140800 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 494.2, step = 140800, mean_episode_return = 0.057621, mean_episode_step = 59.608, total_loss = 40.908, entropy_loss = -9.313, pg_loss = -49.693, baseline_loss = 99.914, learner_queue_size = 32, _tick = 54, _time = 1.7371e+09)
[2025-01-17 21:10:28,482][root][INFO] - Step 143360 @ 511.5 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 499.2, step = 143360, mean_episode_return = 0.07371, mean_episode_step = 66.925, total_loss = -50.46, entropy_loss = -9.1488, pg_loss = -128.08, baseline_loss = 86.769, learner_queue_size = 32, _tick = 55, _time = 1.7371e+09)
[2025-01-17 21:10:33,488][root][INFO] - Step 143360 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 504.3, step = 143360, mean_episode_return = 0.07371, mean_episode_step = 66.925, total_loss = -50.46, entropy_loss = -9.1488, pg_loss = -128.08, baseline_loss = 86.769, learner_queue_size = 32, _tick = 55, _time = 1.7371e+09)
[2025-01-17 21:10:38,493][root][INFO] - Step 145920 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 509.3, step = 145920, mean_episode_return = 0.020475, mean_episode_step = 54.26, total_loss = 370.68, entropy_loss = -8.9583, pg_loss = 281.2, baseline_loss = 98.441, learner_queue_size = 32, _tick = 56, _time = 1.7371e+09)
[2025-01-17 21:10:43,498][root][INFO] - Step 145920 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 514.3, step = 145920, mean_episode_return = 0.020475, mean_episode_step = 54.26, total_loss = 370.68, entropy_loss = -8.9583, pg_loss = 281.2, baseline_loss = 98.441, learner_queue_size = 32, _tick = 56, _time = 1.7371e+09)
[2025-01-17 21:10:48,503][root][INFO] - Step 148480 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 519.3, step = 148480, mean_episode_return = 0.063625, mean_episode_step = 77.491, total_loss = 292.14, entropy_loss = -8.8971, pg_loss = 174.51, baseline_loss = 126.52, learner_queue_size = 32, _tick = 57, _time = 1.7371e+09)
[2025-01-17 21:10:53,508][root][INFO] - Step 151040 @ 511.5 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 524.3, step = 151040, mean_episode_return = 0.12326, mean_episode_step = 67.28, total_loss = 333.77, entropy_loss = -8.826, pg_loss = 205.04, baseline_loss = 137.55, learner_queue_size = 32, _tick = 58, _time = 1.7371e+09)
[2025-01-17 21:10:58,513][root][INFO] - Step 151040 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 529.3, step = 151040, mean_episode_return = 0.12326, mean_episode_step = 67.28, total_loss = 333.77, entropy_loss = -8.826, pg_loss = 205.04, baseline_loss = 137.55, learner_queue_size = 32, _tick = 58, _time = 1.7371e+09)
[2025-01-17 21:11:03,518][root][INFO] - Step 153600 @ 511.5 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (train_seconds = 534.3, step = 153600, mean_episode_return = 0.11956, mean_episode_step = 61.837, total_loss = -57.188, entropy_loss = -8.7836, pg_loss = -176.79, baseline_loss = 128.38, learner_queue_size = 32, _tick = 59, _time = 1.7371e+09)
[2025-01-17 21:11:08,523][root][INFO] - Step 153600 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 539.3, step = 153600, mean_episode_return = 0.11956, mean_episode_step = 61.837, total_loss = -57.188, entropy_loss = -8.7836, pg_loss = -176.79, baseline_loss = 128.38, learner_queue_size = 32, _tick = 59, _time = 1.7371e+09)
[2025-01-17 21:11:13,528][root][INFO] - Step 156160 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 544.3, step = 156160, mean_episode_return = 0.14754, mean_episode_step = 72.754, total_loss = -32.911, entropy_loss = -8.7747, pg_loss = -155.65, baseline_loss = 131.52, learner_queue_size = 32, _tick = 60, _time = 1.7371e+09)
[2025-01-17 21:11:18,533][root][INFO] - Step 158720 @ 511.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 549.3, step = 158720, mean_episode_return = 0.070216, mean_episode_step = 51.428, total_loss = 778.97, entropy_loss = -8.7267, pg_loss = 578.74, baseline_loss = 208.96, learner_queue_size = 32, _tick = 61, _time = 1.7371e+09)
[2025-01-17 21:11:23,538][root][INFO] - Step 158720 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 554.3, step = 158720, mean_episode_return = 0.070216, mean_episode_step = 51.428, total_loss = 778.97, entropy_loss = -8.7267, pg_loss = 578.74, baseline_loss = 208.96, learner_queue_size = 32, _tick = 61, _time = 1.7371e+09)
[2025-01-17 21:11:28,544][root][INFO] - Step 161280 @ 511.4 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 559.3, step = 161280, mean_episode_return = 0.1348, mean_episode_step = 62.543, total_loss = -799.59, entropy_loss = -8.7698, pg_loss = -850.06, baseline_loss = 59.232, learner_queue_size = 32, _tick = 62, _time = 1.7371e+09)
[2025-01-17 21:11:33,549][root][INFO] - Step 163840 @ 511.5 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (train_seconds = 564.3, step = 163840, mean_episode_return = 0.077763, mean_episode_step = 55.105, total_loss = -431.27, entropy_loss = -8.7689, pg_loss = -565.39, baseline_loss = 142.89, learner_queue_size = 32, _tick = 63, _time = 1.7371e+09)
[2025-01-17 21:11:38,555][root][INFO] - Step 163840 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 569.3, step = 163840, mean_episode_return = 0.077763, mean_episode_step = 55.105, total_loss = -431.27, entropy_loss = -8.7689, pg_loss = -565.39, baseline_loss = 142.89, learner_queue_size = 32, _tick = 63, _time = 1.7371e+09)
[2025-01-17 21:11:43,560][root][INFO] - Step 166400 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 574.3, step = 166400, mean_episode_return = 0.13164, mean_episode_step = 69.936, total_loss = 408.65, entropy_loss = -8.7418, pg_loss = 238.32, baseline_loss = 179.07, learner_queue_size = 32, _tick = 64, _time = 1.7371e+09)
[2025-01-17 21:11:48,565][root][INFO] - Step 166400 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 579.3, step = 166400, mean_episode_return = 0.13164, mean_episode_step = 69.936, total_loss = 408.65, entropy_loss = -8.7418, pg_loss = 238.32, baseline_loss = 179.07, learner_queue_size = 32, _tick = 64, _time = 1.7371e+09)
[2025-01-17 21:11:53,570][root][INFO] - Step 168960 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 584.3, step = 168960, mean_episode_return = 0.13382, mean_episode_step = 71.99, total_loss = 84.353, entropy_loss = -8.8218, pg_loss = -109.94, baseline_loss = 203.11, learner_queue_size = 32, _tick = 65, _time = 1.7371e+09)
[2025-01-17 21:11:58,575][root][INFO] - Step 171520 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 589.3, step = 171520, mean_episode_return = 0.14826, mean_episode_step = 69.57, total_loss = 539.06, entropy_loss = -8.7608, pg_loss = 334.21, baseline_loss = 213.61, learner_queue_size = 32, _tick = 66, _time = 1.7371e+09)
[2025-01-17 21:12:03,580][root][INFO] - Step 171520 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 594.3, step = 171520, mean_episode_return = 0.14826, mean_episode_step = 69.57, total_loss = 539.06, entropy_loss = -8.7608, pg_loss = 334.21, baseline_loss = 213.61, learner_queue_size = 32, _tick = 66, _time = 1.7371e+09)
[2025-01-17 21:12:08,587][root][INFO] - Step 174080 @ 511.3 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 599.4, step = 174080, mean_episode_return = 0.16971, mean_episode_step = 70.198, total_loss = 242.99, entropy_loss = -8.7677, pg_loss = 54.518, baseline_loss = 197.24, learner_queue_size = 32, _tick = 67, _time = 1.7371e+09)
[2025-01-17 21:12:13,592][root][INFO] - Saving checkpoint to /opt/minihack/checkpoint.tar
[2025-01-17 21:12:13,622][root][INFO] - Step 176640 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 604.4, step = 176640, mean_episode_return = 0.16552, mean_episode_step = 62.696, total_loss = -211.19, entropy_loss = -8.7574, pg_loss = -355.92, baseline_loss = 153.48, learner_queue_size = 32, _tick = 68, _time = 1.7371e+09)
[2025-01-17 21:12:18,627][root][INFO] - Step 176640 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 609.4, step = 176640, mean_episode_return = 0.16552, mean_episode_step = 62.696, total_loss = -211.19, entropy_loss = -8.7574, pg_loss = -355.92, baseline_loss = 153.48, learner_queue_size = 32, _tick = 68, _time = 1.7371e+09)
[2025-01-17 21:12:23,632][root][INFO] - Step 179200 @ 511.5 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (train_seconds = 614.4, step = 179200, mean_episode_return = 0.13331, mean_episode_step = 73.818, total_loss = 521.94, entropy_loss = -8.7373, pg_loss = 317.37, baseline_loss = 213.3, learner_queue_size = 32, _tick = 69, _time = 1.7371e+09)
[2025-01-17 21:12:28,637][root][INFO] - Step 179200 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 619.4, step = 179200, mean_episode_return = 0.13331, mean_episode_step = 73.818, total_loss = 521.94, entropy_loss = -8.7373, pg_loss = 317.37, baseline_loss = 213.3, learner_queue_size = 32, _tick = 69, _time = 1.7371e+09)
[2025-01-17 21:12:33,644][root][INFO] - Step 181760 @ 511.3 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (train_seconds = 624.4, step = 181760, mean_episode_return = 0.21752, mean_episode_step = 83.111, total_loss = 531.32, entropy_loss = -8.7873, pg_loss = 340.54, baseline_loss = 199.57, learner_queue_size = 32, _tick = 70, _time = 1.7371e+09)
[2025-01-17 21:12:38,651][root][INFO] - Step 184320 @ 511.4 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 629.4, step = 184320, mean_episode_return = 0.19959, mean_episode_step = 66.48, total_loss = -168.23, entropy_loss = -8.8026, pg_loss = -349.74, baseline_loss = 190.32, learner_queue_size = 32, _tick = 71, _time = 1.7371e+09)
[2025-01-17 21:12:43,656][root][INFO] - Step 184320 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 634.4, step = 184320, mean_episode_return = 0.19959, mean_episode_step = 66.48, total_loss = -168.23, entropy_loss = -8.8026, pg_loss = -349.74, baseline_loss = 190.32, learner_queue_size = 32, _tick = 71, _time = 1.7371e+09)
[2025-01-17 21:12:48,664][root][INFO] - Step 186880 @ 511.2 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 639.4, step = 186880, mean_episode_return = 0.18239, mean_episode_step = 73.161, total_loss = -274.72, entropy_loss = -8.7678, pg_loss = -467.37, baseline_loss = 201.42, learner_queue_size = 32, _tick = 72, _time = 1.7371e+09)
[2025-01-17 21:12:53,669][root][INFO] - Step 186880 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 644.4, step = 186880, mean_episode_return = 0.18239, mean_episode_step = 73.161, total_loss = -274.72, entropy_loss = -8.7678, pg_loss = -467.37, baseline_loss = 201.42, learner_queue_size = 32, _tick = 72, _time = 1.7371e+09)
[2025-01-17 21:12:58,674][root][INFO] - Step 189440 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 649.4, step = 189440, mean_episode_return = 0.12143, mean_episode_step = 79.879, total_loss = -810.15, entropy_loss = -8.7957, pg_loss = -901.54, baseline_loss = 100.19, learner_queue_size = 32, _tick = 73, _time = 1.7371e+09)
[2025-01-17 21:13:03,679][root][INFO] - Step 192000 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 654.4, step = 192000, mean_episode_return = 0.11563, mean_episode_step = 78.699, total_loss = 4.202, entropy_loss = -8.7964, pg_loss = -177.38, baseline_loss = 190.38, learner_queue_size = 32, _tick = 74, _time = 1.7371e+09)
[2025-01-17 21:13:08,684][root][INFO] - Step 192000 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 659.4, step = 192000, mean_episode_return = 0.11563, mean_episode_step = 78.699, total_loss = 4.202, entropy_loss = -8.7964, pg_loss = -177.38, baseline_loss = 190.38, learner_queue_size = 32, _tick = 74, _time = 1.7371e+09)
[2025-01-17 21:13:13,689][root][INFO] - Step 194560 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 664.5, step = 194560, mean_episode_return = 0.23137, mean_episode_step = 63.53, total_loss = 680.07, entropy_loss = -8.816, pg_loss = 501.24, baseline_loss = 187.64, learner_queue_size = 32, _tick = 75, _time = 1.7371e+09)
[2025-01-17 21:13:18,694][root][INFO] - Step 197120 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 669.5, step = 197120, mean_episode_return = 0.23094, mean_episode_step = 74.434, total_loss = 804.41, entropy_loss = -8.7686, pg_loss = 585.76, baseline_loss = 227.42, learner_queue_size = 32, _tick = 76, _time = 1.7371e+09)
[2025-01-17 21:13:23,699][root][INFO] - Step 197120 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 674.5, step = 197120, mean_episode_return = 0.23094, mean_episode_step = 74.434, total_loss = 804.41, entropy_loss = -8.7686, pg_loss = 585.76, baseline_loss = 227.42, learner_queue_size = 32, _tick = 76, _time = 1.7371e+09)
[2025-01-17 21:13:28,704][root][INFO] - Step 199680 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 679.5, step = 199680, mean_episode_return = 0.10622, mean_episode_step = 68.329, total_loss = -278.65, entropy_loss = -8.7085, pg_loss = -417.12, baseline_loss = 147.17, learner_queue_size = 32, _tick = 77, _time = 1.7371e+09)
[2025-01-17 21:13:33,709][root][INFO] - Step 199680 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 684.5, step = 199680, mean_episode_return = 0.10622, mean_episode_step = 68.329, total_loss = -278.65, entropy_loss = -8.7085, pg_loss = -417.12, baseline_loss = 147.17, learner_queue_size = 32, _tick = 77, _time = 1.7371e+09)
[2025-01-17 21:13:38,714][root][INFO] - Step 202240 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 689.5, step = 202240, mean_episode_return = 0.21415, mean_episode_step = 66.602, total_loss = 372.12, entropy_loss = -8.6943, pg_loss = 162.48, baseline_loss = 218.33, learner_queue_size = 32, _tick = 78, _time = 1.7371e+09)
[2025-01-17 21:13:43,719][root][INFO] - Step 204800 @ 511.5 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (train_seconds = 694.5, step = 204800, mean_episode_return = 0.18471, mean_episode_step = 60.781, total_loss = -95.447, entropy_loss = -8.696, pg_loss = -264.57, baseline_loss = 177.82, learner_queue_size = 32, _tick = 79, _time = 1.7371e+09)
[2025-01-17 21:13:48,720][root][INFO] - Step 204800 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 699.5, step = 204800, mean_episode_return = 0.18471, mean_episode_step = 60.781, total_loss = -95.447, entropy_loss = -8.696, pg_loss = -264.57, baseline_loss = 177.82, learner_queue_size = 32, _tick = 79, _time = 1.7371e+09)
[2025-01-17 21:13:53,725][root][INFO] - Step 207360 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 704.5, step = 207360, mean_episode_return = 0.16288, mean_episode_step = 80.154, total_loss = -680.64, entropy_loss = -8.6862, pg_loss = -755.01, baseline_loss = 83.054, learner_queue_size = 32, _tick = 80, _time = 1.7371e+09)
[2025-01-17 21:13:58,730][root][INFO] - Step 209920 @ 511.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 709.5, step = 209920, mean_episode_return = 0.3295, mean_episode_step = 83.036, total_loss = 493.24, entropy_loss = -8.6762, pg_loss = 314.53, baseline_loss = 187.38, learner_queue_size = 32, _tick = 81, _time = 1.7371e+09)
[2025-01-17 21:14:03,735][root][INFO] - Step 209920 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 714.5, step = 209920, mean_episode_return = 0.3295, mean_episode_step = 83.036, total_loss = 493.24, entropy_loss = -8.6762, pg_loss = 314.53, baseline_loss = 187.38, learner_queue_size = 32, _tick = 81, _time = 1.7371e+09)
[2025-01-17 21:14:08,740][root][INFO] - Step 212480 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 719.5, step = 212480, mean_episode_return = 0.16118, mean_episode_step = 69.021, total_loss = -629.88, entropy_loss = -8.6471, pg_loss = -744.69, baseline_loss = 123.46, learner_queue_size = 32, _tick = 82, _time = 1.7371e+09)
[2025-01-17 21:14:13,745][root][INFO] - Step 212480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 724.5, step = 212480, mean_episode_return = 0.16118, mean_episode_step = 69.021, total_loss = -629.88, entropy_loss = -8.6471, pg_loss = -744.69, baseline_loss = 123.46, learner_queue_size = 32, _tick = 82, _time = 1.7371e+09)
[2025-01-17 21:14:18,750][root][INFO] - Step 215040 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 729.5, step = 215040, mean_episode_return = 0.12069, mean_episode_step = 76.193, total_loss = -73.588, entropy_loss = -8.6525, pg_loss = -195.6, baseline_loss = 130.66, learner_queue_size = 32, _tick = 83, _time = 1.7371e+09)
[2025-01-17 21:14:23,755][root][INFO] - Step 217600 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 734.5, step = 217600, mean_episode_return = 0.13697, mean_episode_step = 73.943, total_loss = 388.79, entropy_loss = -8.6929, pg_loss = 230.23, baseline_loss = 167.25, learner_queue_size = 32, _tick = 84, _time = 1.7371e+09)
[2025-01-17 21:14:28,760][root][INFO] - Step 217600 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 739.5, step = 217600, mean_episode_return = 0.13697, mean_episode_step = 73.943, total_loss = 388.79, entropy_loss = -8.6929, pg_loss = 230.23, baseline_loss = 167.25, learner_queue_size = 32, _tick = 84, _time = 1.7371e+09)
[2025-01-17 21:14:33,765][root][INFO] - Step 220160 @ 511.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 744.5, step = 220160, mean_episode_return = 0.18341, mean_episode_step = 69.817, total_loss = -997.24, entropy_loss = -8.688, pg_loss = -1093.9, baseline_loss = 105.37, learner_queue_size = 32, _tick = 85, _time = 1.7371e+09)
[2025-01-17 21:14:38,770][root][INFO] - Step 222720 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 749.5, step = 222720, mean_episode_return = 0.25042, mean_episode_step = 75.107, total_loss = -247.58, entropy_loss = -8.6945, pg_loss = -395.91, baseline_loss = 157.03, learner_queue_size = 32, _tick = 86, _time = 1.7371e+09)
[2025-01-17 21:14:43,776][root][INFO] - Step 222720 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 754.5, step = 222720, mean_episode_return = 0.25042, mean_episode_step = 75.107, total_loss = -247.58, entropy_loss = -8.6945, pg_loss = -395.91, baseline_loss = 157.03, learner_queue_size = 32, _tick = 86, _time = 1.7371e+09)
[2025-01-17 21:14:48,783][root][INFO] - Step 225280 @ 511.2 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 759.5, step = 225280, mean_episode_return = 0.31727, mean_episode_step = 57.716, total_loss = 896.58, entropy_loss = -8.7082, pg_loss = 667.39, baseline_loss = 237.9, learner_queue_size = 32, _tick = 87, _time = 1.7371e+09)
[2025-01-17 21:14:53,804][root][INFO] - Step 225280 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 764.6, step = 225280, mean_episode_return = 0.31727, mean_episode_step = 57.716, total_loss = 896.58, entropy_loss = -8.7082, pg_loss = 667.39, baseline_loss = 237.9, learner_queue_size = 32, _tick = 87, _time = 1.7371e+09)
[2025-01-17 21:14:58,810][root][INFO] - Step 227840 @ 511.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 769.6, step = 227840, mean_episode_return = 0.22829, mean_episode_step = 78.057, total_loss = 1300.2, entropy_loss = -8.6756, pg_loss = 1027.6, baseline_loss = 281.32, learner_queue_size = 32, _tick = 88, _time = 1.7371e+09)
[2025-01-17 21:15:03,816][root][INFO] - Step 230400 @ 511.4 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 774.6, step = 230400, mean_episode_return = 0.22178, mean_episode_step = 65.854, total_loss = 218.15, entropy_loss = -8.671, pg_loss = -10.378, baseline_loss = 237.2, learner_queue_size = 32, _tick = 89, _time = 1.7371e+09)
[2025-01-17 21:15:08,821][root][INFO] - Step 230400 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 779.6, step = 230400, mean_episode_return = 0.22178, mean_episode_step = 65.854, total_loss = 218.15, entropy_loss = -8.671, pg_loss = -10.378, baseline_loss = 237.2, learner_queue_size = 32, _tick = 89, _time = 1.7371e+09)
[2025-01-17 21:15:13,826][root][INFO] - Step 232960 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 784.6, step = 232960, mean_episode_return = 0.15504, mean_episode_step = 74.762, total_loss = -59.61, entropy_loss = -8.6159, pg_loss = -212.34, baseline_loss = 161.34, learner_queue_size = 32, _tick = 90, _time = 1.7371e+09)
[2025-01-17 21:15:18,831][root][INFO] - Step 235520 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 789.6, step = 235520, mean_episode_return = 0.15468, mean_episode_step = 72.396, total_loss = 304.38, entropy_loss = -8.6314, pg_loss = 117.05, baseline_loss = 195.96, learner_queue_size = 32, _tick = 91, _time = 1.7371e+09)
[2025-01-17 21:15:23,837][root][INFO] - Step 235520 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 794.6, step = 235520, mean_episode_return = 0.15468, mean_episode_step = 72.396, total_loss = 304.38, entropy_loss = -8.6314, pg_loss = 117.05, baseline_loss = 195.96, learner_queue_size = 32, _tick = 91, _time = 1.7371e+09)
[2025-01-17 21:15:28,843][root][INFO] - Step 238080 @ 511.4 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 799.6, step = 238080, mean_episode_return = 0.26324, mean_episode_step = 85.86, total_loss = 445.03, entropy_loss = -8.5841, pg_loss = 271.31, baseline_loss = 182.3, learner_queue_size = 32, _tick = 92, _time = 1.7371e+09)
[2025-01-17 21:15:33,849][root][INFO] - Step 240640 @ 511.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 804.6, step = 240640, mean_episode_return = 0.13463, mean_episode_step = 71.434, total_loss = -689.87, entropy_loss = -8.582, pg_loss = -791.21, baseline_loss = 109.91, learner_queue_size = 32, _tick = 93, _time = 1.7371e+09)
[2025-01-17 21:15:38,854][root][INFO] - Step 240640 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 809.6, step = 240640, mean_episode_return = 0.13463, mean_episode_step = 71.434, total_loss = -689.87, entropy_loss = -8.582, pg_loss = -791.21, baseline_loss = 109.91, learner_queue_size = 32, _tick = 93, _time = 1.7371e+09)
[2025-01-17 21:15:43,861][root][INFO] - Step 243200 @ 511.3 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 814.6, step = 243200, mean_episode_return = 0.20362, mean_episode_step = 83.165, total_loss = 856.99, entropy_loss = -8.5818, pg_loss = 660.53, baseline_loss = 205.04, learner_queue_size = 32, _tick = 94, _time = 1.7371e+09)
[2025-01-17 21:15:48,866][root][INFO] - Step 243200 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 819.6, step = 243200, mean_episode_return = 0.20362, mean_episode_step = 83.165, total_loss = 856.99, entropy_loss = -8.5818, pg_loss = 660.53, baseline_loss = 205.04, learner_queue_size = 32, _tick = 94, _time = 1.7371e+09)
[2025-01-17 21:15:53,875][root][INFO] - Step 245760 @ 511.1 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 824.6, step = 245760, mean_episode_return = 0.15959, mean_episode_step = 66.861, total_loss = -787.0, entropy_loss = -8.5753, pg_loss = -899.16, baseline_loss = 120.74, learner_queue_size = 32, _tick = 95, _time = 1.7371e+09)
[2025-01-17 21:15:58,880][root][INFO] - Step 248320 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 829.6, step = 248320, mean_episode_return = 0.090703, mean_episode_step = 78.641, total_loss = -279.59, entropy_loss = -8.5776, pg_loss = -424.93, baseline_loss = 153.92, learner_queue_size = 32, _tick = 96, _time = 1.7371e+09)
[2025-01-17 21:16:03,885][root][INFO] - Step 248320 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 834.7, step = 248320, mean_episode_return = 0.090703, mean_episode_step = 78.641, total_loss = -279.59, entropy_loss = -8.5776, pg_loss = -424.93, baseline_loss = 153.92, learner_queue_size = 32, _tick = 96, _time = 1.7371e+09)
[2025-01-17 21:16:08,890][root][INFO] - Saving checkpoint to /opt/minihack/checkpoint_0.5.tar
[2025-01-17 21:16:08,922][root][INFO] - Step 250880 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 839.7, step = 250880, mean_episode_return = 0.18411, mean_episode_step = 68.448, total_loss = 209.33, entropy_loss = -8.5859, pg_loss = 43.158, baseline_loss = 174.76, learner_queue_size = 32, _tick = 97, _time = 1.7371e+09)
[2025-01-17 21:16:13,927][root][INFO] - Step 253440 @ 508.2 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 844.7, step = 253440, mean_episode_return = 0.17464, mean_episode_step = 77.434, total_loss = -457.95, entropy_loss = -8.5683, pg_loss = -555.78, baseline_loss = 106.4, learner_queue_size = 32, _tick = 98, _time = 1.7371e+09)
[2025-01-17 21:16:18,932][root][INFO] - Step 253440 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 849.7, step = 253440, mean_episode_return = 0.17464, mean_episode_step = 77.434, total_loss = -457.95, entropy_loss = -8.5683, pg_loss = -555.78, baseline_loss = 106.4, learner_queue_size = 32, _tick = 98, _time = 1.7371e+09)
[2025-01-17 21:16:23,937][root][INFO] - Step 256000 @ 511.5 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (train_seconds = 854.7, step = 256000, mean_episode_return = 0.24655, mean_episode_step = 64.039, total_loss = -176.0, entropy_loss = -8.5754, pg_loss = -284.43, baseline_loss = 117.01, learner_queue_size = 32, _tick = 99, _time = 1.7371e+09)
[2025-01-17 21:16:28,942][root][INFO] - Step 256000 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 859.7, step = 256000, mean_episode_return = 0.24655, mean_episode_step = 64.039, total_loss = -176.0, entropy_loss = -8.5754, pg_loss = -284.43, baseline_loss = 117.01, learner_queue_size = 32, _tick = 99, _time = 1.7371e+09)
[2025-01-17 21:16:33,947][root][INFO] - Step 258560 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 864.7, step = 258560, mean_episode_return = 0.15909, mean_episode_step = 77.726, total_loss = -149.62, entropy_loss = -8.613, pg_loss = -263.44, baseline_loss = 122.43, learner_queue_size = 32, _tick = 100, _time = 1.7371e+09)
[2025-01-17 21:16:38,954][root][INFO] - Step 261120 @ 511.4 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 869.7, step = 261120, mean_episode_return = 0.1575, mean_episode_step = 57.466, total_loss = -204.89, entropy_loss = -8.5796, pg_loss = -322.94, baseline_loss = 126.63, learner_queue_size = 32, _tick = 101, _time = 1.7371e+09)
[2025-01-17 21:16:43,960][root][INFO] - Step 261120 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 874.7, step = 261120, mean_episode_return = 0.1575, mean_episode_step = 57.466, total_loss = -204.89, entropy_loss = -8.5796, pg_loss = -322.94, baseline_loss = 126.63, learner_queue_size = 32, _tick = 101, _time = 1.7371e+09)
[2025-01-17 21:16:48,966][root][INFO] - Step 263680 @ 511.4 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 879.7, step = 263680, mean_episode_return = 0.17182, mean_episode_step = 81.37, total_loss = -573.98, entropy_loss = -8.5369, pg_loss = -614.18, baseline_loss = 48.738, learner_queue_size = 32, _tick = 102, _time = 1.7371e+09)
[2025-01-17 21:16:53,971][root][INFO] - Step 263680 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 884.7, step = 263680, mean_episode_return = 0.17182, mean_episode_step = 81.37, total_loss = -573.98, entropy_loss = -8.5369, pg_loss = -614.18, baseline_loss = 48.738, learner_queue_size = 32, _tick = 102, _time = 1.7371e+09)
[2025-01-17 21:16:58,976][root][INFO] - Step 266240 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 889.7, step = 266240, mean_episode_return = 0.31793, mean_episode_step = 76.395, total_loss = 622.16, entropy_loss = -8.4956, pg_loss = 511.24, baseline_loss = 119.42, learner_queue_size = 32, _tick = 103, _time = 1.7371e+09)
[2025-01-17 21:17:03,981][root][INFO] - Step 268800 @ 511.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 894.7, step = 268800, mean_episode_return = 0.21359, mean_episode_step = 73.428, total_loss = 832.66, entropy_loss = -8.5154, pg_loss = 675.15, baseline_loss = 166.02, learner_queue_size = 32, _tick = 104, _time = 1.7371e+09)
[2025-01-17 21:17:08,986][root][INFO] - Step 268800 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 899.8, step = 268800, mean_episode_return = 0.21359, mean_episode_step = 73.428, total_loss = 832.66, entropy_loss = -8.5154, pg_loss = 675.15, baseline_loss = 166.02, learner_queue_size = 32, _tick = 104, _time = 1.7371e+09)
[2025-01-17 21:17:13,991][root][INFO] - Step 271360 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 904.8, step = 271360, mean_episode_return = 0.22268, mean_episode_step = 63.052, total_loss = 543.79, entropy_loss = -8.5244, pg_loss = 388.58, baseline_loss = 163.74, learner_queue_size = 32, _tick = 105, _time = 1.7371e+09)
[2025-01-17 21:17:18,996][root][INFO] - Step 273920 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 909.8, step = 273920, mean_episode_return = 0.19562, mean_episode_step = 71.718, total_loss = 1265.9, entropy_loss = -8.5738, pg_loss = 1028.9, baseline_loss = 245.52, learner_queue_size = 32, _tick = 106, _time = 1.7371e+09)
[2025-01-17 21:17:24,002][root][INFO] - Step 273920 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 914.8, step = 273920, mean_episode_return = 0.19562, mean_episode_step = 71.718, total_loss = 1265.9, entropy_loss = -8.5738, pg_loss = 1028.9, baseline_loss = 245.52, learner_queue_size = 32, _tick = 106, _time = 1.7371e+09)
[2025-01-17 21:17:29,008][root][INFO] - Step 276480 @ 511.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 919.8, step = 276480, mean_episode_return = 0.17659, mean_episode_step = 71.395, total_loss = -466.3, entropy_loss = -8.5737, pg_loss = -571.43, baseline_loss = 113.7, learner_queue_size = 32, _tick = 107, _time = 1.7371e+09)
[2025-01-17 21:17:34,013][root][INFO] - Step 276480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 924.8, step = 276480, mean_episode_return = 0.17659, mean_episode_step = 71.395, total_loss = -466.3, entropy_loss = -8.5737, pg_loss = -571.43, baseline_loss = 113.7, learner_queue_size = 32, _tick = 107, _time = 1.7371e+09)
[2025-01-17 21:17:39,018][root][INFO] - Step 279040 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 929.8, step = 279040, mean_episode_return = 0.17617, mean_episode_step = 67.411, total_loss = 376.65, entropy_loss = -8.5931, pg_loss = 200.48, baseline_loss = 184.76, learner_queue_size = 32, _tick = 108, _time = 1.7371e+09)
[2025-01-17 21:17:44,024][root][INFO] - Step 281600 @ 511.4 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 934.8, step = 281600, mean_episode_return = 0.25388, mean_episode_step = 85.21, total_loss = -804.21, entropy_loss = -8.5448, pg_loss = -878.82, baseline_loss = 83.155, learner_queue_size = 32, _tick = 109, _time = 1.7371e+09)
[2025-01-17 21:17:49,029][root][INFO] - Step 281600 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 939.8, step = 281600, mean_episode_return = 0.25388, mean_episode_step = 85.21, total_loss = -804.21, entropy_loss = -8.5448, pg_loss = -878.82, baseline_loss = 83.155, learner_queue_size = 32, _tick = 109, _time = 1.7371e+09)
[2025-01-17 21:17:54,035][root][INFO] - Step 284160 @ 511.4 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 944.8, step = 284160, mean_episode_return = 0.22135, mean_episode_step = 75.165, total_loss = 339.98, entropy_loss = -8.5383, pg_loss = 200.17, baseline_loss = 148.35, learner_queue_size = 32, _tick = 110, _time = 1.7371e+09)
[2025-01-17 21:17:59,041][root][INFO] - Step 286720 @ 511.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 949.8, step = 286720, mean_episode_return = 0.083244, mean_episode_step = 59.584, total_loss = -97.784, entropy_loss = -8.5082, pg_loss = -216.34, baseline_loss = 127.06, learner_queue_size = 32, _tick = 111, _time = 1.7371e+09)
[2025-01-17 21:18:04,046][root][INFO] - Step 286720 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 954.8, step = 286720, mean_episode_return = 0.083244, mean_episode_step = 59.584, total_loss = -97.784, entropy_loss = -8.5082, pg_loss = -216.34, baseline_loss = 127.06, learner_queue_size = 32, _tick = 111, _time = 1.7371e+09)
[2025-01-17 21:18:09,051][root][INFO] - Step 289280 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 959.8, step = 289280, mean_episode_return = 0.24055, mean_episode_step = 70.929, total_loss = 1264.7, entropy_loss = -8.5127, pg_loss = 1018.7, baseline_loss = 254.55, learner_queue_size = 32, _tick = 112, _time = 1.7371e+09)
[2025-01-17 21:18:14,056][root][INFO] - Step 289280 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 964.8, step = 289280, mean_episode_return = 0.24055, mean_episode_step = 70.929, total_loss = 1264.7, entropy_loss = -8.5127, pg_loss = 1018.7, baseline_loss = 254.55, learner_queue_size = 32, _tick = 112, _time = 1.7371e+09)
[2025-01-17 21:18:19,061][root][INFO] - Step 291840 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 969.8, step = 291840, mean_episode_return = 0.16205, mean_episode_step = 70.215, total_loss = 27.763, entropy_loss = -8.5448, pg_loss = -126.1, baseline_loss = 162.41, learner_queue_size = 32, _tick = 113, _time = 1.7371e+09)
[2025-01-17 21:18:24,067][root][INFO] - Step 294400 @ 511.4 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 974.8, step = 294400, mean_episode_return = 0.24672, mean_episode_step = 73.765, total_loss = 39.3, entropy_loss = -8.5788, pg_loss = -107.17, baseline_loss = 155.05, learner_queue_size = 32, _tick = 114, _time = 1.7371e+09)
[2025-01-17 21:18:29,073][root][INFO] - Step 294400 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 979.8, step = 294400, mean_episode_return = 0.24672, mean_episode_step = 73.765, total_loss = 39.3, entropy_loss = -8.5788, pg_loss = -107.17, baseline_loss = 155.05, learner_queue_size = 32, _tick = 114, _time = 1.7371e+09)
[2025-01-17 21:18:34,078][root][INFO] - Step 296960 @ 511.5 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (train_seconds = 984.8, step = 296960, mean_episode_return = 0.14623, mean_episode_step = 68.739, total_loss = 100.3, entropy_loss = -8.5254, pg_loss = -58.294, baseline_loss = 167.11, learner_queue_size = 32, _tick = 115, _time = 1.7371e+09)
[2025-01-17 21:18:39,085][root][INFO] - Step 299520 @ 511.4 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (train_seconds = 989.8, step = 299520, mean_episode_return = 0.22272, mean_episode_step = 59.306, total_loss = 178.43, entropy_loss = -8.4736, pg_loss = 39.295, baseline_loss = 147.61, learner_queue_size = 32, _tick = 116, _time = 1.7371e+09)
[2025-01-17 21:18:44,090][root][INFO] - Step 299520 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 994.9, step = 299520, mean_episode_return = 0.22272, mean_episode_step = 59.306, total_loss = 178.43, entropy_loss = -8.4736, pg_loss = 39.295, baseline_loss = 147.61, learner_queue_size = 32, _tick = 116, _time = 1.7371e+09)
[2025-01-17 21:18:49,096][root][INFO] - Step 302080 @ 511.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 999.9, step = 302080, mean_episode_return = 0.084161, mean_episode_step = 75.934, total_loss = -30.492, entropy_loss = -8.4477, pg_loss = -135.65, baseline_loss = 113.61, learner_queue_size = 32, _tick = 117, _time = 1.7371e+09)
[2025-01-17 21:18:54,101][root][INFO] - Step 302080 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1004.9, step = 302080, mean_episode_return = 0.084161, mean_episode_step = 75.934, total_loss = -30.492, entropy_loss = -8.4477, pg_loss = -135.65, baseline_loss = 113.61, learner_queue_size = 32, _tick = 117, _time = 1.7371e+09)
[2025-01-17 21:18:59,106][root][INFO] - Step 304640 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1009.9, step = 304640, mean_episode_return = 0.26707, mean_episode_step = 89.505, total_loss = 677.97, entropy_loss = -8.424, pg_loss = 530.45, baseline_loss = 155.94, learner_queue_size = 32, _tick = 118, _time = 1.7371e+09)
[2025-01-17 21:19:04,112][root][INFO] - Step 307200 @ 511.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1014.9, step = 307200, mean_episode_return = 0.1678, mean_episode_step = 71.234, total_loss = 81.867, entropy_loss = -8.4412, pg_loss = -25.895, baseline_loss = 116.2, learner_queue_size = 32, _tick = 119, _time = 1.7371e+09)
[2025-01-17 21:19:09,119][root][INFO] - Step 307200 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1019.9, step = 307200, mean_episode_return = 0.1678, mean_episode_step = 71.234, total_loss = 81.867, entropy_loss = -8.4412, pg_loss = -25.895, baseline_loss = 116.2, learner_queue_size = 32, _tick = 119, _time = 1.7371e+09)
[2025-01-17 21:19:14,124][root][INFO] - Step 309760 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1024.9, step = 309760, mean_episode_return = 0.10676, mean_episode_step = 60.146, total_loss = 354.61, entropy_loss = -8.4747, pg_loss = 233.7, baseline_loss = 129.39, learner_queue_size = 32, _tick = 120, _time = 1.7371e+09)
[2025-01-17 21:19:19,132][root][INFO] - Step 312320 @ 511.2 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (train_seconds = 1029.9, step = 312320, mean_episode_return = 0.26713, mean_episode_step = 74.139, total_loss = -21.406, entropy_loss = -8.4735, pg_loss = -136.94, baseline_loss = 124.01, learner_queue_size = 32, _tick = 121, _time = 1.7371e+09)
[2025-01-17 21:19:24,137][root][INFO] - Step 312320 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1034.9, step = 312320, mean_episode_return = 0.26713, mean_episode_step = 74.139, total_loss = -21.406, entropy_loss = -8.4735, pg_loss = -136.94, baseline_loss = 124.01, learner_queue_size = 32, _tick = 121, _time = 1.7371e+09)
[2025-01-17 21:19:29,142][root][INFO] - Step 314880 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 1039.9, step = 314880, mean_episode_return = 0.34348, mean_episode_step = 71.371, total_loss = 574.07, entropy_loss = -8.4402, pg_loss = 393.36, baseline_loss = 189.15, learner_queue_size = 32, _tick = 122, _time = 1.7371e+09)
[2025-01-17 21:19:34,148][root][INFO] - Step 314880 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1044.9, step = 314880, mean_episode_return = 0.34348, mean_episode_step = 71.371, total_loss = 574.07, entropy_loss = -8.4402, pg_loss = 393.36, baseline_loss = 189.15, learner_queue_size = 32, _tick = 122, _time = 1.7371e+09)
[2025-01-17 21:19:39,153][root][INFO] - Step 317440 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1049.9, step = 317440, mean_episode_return = 0.12479, mean_episode_step = 78.741, total_loss = -569.76, entropy_loss = -8.4183, pg_loss = -637.96, baseline_loss = 76.612, learner_queue_size = 32, _tick = 123, _time = 1.7371e+09)
[2025-01-17 21:19:44,159][root][INFO] - Step 320000 @ 511.4 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 1054.9, step = 320000, mean_episode_return = 0.35865, mean_episode_step = 89.834, total_loss = 1750.2, entropy_loss = -8.3972, pg_loss = 1493.6, baseline_loss = 265.01, learner_queue_size = 32, _tick = 124, _time = 1.7371e+09)
[2025-01-17 21:19:49,164][root][INFO] - Step 320000 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1059.9, step = 320000, mean_episode_return = 0.35865, mean_episode_step = 89.834, total_loss = 1750.2, entropy_loss = -8.3972, pg_loss = 1493.6, baseline_loss = 265.01, learner_queue_size = 32, _tick = 124, _time = 1.7371e+09)
[2025-01-17 21:19:54,169][root][INFO] - Step 322560 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 1064.9, step = 322560, mean_episode_return = 0.2444, mean_episode_step = 71.724, total_loss = 186.99, entropy_loss = -8.4157, pg_loss = 5.4397, baseline_loss = 189.96, learner_queue_size = 32, _tick = 125, _time = 1.7371e+09)
[2025-01-17 21:19:59,174][root][INFO] - Step 322560 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1069.9, step = 322560, mean_episode_return = 0.2444, mean_episode_step = 71.724, total_loss = 186.99, entropy_loss = -8.4157, pg_loss = 5.4397, baseline_loss = 189.96, learner_queue_size = 32, _tick = 125, _time = 1.7371e+09)
[2025-01-17 21:20:04,180][root][INFO] - Step 325120 @ 511.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1074.9, step = 325120, mean_episode_return = 0.27659, mean_episode_step = 91.185, total_loss = 31.174, entropy_loss = -8.405, pg_loss = -86.432, baseline_loss = 126.01, learner_queue_size = 32, _tick = 126, _time = 1.7371e+09)
[2025-01-17 21:20:09,186][root][INFO] - Step 327680 @ 511.4 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 1080.0, step = 327680, mean_episode_return = 0.24566, mean_episode_step = 76.957, total_loss = 859.08, entropy_loss = -8.4108, pg_loss = 656.33, baseline_loss = 211.16, learner_queue_size = 32, _tick = 127, _time = 1.7371e+09)
[2025-01-17 21:20:14,191][root][INFO] - Step 327680 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1085.0, step = 327680, mean_episode_return = 0.24566, mean_episode_step = 76.957, total_loss = 859.08, entropy_loss = -8.4108, pg_loss = 656.33, baseline_loss = 211.16, learner_queue_size = 32, _tick = 127, _time = 1.7371e+09)
[2025-01-17 21:20:19,196][root][INFO] - Step 330240 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1090.0, step = 330240, mean_episode_return = 0.2775, mean_episode_step = 80.564, total_loss = 824.79, entropy_loss = -8.393, pg_loss = 638.63, baseline_loss = 194.55, learner_queue_size = 32, _tick = 128, _time = 1.7371e+09)
[2025-01-17 21:20:24,203][root][INFO] - Step 332800 @ 511.3 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (train_seconds = 1095.0, step = 332800, mean_episode_return = 0.17112, mean_episode_step = 68.854, total_loss = -192.69, entropy_loss = -8.4049, pg_loss = -312.14, baseline_loss = 127.86, learner_queue_size = 32, _tick = 129, _time = 1.7371e+09)
[2025-01-17 21:20:29,208][root][INFO] - Step 332800 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1100.0, step = 332800, mean_episode_return = 0.17112, mean_episode_step = 68.854, total_loss = -192.69, entropy_loss = -8.4049, pg_loss = -312.14, baseline_loss = 127.86, learner_queue_size = 32, _tick = 129, _time = 1.7371e+09)
[2025-01-17 21:20:34,213][root][INFO] - Step 335360 @ 511.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 1105.0, step = 335360, mean_episode_return = 0.2195, mean_episode_step = 67.379, total_loss = 306.63, entropy_loss = -8.4211, pg_loss = 120.2, baseline_loss = 194.85, learner_queue_size = 32, _tick = 130, _time = 1.7371e+09)
[2025-01-17 21:20:39,218][root][INFO] - Step 335360 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1110.0, step = 335360, mean_episode_return = 0.2195, mean_episode_step = 67.379, total_loss = 306.63, entropy_loss = -8.4211, pg_loss = 120.2, baseline_loss = 194.85, learner_queue_size = 32, _tick = 130, _time = 1.7371e+09)
[2025-01-17 21:20:44,223][root][INFO] - Step 337920 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1115.0, step = 337920, mean_episode_return = 0.26274, mean_episode_step = 75.135, total_loss = -39.79, entropy_loss = -8.3819, pg_loss = -178.02, baseline_loss = 146.61, learner_queue_size = 32, _tick = 131, _time = 1.7371e+09)
[2025-01-17 21:20:49,230][root][INFO] - Step 340480 @ 511.3 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (train_seconds = 1120.0, step = 340480, mean_episode_return = 0.23184, mean_episode_step = 70.278, total_loss = -196.57, entropy_loss = -8.394, pg_loss = -311.53, baseline_loss = 123.35, learner_queue_size = 32, _tick = 132, _time = 1.7371e+09)
[2025-01-17 21:20:54,237][root][INFO] - Step 340480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1125.0, step = 340480, mean_episode_return = 0.23184, mean_episode_step = 70.278, total_loss = -196.57, entropy_loss = -8.394, pg_loss = -311.53, baseline_loss = 123.35, learner_queue_size = 32, _tick = 132, _time = 1.7371e+09)
[2025-01-17 21:20:59,242][root][INFO] - Step 343040 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1130.0, step = 343040, mean_episode_return = 0.31242, mean_episode_step = 79.592, total_loss = 4.3361, entropy_loss = -8.3568, pg_loss = -131.02, baseline_loss = 143.72, learner_queue_size = 32, _tick = 133, _time = 1.7371e+09)
[2025-01-17 21:21:04,247][root][INFO] - Step 343040 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1135.0, step = 343040, mean_episode_return = 0.31242, mean_episode_step = 79.592, total_loss = 4.3361, entropy_loss = -8.3568, pg_loss = -131.02, baseline_loss = 143.72, learner_queue_size = 32, _tick = 133, _time = 1.7371e+09)
[2025-01-17 21:21:09,252][root][INFO] - Step 345600 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1140.0, step = 345600, mean_episode_return = 0.2839, mean_episode_step = 78.209, total_loss = 475.01, entropy_loss = -8.3552, pg_loss = 297.25, baseline_loss = 186.12, learner_queue_size = 32, _tick = 134, _time = 1.7371e+09)
[2025-01-17 21:21:14,257][root][INFO] - Step 348160 @ 511.5 SPS. Inference batcher size: 24. Learner queue size: 32. Other stats: (train_seconds = 1145.0, step = 348160, mean_episode_return = 0.19718, mean_episode_step = 62.147, total_loss = 9.3615, entropy_loss = -8.3419, pg_loss = -123.15, baseline_loss = 140.85, learner_queue_size = 32, _tick = 135, _time = 1.7371e+09)
[2025-01-17 21:21:19,262][root][INFO] - Step 348160 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1150.0, step = 348160, mean_episode_return = 0.19718, mean_episode_step = 62.147, total_loss = 9.3615, entropy_loss = -8.3419, pg_loss = -123.15, baseline_loss = 140.85, learner_queue_size = 32, _tick = 135, _time = 1.7371e+09)
[2025-01-17 21:21:24,267][root][INFO] - Step 350720 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1155.0, step = 350720, mean_episode_return = 0.1983, mean_episode_step = 73.923, total_loss = 55.851, entropy_loss = -8.2993, pg_loss = -59.302, baseline_loss = 123.45, learner_queue_size = 32, _tick = 136, _time = 1.7371e+09)
[2025-01-17 21:21:29,272][root][INFO] - Step 353280 @ 511.5 SPS. Inference batcher size: 27. Learner queue size: 32. Other stats: (train_seconds = 1160.0, step = 353280, mean_episode_return = 0.16578, mean_episode_step = 80.811, total_loss = -322.06, entropy_loss = -8.2716, pg_loss = -403.74, baseline_loss = 89.953, learner_queue_size = 32, _tick = 137, _time = 1.7371e+09)
[2025-01-17 21:21:34,277][root][INFO] - Step 353280 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1165.0, step = 353280, mean_episode_return = 0.16578, mean_episode_step = 80.811, total_loss = -322.06, entropy_loss = -8.2716, pg_loss = -403.74, baseline_loss = 89.953, learner_queue_size = 32, _tick = 137, _time = 1.7371e+09)
[2025-01-17 21:21:39,283][root][INFO] - Step 355840 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1170.0, step = 355840, mean_episode_return = 0.20636, mean_episode_step = 87.34, total_loss = 189.67, entropy_loss = -8.2628, pg_loss = 89.706, baseline_loss = 108.23, learner_queue_size = 32, _tick = 138, _time = 1.7371e+09)
[2025-01-17 21:21:44,288][root][INFO] - Step 355840 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1175.1, step = 355840, mean_episode_return = 0.20636, mean_episode_step = 87.34, total_loss = 189.67, entropy_loss = -8.2628, pg_loss = 89.706, baseline_loss = 108.23, learner_queue_size = 32, _tick = 138, _time = 1.7371e+09)
[2025-01-17 21:21:49,293][root][INFO] - Step 358400 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1180.1, step = 358400, mean_episode_return = 0.26826, mean_episode_step = 75.661, total_loss = 889.69, entropy_loss = -8.2828, pg_loss = 708.56, baseline_loss = 189.41, learner_queue_size = 32, _tick = 139, _time = 1.7371e+09)
[2025-01-17 21:21:54,298][root][INFO] - Step 360960 @ 511.5 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (train_seconds = 1185.1, step = 360960, mean_episode_return = 0.12237, mean_episode_step = 64.716, total_loss = 1020.0, entropy_loss = -8.2834, pg_loss = 798.46, baseline_loss = 229.84, learner_queue_size = 32, _tick = 140, _time = 1.7371e+09)
[2025-01-17 21:21:59,303][root][INFO] - Step 360960 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1190.1, step = 360960, mean_episode_return = 0.12237, mean_episode_step = 64.716, total_loss = 1020.0, entropy_loss = -8.2834, pg_loss = 798.46, baseline_loss = 229.84, learner_queue_size = 32, _tick = 140, _time = 1.7371e+09)
[2025-01-17 21:22:04,308][root][INFO] - Step 363520 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1195.1, step = 363520, mean_episode_return = 0.27261, mean_episode_step = 74.658, total_loss = 602.22, entropy_loss = -8.2874, pg_loss = 413.64, baseline_loss = 196.87, learner_queue_size = 32, _tick = 141, _time = 1.7371e+09)
[2025-01-17 21:22:09,314][root][INFO] - Step 366080 @ 511.4 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (train_seconds = 1200.1, step = 366080, mean_episode_return = 0.25046, mean_episode_step = 82.321, total_loss = -425.69, entropy_loss = -8.2903, pg_loss = -551.66, baseline_loss = 134.26, learner_queue_size = 32, _tick = 142, _time = 1.7371e+09)
[2025-01-17 21:22:14,319][root][INFO] - Saving checkpoint to /opt/minihack/checkpoint.tar
[2025-01-17 21:22:14,446][root][INFO] - Step 366080 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1205.1, step = 366080, mean_episode_return = 0.25046, mean_episode_step = 82.321, total_loss = -425.69, entropy_loss = -8.2903, pg_loss = -551.66, baseline_loss = 134.26, learner_queue_size = 32, _tick = 142, _time = 1.7371e+09)
[2025-01-17 21:22:19,454][root][INFO] - Step 368640 @ 498.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1210.2, step = 368640, mean_episode_return = 0.3269, mean_episode_step = 82.5, total_loss = 585.89, entropy_loss = -8.2743, pg_loss = 398.65, baseline_loss = 195.52, learner_queue_size = 32, _tick = 143, _time = 1.7371e+09)
[2025-01-17 21:22:24,459][root][INFO] - Step 368640 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1215.2, step = 368640, mean_episode_return = 0.3269, mean_episode_step = 82.5, total_loss = 585.89, entropy_loss = -8.2743, pg_loss = 398.65, baseline_loss = 195.52, learner_queue_size = 32, _tick = 143, _time = 1.7371e+09)
[2025-01-17 21:22:29,464][root][INFO] - Step 371200 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1220.2, step = 371200, mean_episode_return = 0.11573, mean_episode_step = 71.082, total_loss = -368.72, entropy_loss = -8.2872, pg_loss = -481.5, baseline_loss = 121.07, learner_queue_size = 32, _tick = 144, _time = 1.7371e+09)
[2025-01-17 21:22:34,469][root][INFO] - Step 373760 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1225.2, step = 373760, mean_episode_return = 0.29524, mean_episode_step = 76.062, total_loss = -216.39, entropy_loss = -8.2937, pg_loss = -351.21, baseline_loss = 143.11, learner_queue_size = 32, _tick = 145, _time = 1.7371e+09)
[2025-01-17 21:22:39,474][root][INFO] - Step 373760 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1230.2, step = 373760, mean_episode_return = 0.29524, mean_episode_step = 76.062, total_loss = -216.39, entropy_loss = -8.2937, pg_loss = -351.21, baseline_loss = 143.11, learner_queue_size = 32, _tick = 145, _time = 1.7371e+09)
[2025-01-17 21:22:44,479][root][INFO] - Saving checkpoint to /opt/minihack/checkpoint_0.75.tar
[2025-01-17 21:22:44,518][root][INFO] - Step 376320 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1235.2, step = 376320, mean_episode_return = 0.24121, mean_episode_step = 85.862, total_loss = 99.576, entropy_loss = -8.2721, pg_loss = -78.389, baseline_loss = 186.24, learner_queue_size = 32, _tick = 146, _time = 1.7371e+09)
[2025-01-17 21:22:49,524][root][INFO] - Step 378880 @ 507.4 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 1240.3, step = 378880, mean_episode_return = 0.27023, mean_episode_step = 74.825, total_loss = 340.23, entropy_loss = -8.2284, pg_loss = 198.3, baseline_loss = 150.16, learner_queue_size = 32, _tick = 147, _time = 1.7371e+09)
[2025-01-17 21:22:54,530][root][INFO] - Step 378880 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1245.3, step = 378880, mean_episode_return = 0.27023, mean_episode_step = 74.825, total_loss = 340.23, entropy_loss = -8.2284, pg_loss = 198.3, baseline_loss = 150.16, learner_queue_size = 32, _tick = 147, _time = 1.7371e+09)
[2025-01-17 21:22:59,535][root][INFO] - Step 381440 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1250.3, step = 381440, mean_episode_return = 0.28759, mean_episode_step = 72.11, total_loss = 1255.7, entropy_loss = -8.241, pg_loss = 1024.5, baseline_loss = 239.41, learner_queue_size = 32, _tick = 148, _time = 1.7371e+09)
[2025-01-17 21:23:04,540][root][INFO] - Step 384000 @ 511.5 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (train_seconds = 1255.3, step = 384000, mean_episode_return = 0.30803, mean_episode_step = 82.398, total_loss = 569.04, entropy_loss = -8.2401, pg_loss = 390.03, baseline_loss = 187.25, learner_queue_size = 32, _tick = 149, _time = 1.7371e+09)
[2025-01-17 21:23:09,546][root][INFO] - Step 384000 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1260.3, step = 384000, mean_episode_return = 0.30803, mean_episode_step = 82.398, total_loss = 569.04, entropy_loss = -8.2401, pg_loss = 390.03, baseline_loss = 187.25, learner_queue_size = 32, _tick = 149, _time = 1.7371e+09)
[2025-01-17 21:23:14,551][root][INFO] - Step 386560 @ 511.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 1265.3, step = 386560, mean_episode_return = 0.24241, mean_episode_step = 87.063, total_loss = -717.98, entropy_loss = -8.2214, pg_loss = -777.35, baseline_loss = 67.59, learner_queue_size = 32, _tick = 150, _time = 1.7371e+09)
[2025-01-17 21:23:19,556][root][INFO] - Step 386560 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1270.3, step = 386560, mean_episode_return = 0.24241, mean_episode_step = 87.063, total_loss = -717.98, entropy_loss = -8.2214, pg_loss = -777.35, baseline_loss = 67.59, learner_queue_size = 32, _tick = 150, _time = 1.7371e+09)
[2025-01-17 21:23:24,563][root][INFO] - Step 389120 @ 511.3 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (train_seconds = 1275.3, step = 389120, mean_episode_return = 0.31998, mean_episode_step = 72.076, total_loss = 29.404, entropy_loss = -8.2447, pg_loss = -138.41, baseline_loss = 176.06, learner_queue_size = 32, _tick = 151, _time = 1.7371e+09)
[2025-01-17 21:23:29,568][root][INFO] - Step 391680 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1280.3, step = 391680, mean_episode_return = 0.19167, mean_episode_step = 82.279, total_loss = 165.09, entropy_loss = -8.2477, pg_loss = -8.775, baseline_loss = 182.11, learner_queue_size = 32, _tick = 152, _time = 1.7371e+09)
[2025-01-17 21:23:34,573][root][INFO] - Step 391680 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1285.3, step = 391680, mean_episode_return = 0.19167, mean_episode_step = 82.279, total_loss = 165.09, entropy_loss = -8.2477, pg_loss = -8.775, baseline_loss = 182.11, learner_queue_size = 32, _tick = 152, _time = 1.7371e+09)
[2025-01-17 21:23:39,578][root][INFO] - Step 394240 @ 511.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 1290.3, step = 394240, mean_episode_return = 0.25813, mean_episode_step = 82.065, total_loss = -614.31, entropy_loss = -8.2657, pg_loss = -702.08, baseline_loss = 96.037, learner_queue_size = 32, _tick = 153, _time = 1.7371e+09)
[2025-01-17 21:23:44,583][root][INFO] - Step 394240 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1295.3, step = 394240, mean_episode_return = 0.25813, mean_episode_step = 82.065, total_loss = -614.31, entropy_loss = -8.2657, pg_loss = -702.08, baseline_loss = 96.037, learner_queue_size = 32, _tick = 153, _time = 1.7371e+09)
[2025-01-17 21:23:49,588][root][INFO] - Step 396800 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1300.4, step = 396800, mean_episode_return = 0.21363, mean_episode_step = 68.391, total_loss = 1235.3, entropy_loss = -8.2606, pg_loss = 998.54, baseline_loss = 245.0, learner_queue_size = 32, _tick = 154, _time = 1.7371e+09)
[2025-01-17 21:23:54,594][root][INFO] - Step 399360 @ 511.4 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (train_seconds = 1305.4, step = 399360, mean_episode_return = 0.17873, mean_episode_step = 74.963, total_loss = 167.93, entropy_loss = -8.2488, pg_loss = 27.719, baseline_loss = 148.46, learner_queue_size = 32, _tick = 155, _time = 1.7371e+09)
[2025-01-17 21:23:59,599][root][INFO] - Step 399360 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1310.4, step = 399360, mean_episode_return = 0.17873, mean_episode_step = 74.963, total_loss = 167.93, entropy_loss = -8.2488, pg_loss = 27.719, baseline_loss = 148.46, learner_queue_size = 32, _tick = 155, _time = 1.7371e+09)
[2025-01-17 21:24:04,604][root][INFO] - Step 401920 @ 511.5 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (train_seconds = 1315.4, step = 401920, mean_episode_return = 0.28925, mean_episode_step = 90.735, total_loss = -31.78, entropy_loss = -8.2264, pg_loss = -138.34, baseline_loss = 114.79, learner_queue_size = 32, _tick = 156, _time = 1.7371e+09)
[2025-01-17 21:24:09,609][root][INFO] - Step 404480 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 1320.4, step = 404480, mean_episode_return = 0.33126, mean_episode_step = 70.732, total_loss = 1091.1, entropy_loss = -8.2305, pg_loss = 883.48, baseline_loss = 215.83, learner_queue_size = 32, _tick = 157, _time = 1.7371e+09)
[2025-01-17 21:24:14,615][root][INFO] - Step 404480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1325.4, step = 404480, mean_episode_return = 0.33126, mean_episode_step = 70.732, total_loss = 1091.1, entropy_loss = -8.2305, pg_loss = 883.48, baseline_loss = 215.83, learner_queue_size = 32, _tick = 157, _time = 1.7371e+09)
[2025-01-17 21:24:19,620][root][INFO] - Step 407040 @ 511.4 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (train_seconds = 1330.4, step = 407040, mean_episode_return = 0.26863, mean_episode_step = 88.369, total_loss = -446.36, entropy_loss = -8.2275, pg_loss = -543.71, baseline_loss = 105.57, learner_queue_size = 32, _tick = 158, _time = 1.7371e+09)
[2025-01-17 21:24:24,626][root][INFO] - Step 407040 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1335.4, step = 407040, mean_episode_return = 0.26863, mean_episode_step = 88.369, total_loss = -446.36, entropy_loss = -8.2275, pg_loss = -543.71, baseline_loss = 105.57, learner_queue_size = 32, _tick = 158, _time = 1.7371e+09)
[2025-01-17 21:24:29,631][root][INFO] - Step 409600 @ 511.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1340.4, step = 409600, mean_episode_return = 0.24315, mean_episode_step = 84.858, total_loss = -684.88, entropy_loss = -8.2254, pg_loss = -744.54, baseline_loss = 67.885, learner_queue_size = 32, _tick = 159, _time = 1.7371e+09)
[2025-01-17 21:24:34,637][root][INFO] - Step 412160 @ 511.4 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 1345.4, step = 412160, mean_episode_return = 0.23063, mean_episode_step = 68.097, total_loss = 1355.0, entropy_loss = -8.2308, pg_loss = 1142.8, baseline_loss = 220.47, learner_queue_size = 32, _tick = 160, _time = 1.7371e+09)
[2025-01-17 21:24:39,643][root][INFO] - Step 412160 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1350.4, step = 412160, mean_episode_return = 0.23063, mean_episode_step = 68.097, total_loss = 1355.0, entropy_loss = -8.2308, pg_loss = 1142.8, baseline_loss = 220.47, learner_queue_size = 32, _tick = 160, _time = 1.7371e+09)
[2025-01-17 21:24:44,648][root][INFO] - Step 414720 @ 511.5 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 1355.4, step = 414720, mean_episode_return = 0.23447, mean_episode_step = 66.987, total_loss = -337.49, entropy_loss = -8.2483, pg_loss = -462.53, baseline_loss = 133.29, learner_queue_size = 32, _tick = 161, _time = 1.7371e+09)
[2025-01-17 21:24:49,653][root][INFO] - Step 417280 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1360.4, step = 417280, mean_episode_return = 0.42655, mean_episode_step = 89.995, total_loss = 626.33, entropy_loss = -8.2175, pg_loss = 481.91, baseline_loss = 152.64, learner_queue_size = 32, _tick = 162, _time = 1.7371e+09)
[2025-01-17 21:24:54,658][root][INFO] - Step 417280 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1365.4, step = 417280, mean_episode_return = 0.42655, mean_episode_step = 89.995, total_loss = 626.33, entropy_loss = -8.2175, pg_loss = 481.91, baseline_loss = 152.64, learner_queue_size = 32, _tick = 162, _time = 1.7371e+09)
[2025-01-17 21:24:59,663][root][INFO] - Step 419840 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1370.4, step = 419840, mean_episode_return = 0.23868, mean_episode_step = 82.532, total_loss = -78.718, entropy_loss = -8.2214, pg_loss = -172.52, baseline_loss = 102.02, learner_queue_size = 32, _tick = 163, _time = 1.7371e+09)
[2025-01-17 21:25:04,668][root][INFO] - Step 419840 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1375.4, step = 419840, mean_episode_return = 0.23868, mean_episode_step = 82.532, total_loss = -78.718, entropy_loss = -8.2214, pg_loss = -172.52, baseline_loss = 102.02, learner_queue_size = 32, _tick = 163, _time = 1.7371e+09)
[2025-01-17 21:25:09,673][root][INFO] - Step 422400 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1380.4, step = 422400, mean_episode_return = 0.23681, mean_episode_step = 78.746, total_loss = 878.28, entropy_loss = -8.2224, pg_loss = 678.44, baseline_loss = 208.07, learner_queue_size = 32, _tick = 164, _time = 1.7371e+09)
[2025-01-17 21:25:14,679][root][INFO] - Step 424960 @ 511.4 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 1385.4, step = 424960, mean_episode_return = 0.23373, mean_episode_step = 79.652, total_loss = 375.24, entropy_loss = -8.234, pg_loss = 192.18, baseline_loss = 191.29, learner_queue_size = 32, _tick = 165, _time = 1.7371e+09)
[2025-01-17 21:25:19,685][root][INFO] - Step 424960 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1390.5, step = 424960, mean_episode_return = 0.23373, mean_episode_step = 79.652, total_loss = 375.24, entropy_loss = -8.234, pg_loss = 192.18, baseline_loss = 191.29, learner_queue_size = 32, _tick = 165, _time = 1.7371e+09)
[2025-01-17 21:25:24,690][root][INFO] - Step 427520 @ 511.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (train_seconds = 1395.5, step = 427520, mean_episode_return = 0.14941, mean_episode_step = 68.203, total_loss = -83.39, entropy_loss = -8.2185, pg_loss = -209.16, baseline_loss = 133.99, learner_queue_size = 32, _tick = 166, _time = 1.7371e+09)
[2025-01-17 21:25:29,695][root][INFO] - Step 427520 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1400.5, step = 427520, mean_episode_return = 0.14941, mean_episode_step = 68.203, total_loss = -83.39, entropy_loss = -8.2185, pg_loss = -209.16, baseline_loss = 133.99, learner_queue_size = 32, _tick = 166, _time = 1.7371e+09)
[2025-01-17 21:25:34,700][root][INFO] - Step 430080 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1405.5, step = 430080, mean_episode_return = 0.35025, mean_episode_step = 72.529, total_loss = 303.42, entropy_loss = -8.2135, pg_loss = 156.44, baseline_loss = 155.2, learner_queue_size = 32, _tick = 167, _time = 1.7371e+09)
[2025-01-17 21:25:39,705][root][INFO] - Step 432640 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 1410.5, step = 432640, mean_episode_return = 0.17133, mean_episode_step = 72.018, total_loss = -786.26, entropy_loss = -8.2379, pg_loss = -895.68, baseline_loss = 117.66, learner_queue_size = 32, _tick = 168, _time = 1.7371e+09)
[2025-01-17 21:25:44,710][root][INFO] - Step 432640 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1415.5, step = 432640, mean_episode_return = 0.17133, mean_episode_step = 72.018, total_loss = -786.26, entropy_loss = -8.2379, pg_loss = -895.68, baseline_loss = 117.66, learner_queue_size = 32, _tick = 168, _time = 1.7371e+09)
[2025-01-17 21:25:49,715][root][INFO] - Step 435200 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1420.5, step = 435200, mean_episode_return = 0.3132, mean_episode_step = 80.074, total_loss = -266.94, entropy_loss = -8.2167, pg_loss = -379.27, baseline_loss = 120.54, learner_queue_size = 32, _tick = 169, _time = 1.7371e+09)
[2025-01-17 21:25:54,720][root][INFO] - Step 437760 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1425.5, step = 437760, mean_episode_return = 0.26434, mean_episode_step = 72.77, total_loss = 785.49, entropy_loss = -8.2154, pg_loss = 581.13, baseline_loss = 212.57, learner_queue_size = 32, _tick = 170, _time = 1.7371e+09)
[2025-01-17 21:25:59,727][root][INFO] - Step 437760 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1430.5, step = 437760, mean_episode_return = 0.26434, mean_episode_step = 72.77, total_loss = 785.49, entropy_loss = -8.2154, pg_loss = 581.13, baseline_loss = 212.57, learner_queue_size = 32, _tick = 170, _time = 1.7371e+09)
[2025-01-17 21:26:04,732][root][INFO] - Step 440320 @ 511.5 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (train_seconds = 1435.5, step = 440320, mean_episode_return = 0.20992, mean_episode_step = 81.259, total_loss = 607.41, entropy_loss = -8.2106, pg_loss = 436.48, baseline_loss = 179.14, learner_queue_size = 32, _tick = 171, _time = 1.7371e+09)
[2025-01-17 21:26:09,737][root][INFO] - Step 440320 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1440.5, step = 440320, mean_episode_return = 0.20992, mean_episode_step = 81.259, total_loss = 607.41, entropy_loss = -8.2106, pg_loss = 436.48, baseline_loss = 179.14, learner_queue_size = 32, _tick = 171, _time = 1.7371e+09)
[2025-01-17 21:26:14,742][root][INFO] - Step 442880 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1445.5, step = 442880, mean_episode_return = 0.29715, mean_episode_step = 80.487, total_loss = -5.5288, entropy_loss = -8.2149, pg_loss = -158.89, baseline_loss = 161.57, learner_queue_size = 32, _tick = 172, _time = 1.7371e+09)
[2025-01-17 21:26:19,748][root][INFO] - Step 445440 @ 511.4 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (train_seconds = 1450.5, step = 445440, mean_episode_return = 0.19907, mean_episode_step = 64.849, total_loss = 792.28, entropy_loss = -8.2381, pg_loss = 567.29, baseline_loss = 233.23, learner_queue_size = 32, _tick = 173, _time = 1.7371e+09)
[2025-01-17 21:26:24,753][root][INFO] - Step 445440 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1455.5, step = 445440, mean_episode_return = 0.19907, mean_episode_step = 64.849, total_loss = 792.28, entropy_loss = -8.2381, pg_loss = 567.29, baseline_loss = 233.23, learner_queue_size = 32, _tick = 173, _time = 1.7371e+09)
[2025-01-17 21:26:29,759][root][INFO] - Step 448000 @ 511.4 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 1460.5, step = 448000, mean_episode_return = 0.269, mean_episode_step = 80.782, total_loss = -39.296, entropy_loss = -8.2195, pg_loss = -154.16, baseline_loss = 123.09, learner_queue_size = 32, _tick = 174, _time = 1.7371e+09)
[2025-01-17 21:26:34,764][root][INFO] - Step 448000 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1465.5, step = 448000, mean_episode_return = 0.269, mean_episode_step = 80.782, total_loss = -39.296, entropy_loss = -8.2195, pg_loss = -154.16, baseline_loss = 123.09, learner_queue_size = 32, _tick = 174, _time = 1.7371e+09)
[2025-01-17 21:26:39,769][root][INFO] - Step 450560 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1470.5, step = 450560, mean_episode_return = 0.27731, mean_episode_step = 86.321, total_loss = -116.9, entropy_loss = -8.2322, pg_loss = -242.58, baseline_loss = 133.91, learner_queue_size = 32, _tick = 175, _time = 1.7371e+09)
[2025-01-17 21:26:44,774][root][INFO] - Step 453120 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1475.5, step = 453120, mean_episode_return = 0.20305, mean_episode_step = 77.017, total_loss = 69.373, entropy_loss = -8.2496, pg_loss = -99.888, baseline_loss = 177.51, learner_queue_size = 32, _tick = 176, _time = 1.7371e+09)
[2025-01-17 21:26:49,780][root][INFO] - Step 453120 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1480.5, step = 453120, mean_episode_return = 0.20305, mean_episode_step = 77.017, total_loss = 69.373, entropy_loss = -8.2496, pg_loss = -99.888, baseline_loss = 177.51, learner_queue_size = 32, _tick = 176, _time = 1.7371e+09)
[2025-01-17 21:26:54,785][root][INFO] - Step 455680 @ 511.5 SPS. Inference batcher size: 4. Learner queue size: 32. Other stats: (train_seconds = 1485.6, step = 455680, mean_episode_return = 0.22498, mean_episode_step = 59.802, total_loss = 102.78, entropy_loss = -8.2401, pg_loss = -59.801, baseline_loss = 170.82, learner_queue_size = 32, _tick = 177, _time = 1.7371e+09)
[2025-01-17 21:26:59,790][root][INFO] - Step 458240 @ 511.5 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 1490.6, step = 458240, mean_episode_return = 0.28137, mean_episode_step = 69.042, total_loss = 920.26, entropy_loss = -8.2181, pg_loss = 702.93, baseline_loss = 225.55, learner_queue_size = 32, _tick = 178, _time = 1.7371e+09)
[2025-01-17 21:27:04,796][root][INFO] - Step 458240 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1495.6, step = 458240, mean_episode_return = 0.28137, mean_episode_step = 69.042, total_loss = 920.26, entropy_loss = -8.2181, pg_loss = 702.93, baseline_loss = 225.55, learner_queue_size = 32, _tick = 178, _time = 1.7371e+09)
[2025-01-17 21:27:09,801][root][INFO] - Step 460800 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1500.6, step = 460800, mean_episode_return = 0.4473, mean_episode_step = 76.661, total_loss = 1020.2, entropy_loss = -8.2225, pg_loss = 776.7, baseline_loss = 251.68, learner_queue_size = 32, _tick = 179, _time = 1.7371e+09)
[2025-01-17 21:27:14,806][root][INFO] - Step 460800 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1505.6, step = 460800, mean_episode_return = 0.4473, mean_episode_step = 76.661, total_loss = 1020.2, entropy_loss = -8.2225, pg_loss = 776.7, baseline_loss = 251.68, learner_queue_size = 32, _tick = 179, _time = 1.7371e+09)
[2025-01-17 21:27:19,811][root][INFO] - Step 463360 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1510.6, step = 463360, mean_episode_return = 0.27814, mean_episode_step = 73.229, total_loss = 1299.5, entropy_loss = -8.2137, pg_loss = 1061.0, baseline_loss = 246.72, learner_queue_size = 32, _tick = 180, _time = 1.7371e+09)
[2025-01-17 21:27:24,817][root][INFO] - Step 465920 @ 511.4 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 1515.6, step = 465920, mean_episode_return = 0.1619, mean_episode_step = 87.236, total_loss = 279.72, entropy_loss = -8.2156, pg_loss = 120.77, baseline_loss = 167.16, learner_queue_size = 32, _tick = 181, _time = 1.7371e+09)
[2025-01-17 21:27:29,822][root][INFO] - Step 465920 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1520.6, step = 465920, mean_episode_return = 0.1619, mean_episode_step = 87.236, total_loss = 279.72, entropy_loss = -8.2156, pg_loss = 120.77, baseline_loss = 167.16, learner_queue_size = 32, _tick = 181, _time = 1.7371e+09)
[2025-01-17 21:27:34,829][root][INFO] - Step 468480 @ 511.3 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 1525.6, step = 468480, mean_episode_return = 0.18759, mean_episode_step = 90.03, total_loss = 78.245, entropy_loss = -8.2197, pg_loss = -113.96, baseline_loss = 200.42, learner_queue_size = 32, _tick = 182, _time = 1.7371e+09)
[2025-01-17 21:27:39,834][root][INFO] - Step 471040 @ 511.5 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (train_seconds = 1530.6, step = 471040, mean_episode_return = 0.20338, mean_episode_step = 75.375, total_loss = -54.619, entropy_loss = -8.2228, pg_loss = -197.95, baseline_loss = 151.55, learner_queue_size = 32, _tick = 183, _time = 1.7371e+09)
[2025-01-17 21:27:44,839][root][INFO] - Step 471040 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1535.6, step = 471040, mean_episode_return = 0.20338, mean_episode_step = 75.375, total_loss = -54.619, entropy_loss = -8.2228, pg_loss = -197.95, baseline_loss = 151.55, learner_queue_size = 32, _tick = 183, _time = 1.7371e+09)
[2025-01-17 21:27:49,846][root][INFO] - Step 473600 @ 511.3 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (train_seconds = 1540.6, step = 473600, mean_episode_return = 0.28017, mean_episode_step = 70.85, total_loss = -150.38, entropy_loss = -8.2224, pg_loss = -302.32, baseline_loss = 160.16, learner_queue_size = 32, _tick = 184, _time = 1.7371e+09)
[2025-01-17 21:27:54,851][root][INFO] - Step 473600 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1545.6, step = 473600, mean_episode_return = 0.28017, mean_episode_step = 70.85, total_loss = -150.38, entropy_loss = -8.2224, pg_loss = -302.32, baseline_loss = 160.16, learner_queue_size = 32, _tick = 184, _time = 1.7371e+09)
[2025-01-17 21:27:59,856][root][INFO] - Step 476160 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1550.6, step = 476160, mean_episode_return = 0.25091, mean_episode_step = 92.664, total_loss = -100.09, entropy_loss = -8.1975, pg_loss = -218.12, baseline_loss = 126.23, learner_queue_size = 32, _tick = 185, _time = 1.7371e+09)
[2025-01-17 21:28:04,861][root][INFO] - Step 476160 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1555.6, step = 476160, mean_episode_return = 0.25091, mean_episode_step = 92.664, total_loss = -100.09, entropy_loss = -8.1975, pg_loss = -218.12, baseline_loss = 126.23, learner_queue_size = 32, _tick = 185, _time = 1.7371e+09)
[2025-01-17 21:28:09,866][root][INFO] - Step 478720 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1560.6, step = 478720, mean_episode_return = 0.27306, mean_episode_step = 81.881, total_loss = 114.84, entropy_loss = -8.2139, pg_loss = -57.407, baseline_loss = 180.47, learner_queue_size = 32, _tick = 186, _time = 1.7371e+09)
[2025-01-17 21:28:14,871][root][INFO] - Step 481280 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1565.6, step = 481280, mean_episode_return = 0.22748, mean_episode_step = 85.871, total_loss = 122.29, entropy_loss = -8.1945, pg_loss = 14.111, baseline_loss = 116.37, learner_queue_size = 32, _tick = 187, _time = 1.7371e+09)
[2025-01-17 21:28:19,876][root][INFO] - Step 481280 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1570.6, step = 481280, mean_episode_return = 0.22748, mean_episode_step = 85.871, total_loss = 122.29, entropy_loss = -8.1945, pg_loss = 14.111, baseline_loss = 116.37, learner_queue_size = 32, _tick = 187, _time = 1.7371e+09)
[2025-01-17 21:28:24,882][root][INFO] - Step 483840 @ 511.4 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (train_seconds = 1575.6, step = 483840, mean_episode_return = 0.17803, mean_episode_step = 64.07, total_loss = 174.06, entropy_loss = -8.2189, pg_loss = 6.2348, baseline_loss = 176.04, learner_queue_size = 32, _tick = 188, _time = 1.7371e+09)
[2025-01-17 21:28:29,887][root][INFO] - Step 486400 @ 511.5 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 1580.7, step = 486400, mean_episode_return = 0.24632, mean_episode_step = 78.745, total_loss = -265.36, entropy_loss = -8.2268, pg_loss = -414.36, baseline_loss = 157.23, learner_queue_size = 32, _tick = 189, _time = 1.7371e+09)
[2025-01-17 21:28:34,892][root][INFO] - Step 486400 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1585.7, step = 486400, mean_episode_return = 0.24632, mean_episode_step = 78.745, total_loss = -265.36, entropy_loss = -8.2268, pg_loss = -414.36, baseline_loss = 157.23, learner_queue_size = 32, _tick = 189, _time = 1.7371e+09)
[2025-01-17 21:28:39,897][root][INFO] - Step 488960 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1590.7, step = 488960, mean_episode_return = 0.24769, mean_episode_step = 77.666, total_loss = -786.15, entropy_loss = -8.2221, pg_loss = -858.86, baseline_loss = 80.929, learner_queue_size = 32, _tick = 190, _time = 1.7371e+09)
[2025-01-17 21:28:44,902][root][INFO] - Step 488960 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1595.7, step = 488960, mean_episode_return = 0.24769, mean_episode_step = 77.666, total_loss = -786.15, entropy_loss = -8.2221, pg_loss = -858.86, baseline_loss = 80.929, learner_queue_size = 32, _tick = 190, _time = 1.7371e+09)
[2025-01-17 21:28:49,907][root][INFO] - Step 491520 @ 511.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1600.7, step = 491520, mean_episode_return = 0.14121, mean_episode_step = 75.855, total_loss = 253.73, entropy_loss = -8.2164, pg_loss = 107.3, baseline_loss = 154.65, learner_queue_size = 32, _tick = 191, _time = 1.7371e+09)
[2025-01-17 21:28:54,915][root][INFO] - Step 494080 @ 511.3 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 1605.7, step = 494080, mean_episode_return = 0.26574, mean_episode_step = 70.055, total_loss = -539.72, entropy_loss = -8.2181, pg_loss = -644.16, baseline_loss = 112.67, learner_queue_size = 32, _tick = 192, _time = 1.7371e+09)
[2025-01-17 21:28:59,920][root][INFO] - Step 494080 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1610.7, step = 494080, mean_episode_return = 0.26574, mean_episode_step = 70.055, total_loss = -539.72, entropy_loss = -8.2181, pg_loss = -644.16, baseline_loss = 112.67, learner_queue_size = 32, _tick = 192, _time = 1.7371e+09)
[2025-01-17 21:29:04,925][root][INFO] - Step 496640 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1615.7, step = 496640, mean_episode_return = 0.29074, mean_episode_step = 81.207, total_loss = -395.96, entropy_loss = -8.2155, pg_loss = -508.92, baseline_loss = 121.18, learner_queue_size = 32, _tick = 193, _time = 1.7371e+09)
[2025-01-17 21:29:09,930][root][INFO] - Step 499200 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1620.7, step = 499200, mean_episode_return = 0.19026, mean_episode_step = 81.141, total_loss = -329.36, entropy_loss = -8.2172, pg_loss = -451.52, baseline_loss = 130.37, learner_queue_size = 32, _tick = 194, _time = 1.7371e+09)
[2025-01-17 21:29:14,935][root][INFO] - Step 499200 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1625.7, step = 499200, mean_episode_return = 0.19026, mean_episode_step = 81.141, total_loss = -329.36, entropy_loss = -8.2172, pg_loss = -451.52, baseline_loss = 130.37, learner_queue_size = 32, _tick = 194, _time = 1.7371e+09)
[2025-01-17 21:29:19,941][root][INFO] - Step 501760 @ 511.3 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1630.7, step = 501760, mean_episode_return = 0.24819, mean_episode_step = 64.341, total_loss = 996.87, entropy_loss = -8.2233, pg_loss = 739.76, baseline_loss = 265.34, learner_queue_size = 32, _tick = 195, _time = 1.7371e+09)
[2025-01-17 21:29:19,942][root][INFO] - Learning finished after 501760 steps.
[2025-01-17 21:29:19,942][root][INFO] - Saving checkpoint to /opt/minihack/checkpoint.tar
[2025-01-17 21:33:01,727][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,727][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,759][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,736][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,763][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,758][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,750][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,733][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,791][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,777][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,789][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,759][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,788][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,781][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,756][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,831][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,729][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,728][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,769][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,730][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,727][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,827][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,730][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,742][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,812][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,839][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,862][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,802][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,861][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,800][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,807][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,792][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,766][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,836][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,768][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,744][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,822][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,768][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,727][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,843][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,772][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,772][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,771][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,799][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,867][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,797][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,741][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,792][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,880][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,791][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,667][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,813][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,732][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,728][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,805][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,788][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,878][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,869][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,791][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,799][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,588][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,851][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,818][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,867][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,884][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,787][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,805][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,871][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,908][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,840][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,863][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,907][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,120][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,808][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,894][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,132][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,817][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,896][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,846][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,913][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,867][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,828][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,869][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,932][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,901][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,906][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,913][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,895][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,074][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,910][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,916][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,863][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,969][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,898][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,924][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,913][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,066][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,845][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,115][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,962][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,913][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,069][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,914][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,958][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,958][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,911][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,969][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,801][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,914][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,125][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,869][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,156][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,126][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,906][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,124][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,148][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,149][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,882][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,124][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,131][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,874][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,912][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,129][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,125][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,132][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,801][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,128][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,154][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,096][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,071][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,152][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,155][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,129][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,138][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,154][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,129][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,153][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,149][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,149][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,141][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,056][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,208][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,152][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,168][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,153][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,830][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,158][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,136][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,171][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,159][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,128][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,164][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,276][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,142][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,225][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,199][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,212][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,122][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,169][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,336][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,156][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,171][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,208][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,150][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,154][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,122][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,155][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,142][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,210][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,788][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,720][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,154][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,323][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,263][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,155][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,150][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,786][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,783][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,336][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,187][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,191][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,156][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,878][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,787][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,169][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,158][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,785][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:01,857][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,282][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,900][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,849][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,803][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,873][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,279][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,892][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,156][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,780][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,939][nle.env.base][INFO] - Not saving any NLE data.
[2025-01-17 21:33:02,963][nle.env.base][INFO] - Not saving any NLE data.
