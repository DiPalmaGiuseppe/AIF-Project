[[36m2025-01-18 10:56:19,737[0m][[34mroot[0m][[32mINFO[0m] - name: null
wandb: false
project: minihack
entity: entity_name
group: default
mock: false
single_ttyrec: true
num_seeds: 0
write_profiler_trace: false
relative_reward: false
fn_penalty_step: constant
penalty_time: 0.0
penalty_step: -0.001
reward_lose: 0
reward_win: 1
save_tty: false
character: null
mode: train
env: MiniHack-Combat-Skill-v0
obs_keys: glyphs,chars,colors,specials,blstats,message
num_actors: 256
total_steps: 500000
batch_size: 32
unroll_length: 80
num_learner_threads: 1
num_inference_threads: 1
disable_cuda: false
learner_device: cuda:0
actor_device: cuda:0
max_learner_queue_size: null
model: baseline
use_lstm: true
hidden_dim: 256
embedding_dim: 64
glyph_type: all_cat
equalize_input_dim: false
equalize_factor: 2
layers: 5
crop_model: cnn
crop_dim: 9
use_index_select: true
entropy_cost: 0.001
baseline_cost: 0.5
discounting: 0.999
reward_clipping: none
normalize_reward: true
learning_rate: 0.0002
grad_norm_clipping: 40
alpha: 0.99
momentum: 0
epsilon: 1.0e-06
state_counter: none
no_extrinsic: false
int:
  twoheaded: true
  input: full
  intrinsic_weight: 0.1
  discounting: 0.99
  baseline_cost: 0.5
  episodic: true
  reward_clipping: none
  normalize_reward: true
ride:
  count_norm: true
  forward_cost: 1
  inverse_cost: 0.1
  hidden_dim: 128
rnd:
  forward_cost: 0.01
msg:
  model: none
  hidden_dim: 64
  embedding_dim: 32
[0m
[[36m2025-01-18 10:56:19,788[0m][[34mroot[0m][[32mINFO[0m] - Symlinked log directory: /opt/minihack/latest[0m
[[36m2025-01-18 10:56:19,789[0m][[34mroot[0m][[32mINFO[0m] - Found archive directory: /opt/minihack/archives[0m
[[36m2025-01-18 10:56:19,794[0m][[34mroot[0m][[32mINFO[0m] - Logging results to /opt/minihack[0m
[[36m2025-01-18 10:56:19,852[0m][[34mpalaas/out[0m][[32mINFO[0m] - Found log directory: /opt/minihack[0m
[[36m2025-01-18 10:56:19,852[0m][[34mpalaas/out[0m][[32mINFO[0m] - Saving arguments to /opt/minihack/meta.json[0m
[[36m2025-01-18 10:56:19,853[0m][[34mpalaas/out[0m][[32mINFO[0m] - Saving messages to /opt/minihack/out.log[0m
[[36m2025-01-18 10:56:19,853[0m][[34mpalaas/out[0m][[32mINFO[0m] - Saving logs data to /opt/minihack/logs.csv[0m
[[36m2025-01-18 10:56:19,853[0m][[34mpalaas/out[0m][[32mINFO[0m] - Saving logs' fields to /opt/minihack/fields.csv[0m
[[36m2025-01-18 10:56:19,855[0m][[34mroot[0m][[32mINFO[0m] - Not using CUDA.[0m
[[36m2025-01-18 10:56:19,865[0m][[34mroot[0m][[32mINFO[0m] - Using model baseline[0m
[[36m2025-01-18 10:56:19,866[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:19,935[0m][[34mroot[0m][[32mINFO[0m] - Number of model parameters: 4264078[0m
[[36m2025-01-18 10:56:19,936[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,004[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,004[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,005[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
First Environment waiting for connection to unix:/tmp/poly..opt.minihack.0 ... connection established.
[[36m2025-01-18 10:56:20,006[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,007[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,007[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,008[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,008[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,007[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,010[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,010[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,011[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,011[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,014[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,014[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,014[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,016[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,008[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,016[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,017[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,013[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,018[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,014[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,017[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,025[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,025[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,011[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,017[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,017[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,029[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,029[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,023[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,033[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,022[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,014[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,037[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,022[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,039[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,022[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,048[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,052[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,028[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:20,073[0m][[34mnle.env.base[0m][[32mINFO[0m] - Not saving any NLE data.[0m
[[36m2025-01-18 10:56:25,005[0m][[34mroot[0m][[32mINFO[0m] - Step 0 @ 0.0 SPS. Inference batcher size: 1. Learner queue size: 11. Other stats: (train_seconds = 5.0)[0m
[[36m2025-01-18 10:56:30,010[0m][[34mroot[0m][[32mINFO[0m] - Step 0 @ 0.0 SPS. Inference batcher size: 2. Learner queue size: 3. Other stats: (train_seconds = 10.0)[0m
[[36m2025-01-18 10:56:35,015[0m][[34mroot[0m][[32mINFO[0m] - Step 0 @ 0.0 SPS. Inference batcher size: 33. Learner queue size: 17. Other stats: (train_seconds = 15.0)[0m
[[36m2025-01-18 10:56:40,021[0m][[34mroot[0m][[32mINFO[0m] - Step 0 @ 0.0 SPS. Inference batcher size: 101. Learner queue size: 18. Other stats: (train_seconds = 20.0)[0m
[[36m2025-01-18 10:56:45,026[0m][[34mroot[0m][[32mINFO[0m] - Step 0 @ 0.0 SPS. Inference batcher size: 89. Learner queue size: 20. Other stats: (train_seconds = 25.0)[0m
[[36m2025-01-18 10:56:50,225[0m][[34mroot[0m][[32mINFO[0m] - Step 0 @ 0.0 SPS. Inference batcher size: 53. Learner queue size: 20. Other stats: (train_seconds = 30.2)[0m
[[36m2025-01-18 10:56:55,232[0m][[34mroot[0m][[32mINFO[0m] - Step 0 @ 0.0 SPS. Inference batcher size: 39. Learner queue size: 20. Other stats: (train_seconds = 35.2)[0m
[[36m2025-01-18 10:57:00,238[0m][[34mroot[0m][[32mINFO[0m] - Step 0 @ 0.0 SPS. Inference batcher size: 36. Learner queue size: 20. Other stats: (train_seconds = 40.2)[0m
[[36m2025-01-18 10:57:05,242[0m][[34mroot[0m][[32mINFO[0m] - Step 0 @ 0.0 SPS. Inference batcher size: 87. Learner queue size: 20. Other stats: (train_seconds = 45.2)[0m
[[36m2025-01-18 10:57:10,247[0m][[34mroot[0m][[32mINFO[0m] - Step 0 @ 0.0 SPS. Inference batcher size: 111. Learner queue size: 21. Other stats: (train_seconds = 50.2)[0m
[[36m2025-01-18 10:57:12,341[0m][[34mpalaas/out[0m][[32mINFO[0m] - Updated log fields: ['_tick', '_time', 'train_seconds', 'step', 'mean_episode_return', 'mean_episode_step', 'total_loss', 'entropy_loss', 'pg_loss', 'baseline_loss', 'learner_queue_size'][0m
[[36m2025-01-18 10:57:15,252[0m][[34mroot[0m][[32mINFO[0m] - Saving checkpoint to /opt/minihack/checkpoint_0.tar[0m
[[36m2025-01-18 10:57:15,292[0m][[34mroot[0m][[32mINFO[0m] - Step 2560 @ 511.5 SPS. Inference batcher size: 101. Learner queue size: 24. Other stats: (train_seconds = 55.3, step = 2560, mean_episode_return = -0.10462, mean_episode_step = 71.63, total_loss = -2108.4, entropy_loss = -11.372, pg_loss = -2129.1, baseline_loss = 32.049, learner_queue_size = 22, _tick = 0, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:57:20,297[0m][[34mroot[0m][[32mINFO[0m] - Step 2560 @ 0.0 SPS. Inference batcher size: 90. Learner queue size: 25. Other stats: (train_seconds = 60.3, step = 2560, mean_episode_return = -0.10462, mean_episode_step = 71.63, total_loss = -2108.4, entropy_loss = -11.372, pg_loss = -2129.1, baseline_loss = 32.049, learner_queue_size = 22, _tick = 0, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:57:25,303[0m][[34mroot[0m][[32mINFO[0m] - Step 2560 @ 0.0 SPS. Inference batcher size: 131. Learner queue size: 29. Other stats: (train_seconds = 65.3, step = 2560, mean_episode_return = -0.10462, mean_episode_step = 71.63, total_loss = -2108.4, entropy_loss = -11.372, pg_loss = -2129.1, baseline_loss = 32.049, learner_queue_size = 22, _tick = 0, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:57:30,310[0m][[34mroot[0m][[32mINFO[0m] - Step 2560 @ 0.0 SPS. Inference batcher size: 49. Learner queue size: 32. Other stats: (train_seconds = 70.3, step = 2560, mean_episode_return = -0.10462, mean_episode_step = 71.63, total_loss = -2108.4, entropy_loss = -11.372, pg_loss = -2129.1, baseline_loss = 32.049, learner_queue_size = 22, _tick = 0, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:57:35,315[0m][[34mroot[0m][[32mINFO[0m] - Step 2560 @ 0.0 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 75.3, step = 2560, mean_episode_return = -0.10462, mean_episode_step = 71.63, total_loss = -2108.4, entropy_loss = -11.372, pg_loss = -2129.1, baseline_loss = 32.049, learner_queue_size = 22, _tick = 0, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:57:40,320[0m][[34mroot[0m][[32mINFO[0m] - Step 2560 @ 0.0 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (train_seconds = 80.3, step = 2560, mean_episode_return = -0.10462, mean_episode_step = 71.63, total_loss = -2108.4, entropy_loss = -11.372, pg_loss = -2129.1, baseline_loss = 32.049, learner_queue_size = 22, _tick = 0, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:57:45,323[0m][[34mroot[0m][[32mINFO[0m] - Step 2560 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 85.3, step = 2560, mean_episode_return = -0.10462, mean_episode_step = 71.63, total_loss = -2108.4, entropy_loss = -11.372, pg_loss = -2129.1, baseline_loss = 32.049, learner_queue_size = 22, _tick = 0, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:57:50,328[0m][[34mroot[0m][[32mINFO[0m] - Step 5120 @ 511.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 90.3, step = 5120, mean_episode_return = -0.030091, mean_episode_step = 55.37, total_loss = 2379.6, entropy_loss = -11.355, pg_loss = 1964.9, baseline_loss = 426.05, learner_queue_size = 32, _tick = 1, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:57:55,334[0m][[34mroot[0m][[32mINFO[0m] - Step 5120 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 95.3, step = 5120, mean_episode_return = -0.030091, mean_episode_step = 55.37, total_loss = 2379.6, entropy_loss = -11.355, pg_loss = 1964.9, baseline_loss = 426.05, learner_queue_size = 32, _tick = 1, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:58:00,339[0m][[34mroot[0m][[32mINFO[0m] - Step 5120 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 100.3, step = 5120, mean_episode_return = -0.030091, mean_episode_step = 55.37, total_loss = 2379.6, entropy_loss = -11.355, pg_loss = 1964.9, baseline_loss = 426.05, learner_queue_size = 32, _tick = 1, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:58:05,344[0m][[34mroot[0m][[32mINFO[0m] - Step 7680 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 105.3, step = 7680, mean_episode_return = 0.010381, mean_episode_step = 31.223, total_loss = 1544.1, entropy_loss = -11.369, pg_loss = 1303.0, baseline_loss = 252.38, learner_queue_size = 32, _tick = 2, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:58:10,350[0m][[34mroot[0m][[32mINFO[0m] - Step 10240 @ 511.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 110.3, step = 10240, mean_episode_return = -0.028687, mean_episode_step = 33.933, total_loss = -850.15, entropy_loss = -11.364, pg_loss = -879.17, baseline_loss = 40.381, learner_queue_size = 32, _tick = 3, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:58:15,358[0m][[34mroot[0m][[32mINFO[0m] - Step 10240 @ 0.0 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 115.4, step = 10240, mean_episode_return = -0.028687, mean_episode_step = 33.933, total_loss = -850.15, entropy_loss = -11.364, pg_loss = -879.17, baseline_loss = 40.381, learner_queue_size = 32, _tick = 3, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:58:20,363[0m][[34mroot[0m][[32mINFO[0m] - Step 10240 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 120.4, step = 10240, mean_episode_return = -0.028687, mean_episode_step = 33.933, total_loss = -850.15, entropy_loss = -11.364, pg_loss = -879.17, baseline_loss = 40.381, learner_queue_size = 32, _tick = 3, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:58:25,368[0m][[34mroot[0m][[32mINFO[0m] - Step 12800 @ 511.5 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (train_seconds = 125.4, step = 12800, mean_episode_return = -0.035435, mean_episode_step = 37.815, total_loss = 114.66, entropy_loss = -11.362, pg_loss = 21.802, baseline_loss = 104.22, learner_queue_size = 32, _tick = 4, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:58:30,373[0m][[34mroot[0m][[32mINFO[0m] - Step 15360 @ 511.5 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (train_seconds = 130.4, step = 15360, mean_episode_return = -0.047909, mean_episode_step = 45.666, total_loss = 58.798, entropy_loss = -11.257, pg_loss = 32.108, baseline_loss = 37.947, learner_queue_size = 32, _tick = 5, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:58:35,378[0m][[34mroot[0m][[32mINFO[0m] - Step 15360 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 135.4, step = 15360, mean_episode_return = -0.047909, mean_episode_step = 45.666, total_loss = 58.798, entropy_loss = -11.257, pg_loss = 32.108, baseline_loss = 37.947, learner_queue_size = 32, _tick = 5, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:58:40,383[0m][[34mroot[0m][[32mINFO[0m] - Step 17920 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 140.4, step = 17920, mean_episode_return = -0.058, mean_episode_step = 41.489, total_loss = -5.2218, entropy_loss = -11.028, pg_loss = -30.204, baseline_loss = 36.011, learner_queue_size = 32, _tick = 6, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:58:45,388[0m][[34mroot[0m][[32mINFO[0m] - Step 17920 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 145.4, step = 17920, mean_episode_return = -0.058, mean_episode_step = 41.489, total_loss = -5.2218, entropy_loss = -11.028, pg_loss = -30.204, baseline_loss = 36.011, learner_queue_size = 32, _tick = 6, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:58:50,394[0m][[34mroot[0m][[32mINFO[0m] - Step 20480 @ 511.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 150.4, step = 20480, mean_episode_return = 0.10429, mean_episode_step = 52.846, total_loss = 35.838, entropy_loss = -11.007, pg_loss = 22.954, baseline_loss = 23.891, learner_queue_size = 32, _tick = 7, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:58:55,399[0m][[34mroot[0m][[32mINFO[0m] - Step 20480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 155.4, step = 20480, mean_episode_return = 0.10429, mean_episode_step = 52.846, total_loss = 35.838, entropy_loss = -11.007, pg_loss = 22.954, baseline_loss = 23.891, learner_queue_size = 32, _tick = 7, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:59:00,405[0m][[34mroot[0m][[32mINFO[0m] - Step 23040 @ 511.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 160.4, step = 23040, mean_episode_return = -0.033233, mean_episode_step = 51.267, total_loss = 58.894, entropy_loss = -10.937, pg_loss = 22.576, baseline_loss = 47.254, learner_queue_size = 32, _tick = 8, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:59:05,411[0m][[34mroot[0m][[32mINFO[0m] - Step 25600 @ 511.4 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 165.4, step = 25600, mean_episode_return = -0.02463, mean_episode_step = 47.398, total_loss = 81.893, entropy_loss = -10.571, pg_loss = 42.232, baseline_loss = 50.232, learner_queue_size = 32, _tick = 9, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:59:10,417[0m][[34mroot[0m][[32mINFO[0m] - Step 25600 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 170.4, step = 25600, mean_episode_return = -0.02463, mean_episode_step = 47.398, total_loss = 81.893, entropy_loss = -10.571, pg_loss = 42.232, baseline_loss = 50.232, learner_queue_size = 32, _tick = 9, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:59:15,422[0m][[34mroot[0m][[32mINFO[0m] - Step 28160 @ 511.5 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (train_seconds = 175.4, step = 28160, mean_episode_return = 0.070409, mean_episode_step = 63.096, total_loss = 82.655, entropy_loss = -10.61, pg_loss = 26.375, baseline_loss = 66.89, learner_queue_size = 32, _tick = 10, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:59:20,428[0m][[34mroot[0m][[32mINFO[0m] - Step 28160 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 180.4, step = 28160, mean_episode_return = 0.070409, mean_episode_step = 63.096, total_loss = 82.655, entropy_loss = -10.61, pg_loss = 26.375, baseline_loss = 66.89, learner_queue_size = 32, _tick = 10, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:59:25,433[0m][[34mroot[0m][[32mINFO[0m] - Step 30720 @ 511.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 185.4, step = 30720, mean_episode_return = -0.023094, mean_episode_step = 63.968, total_loss = -217.12, entropy_loss = -10.626, pg_loss = -241.6, baseline_loss = 35.103, learner_queue_size = 32, _tick = 11, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:59:30,438[0m][[34mroot[0m][[32mINFO[0m] - Step 30720 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 190.4, step = 30720, mean_episode_return = -0.023094, mean_episode_step = 63.968, total_loss = -217.12, entropy_loss = -10.626, pg_loss = -241.6, baseline_loss = 35.103, learner_queue_size = 32, _tick = 11, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:59:35,444[0m][[34mroot[0m][[32mINFO[0m] - Step 33280 @ 511.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 195.4, step = 33280, mean_episode_return = -0.036118, mean_episode_step = 53.887, total_loss = 428.78, entropy_loss = -10.506, pg_loss = 339.68, baseline_loss = 99.602, learner_queue_size = 32, _tick = 12, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:59:40,449[0m][[34mroot[0m][[32mINFO[0m] - Step 35840 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 200.4, step = 35840, mean_episode_return = -0.006436, mean_episode_step = 45.296, total_loss = -360.12, entropy_loss = -10.507, pg_loss = -392.32, baseline_loss = 42.705, learner_queue_size = 32, _tick = 13, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:59:45,454[0m][[34mroot[0m][[32mINFO[0m] - Step 35840 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 205.5, step = 35840, mean_episode_return = -0.006436, mean_episode_step = 45.296, total_loss = -360.12, entropy_loss = -10.507, pg_loss = -392.32, baseline_loss = 42.705, learner_queue_size = 32, _tick = 13, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:59:50,459[0m][[34mroot[0m][[32mINFO[0m] - Step 38400 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 210.5, step = 38400, mean_episode_return = 0.0053235, mean_episode_step = 48.081, total_loss = 1019.8, entropy_loss = -10.357, pg_loss = 840.46, baseline_loss = 189.71, learner_queue_size = 32, _tick = 14, _time = 1.7372e+09)[0m
[[36m2025-01-18 10:59:55,464[0m][[34mroot[0m][[32mINFO[0m] - Step 38400 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 215.5, step = 38400, mean_episode_return = 0.0053235, mean_episode_step = 48.081, total_loss = 1019.8, entropy_loss = -10.357, pg_loss = 840.46, baseline_loss = 189.71, learner_queue_size = 32, _tick = 14, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:00:00,469[0m][[34mroot[0m][[32mINFO[0m] - Step 40960 @ 511.5 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (train_seconds = 220.5, step = 40960, mean_episode_return = -0.057688, mean_episode_step = 45.6, total_loss = 808.77, entropy_loss = -10.381, pg_loss = 639.6, baseline_loss = 179.55, learner_queue_size = 32, _tick = 15, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:00:05,474[0m][[34mroot[0m][[32mINFO[0m] - Step 40960 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 225.5, step = 40960, mean_episode_return = -0.057688, mean_episode_step = 45.6, total_loss = 808.77, entropy_loss = -10.381, pg_loss = 639.6, baseline_loss = 179.55, learner_queue_size = 32, _tick = 15, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:00:10,479[0m][[34mroot[0m][[32mINFO[0m] - Step 43520 @ 511.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 230.5, step = 43520, mean_episode_return = 0.014731, mean_episode_step = 57.526, total_loss = -80.336, entropy_loss = -10.343, pg_loss = -184.06, baseline_loss = 114.07, learner_queue_size = 32, _tick = 16, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:00:15,485[0m][[34mroot[0m][[32mINFO[0m] - Step 43520 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 235.5, step = 43520, mean_episode_return = 0.014731, mean_episode_step = 57.526, total_loss = -80.336, entropy_loss = -10.343, pg_loss = -184.06, baseline_loss = 114.07, learner_queue_size = 32, _tick = 16, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:00:20,490[0m][[34mroot[0m][[32mINFO[0m] - Step 46080 @ 511.5 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 240.5, step = 46080, mean_episode_return = -0.043543, mean_episode_step = 52.035, total_loss = -402.2, entropy_loss = -10.214, pg_loss = -476.24, baseline_loss = 84.251, learner_queue_size = 32, _tick = 17, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:00:25,496[0m][[34mroot[0m][[32mINFO[0m] - Step 46080 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 245.5, step = 46080, mean_episode_return = -0.043543, mean_episode_step = 52.035, total_loss = -402.2, entropy_loss = -10.214, pg_loss = -476.24, baseline_loss = 84.251, learner_queue_size = 32, _tick = 17, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:00:30,501[0m][[34mroot[0m][[32mINFO[0m] - Step 48640 @ 511.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 250.5, step = 48640, mean_episode_return = 0.13966, mean_episode_step = 56.06, total_loss = 569.73, entropy_loss = -10.183, pg_loss = 449.04, baseline_loss = 130.88, learner_queue_size = 32, _tick = 18, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:00:35,509[0m][[34mroot[0m][[32mINFO[0m] - Step 51200 @ 511.3 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (train_seconds = 255.5, step = 51200, mean_episode_return = -0.018781, mean_episode_step = 55.004, total_loss = 296.29, entropy_loss = -10.146, pg_loss = 156.81, baseline_loss = 149.62, learner_queue_size = 32, _tick = 19, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:00:40,514[0m][[34mroot[0m][[32mINFO[0m] - Step 51200 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 260.5, step = 51200, mean_episode_return = -0.018781, mean_episode_step = 55.004, total_loss = 296.29, entropy_loss = -10.146, pg_loss = 156.81, baseline_loss = 149.62, learner_queue_size = 32, _tick = 19, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:00:45,519[0m][[34mroot[0m][[32mINFO[0m] - Step 53760 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 265.5, step = 53760, mean_episode_return = 0.050514, mean_episode_step = 56.752, total_loss = -80.601, entropy_loss = -10.069, pg_loss = -183.93, baseline_loss = 113.4, learner_queue_size = 32, _tick = 20, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:00:50,525[0m][[34mroot[0m][[32mINFO[0m] - Step 53760 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 270.5, step = 53760, mean_episode_return = 0.050514, mean_episode_step = 56.752, total_loss = -80.601, entropy_loss = -10.069, pg_loss = -183.93, baseline_loss = 113.4, learner_queue_size = 32, _tick = 20, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:00:55,530[0m][[34mroot[0m][[32mINFO[0m] - Step 56320 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 275.5, step = 56320, mean_episode_return = 0.068, mean_episode_step = 61.191, total_loss = -267.32, entropy_loss = -10.165, pg_loss = -329.3, baseline_loss = 72.141, learner_queue_size = 32, _tick = 21, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:01:00,535[0m][[34mroot[0m][[32mINFO[0m] - Step 56320 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 280.5, step = 56320, mean_episode_return = 0.068, mean_episode_step = 61.191, total_loss = -267.32, entropy_loss = -10.165, pg_loss = -329.3, baseline_loss = 72.141, learner_queue_size = 32, _tick = 21, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:01:05,540[0m][[34mroot[0m][[32mINFO[0m] - Step 58880 @ 511.5 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (train_seconds = 285.5, step = 58880, mean_episode_return = 0.067344, mean_episode_step = 55.469, total_loss = 596.07, entropy_loss = -10.053, pg_loss = 475.77, baseline_loss = 130.35, learner_queue_size = 32, _tick = 22, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:01:10,546[0m][[34mroot[0m][[32mINFO[0m] - Step 58880 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 290.5, step = 58880, mean_episode_return = 0.067344, mean_episode_step = 55.469, total_loss = 596.07, entropy_loss = -10.053, pg_loss = 475.77, baseline_loss = 130.35, learner_queue_size = 32, _tick = 22, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:01:15,551[0m][[34mroot[0m][[32mINFO[0m] - Step 61440 @ 511.4 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 295.6, step = 61440, mean_episode_return = 0.23154, mean_episode_step = 57.52, total_loss = 461.63, entropy_loss = -10.037, pg_loss = 289.48, baseline_loss = 182.19, learner_queue_size = 32, _tick = 23, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:01:20,557[0m][[34mroot[0m][[32mINFO[0m] - Step 61440 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 300.6, step = 61440, mean_episode_return = 0.23154, mean_episode_step = 57.52, total_loss = 461.63, entropy_loss = -10.037, pg_loss = 289.48, baseline_loss = 182.19, learner_queue_size = 32, _tick = 23, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:01:25,562[0m][[34mroot[0m][[32mINFO[0m] - Step 64000 @ 511.5 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 305.6, step = 64000, mean_episode_return = 0.038814, mean_episode_step = 56.937, total_loss = -35.555, entropy_loss = -9.9754, pg_loss = -169.48, baseline_loss = 143.9, learner_queue_size = 32, _tick = 24, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:01:30,567[0m][[34mroot[0m][[32mINFO[0m] - Step 64000 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 310.6, step = 64000, mean_episode_return = 0.038814, mean_episode_step = 56.937, total_loss = -35.555, entropy_loss = -9.9754, pg_loss = -169.48, baseline_loss = 143.9, learner_queue_size = 32, _tick = 24, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:01:35,572[0m][[34mroot[0m][[32mINFO[0m] - Step 66560 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 315.6, step = 66560, mean_episode_return = 0.055133, mean_episode_step = 55.107, total_loss = 20.749, entropy_loss = -9.846, pg_loss = -87.47, baseline_loss = 118.06, learner_queue_size = 32, _tick = 25, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:01:40,577[0m][[34mroot[0m][[32mINFO[0m] - Step 66560 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 320.6, step = 66560, mean_episode_return = 0.055133, mean_episode_step = 55.107, total_loss = 20.749, entropy_loss = -9.846, pg_loss = -87.47, baseline_loss = 118.06, learner_queue_size = 32, _tick = 25, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:01:45,582[0m][[34mroot[0m][[32mINFO[0m] - Step 69120 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 325.6, step = 69120, mean_episode_return = 0.19354, mean_episode_step = 54.085, total_loss = -169.27, entropy_loss = -9.6423, pg_loss = -242.56, baseline_loss = 82.93, learner_queue_size = 32, _tick = 26, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:01:50,588[0m][[34mroot[0m][[32mINFO[0m] - Step 69120 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 330.6, step = 69120, mean_episode_return = 0.19354, mean_episode_step = 54.085, total_loss = -169.27, entropy_loss = -9.6423, pg_loss = -242.56, baseline_loss = 82.93, learner_queue_size = 32, _tick = 26, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:01:55,595[0m][[34mroot[0m][[32mINFO[0m] - Step 71680 @ 511.3 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 335.6, step = 71680, mean_episode_return = 0.041159, mean_episode_step = 49.664, total_loss = 248.63, entropy_loss = -9.6191, pg_loss = 174.73, baseline_loss = 83.521, learner_queue_size = 32, _tick = 27, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:02:00,601[0m][[34mroot[0m][[32mINFO[0m] - Step 71680 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 340.6, step = 71680, mean_episode_return = 0.041159, mean_episode_step = 49.664, total_loss = 248.63, entropy_loss = -9.6191, pg_loss = 174.73, baseline_loss = 83.521, learner_queue_size = 32, _tick = 27, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:02:05,606[0m][[34mroot[0m][[32mINFO[0m] - Step 74240 @ 511.5 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (train_seconds = 345.6, step = 74240, mean_episode_return = 0.091912, mean_episode_step = 60.986, total_loss = 511.19, entropy_loss = -9.4765, pg_loss = 418.76, baseline_loss = 101.91, learner_queue_size = 32, _tick = 28, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:02:10,611[0m][[34mroot[0m][[32mINFO[0m] - Step 74240 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 350.6, step = 74240, mean_episode_return = 0.091912, mean_episode_step = 60.986, total_loss = 511.19, entropy_loss = -9.4765, pg_loss = 418.76, baseline_loss = 101.91, learner_queue_size = 32, _tick = 28, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:02:15,618[0m][[34mroot[0m][[32mINFO[0m] - Step 76800 @ 511.3 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 355.6, step = 76800, mean_episode_return = 0.10628, mean_episode_step = 60.192, total_loss = -258.84, entropy_loss = -9.5001, pg_loss = -343.55, baseline_loss = 94.211, learner_queue_size = 32, _tick = 29, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:02:20,623[0m][[34mroot[0m][[32mINFO[0m] - Step 76800 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 360.6, step = 76800, mean_episode_return = 0.10628, mean_episode_step = 60.192, total_loss = -258.84, entropy_loss = -9.5001, pg_loss = -343.55, baseline_loss = 94.211, learner_queue_size = 32, _tick = 29, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:02:25,628[0m][[34mroot[0m][[32mINFO[0m] - Step 79360 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 365.6, step = 79360, mean_episode_return = 0.071469, mean_episode_step = 58.1, total_loss = 105.57, entropy_loss = -9.3406, pg_loss = 10.532, baseline_loss = 104.38, learner_queue_size = 32, _tick = 30, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:02:30,634[0m][[34mroot[0m][[32mINFO[0m] - Step 79360 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 370.6, step = 79360, mean_episode_return = 0.071469, mean_episode_step = 58.1, total_loss = 105.57, entropy_loss = -9.3406, pg_loss = 10.532, baseline_loss = 104.38, learner_queue_size = 32, _tick = 30, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:02:35,639[0m][[34mroot[0m][[32mINFO[0m] - Step 81920 @ 511.5 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 375.6, step = 81920, mean_episode_return = 0.1803, mean_episode_step = 51.855, total_loss = -411.67, entropy_loss = -9.0077, pg_loss = -459.17, baseline_loss = 56.516, learner_queue_size = 32, _tick = 31, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:02:40,644[0m][[34mroot[0m][[32mINFO[0m] - Step 81920 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 380.6, step = 81920, mean_episode_return = 0.1803, mean_episode_step = 51.855, total_loss = -411.67, entropy_loss = -9.0077, pg_loss = -459.17, baseline_loss = 56.516, learner_queue_size = 32, _tick = 31, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:02:45,650[0m][[34mroot[0m][[32mINFO[0m] - Step 84480 @ 511.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 385.6, step = 84480, mean_episode_return = 0.076261, mean_episode_step = 57.935, total_loss = 180.83, entropy_loss = -8.8899, pg_loss = 132.22, baseline_loss = 57.506, learner_queue_size = 32, _tick = 32, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:02:50,656[0m][[34mroot[0m][[32mINFO[0m] - Step 84480 @ 0.0 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 390.7, step = 84480, mean_episode_return = 0.076261, mean_episode_step = 57.935, total_loss = 180.83, entropy_loss = -8.8899, pg_loss = 132.22, baseline_loss = 57.506, learner_queue_size = 32, _tick = 32, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:02:55,662[0m][[34mroot[0m][[32mINFO[0m] - Step 84480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 395.7, step = 84480, mean_episode_return = 0.076261, mean_episode_step = 57.935, total_loss = 180.83, entropy_loss = -8.8899, pg_loss = 132.22, baseline_loss = 57.506, learner_queue_size = 32, _tick = 32, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:03:00,667[0m][[34mroot[0m][[32mINFO[0m] - Step 87040 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 400.7, step = 87040, mean_episode_return = 0.20878, mean_episode_step = 52.515, total_loss = 370.37, entropy_loss = -8.8446, pg_loss = 282.97, baseline_loss = 96.239, learner_queue_size = 32, _tick = 33, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:03:05,673[0m][[34mroot[0m][[32mINFO[0m] - Step 89600 @ 511.4 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (train_seconds = 405.7, step = 89600, mean_episode_return = 0.11302, mean_episode_step = 59.295, total_loss = -309.91, entropy_loss = -8.7207, pg_loss = -370.98, baseline_loss = 69.794, learner_queue_size = 32, _tick = 34, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:03:10,678[0m][[34mroot[0m][[32mINFO[0m] - Step 89600 @ 0.0 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 410.7, step = 89600, mean_episode_return = 0.11302, mean_episode_step = 59.295, total_loss = -309.91, entropy_loss = -8.7207, pg_loss = -370.98, baseline_loss = 69.794, learner_queue_size = 32, _tick = 34, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:03:15,685[0m][[34mroot[0m][[32mINFO[0m] - Step 89600 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 415.7, step = 89600, mean_episode_return = 0.11302, mean_episode_step = 59.295, total_loss = -309.91, entropy_loss = -8.7207, pg_loss = -370.98, baseline_loss = 69.794, learner_queue_size = 32, _tick = 34, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:03:20,691[0m][[34mroot[0m][[32mINFO[0m] - Step 92160 @ 511.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 420.7, step = 92160, mean_episode_return = 0.45876, mean_episode_step = 68.235, total_loss = -410.02, entropy_loss = -8.6485, pg_loss = -454.34, baseline_loss = 52.964, learner_queue_size = 32, _tick = 35, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:03:25,697[0m][[34mroot[0m][[32mINFO[0m] - Step 92160 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 425.7, step = 92160, mean_episode_return = 0.45876, mean_episode_step = 68.235, total_loss = -410.02, entropy_loss = -8.6485, pg_loss = -454.34, baseline_loss = 52.964, learner_queue_size = 32, _tick = 35, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:03:30,703[0m][[34mroot[0m][[32mINFO[0m] - Step 94720 @ 511.4 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 430.7, step = 94720, mean_episode_return = 0.26228, mean_episode_step = 71.005, total_loss = -32.953, entropy_loss = -8.6738, pg_loss = -112.73, baseline_loss = 88.451, learner_queue_size = 32, _tick = 36, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:03:35,709[0m][[34mroot[0m][[32mINFO[0m] - Step 94720 @ 0.0 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 435.7, step = 94720, mean_episode_return = 0.26228, mean_episode_step = 71.005, total_loss = -32.953, entropy_loss = -8.6738, pg_loss = -112.73, baseline_loss = 88.451, learner_queue_size = 32, _tick = 36, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:03:40,715[0m][[34mroot[0m][[32mINFO[0m] - Step 97280 @ 511.3 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 440.7, step = 97280, mean_episode_return = 0.052276, mean_episode_step = 66.664, total_loss = -205.22, entropy_loss = -8.6217, pg_loss = -256.58, baseline_loss = 59.99, learner_queue_size = 32, _tick = 37, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:03:45,720[0m][[34mroot[0m][[32mINFO[0m] - Step 97280 @ 0.0 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 445.7, step = 97280, mean_episode_return = 0.052276, mean_episode_step = 66.664, total_loss = -205.22, entropy_loss = -8.6217, pg_loss = -256.58, baseline_loss = 59.99, learner_queue_size = 32, _tick = 37, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:03:50,725[0m][[34mroot[0m][[32mINFO[0m] - Step 99840 @ 511.5 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 450.7, step = 99840, mean_episode_return = 0.65891, mean_episode_step = 78.129, total_loss = 436.54, entropy_loss = -8.4827, pg_loss = 339.9, baseline_loss = 105.12, learner_queue_size = 32, _tick = 38, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:03:55,730[0m][[34mroot[0m][[32mINFO[0m] - Step 99840 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 455.7, step = 99840, mean_episode_return = 0.65891, mean_episode_step = 78.129, total_loss = 436.54, entropy_loss = -8.4827, pg_loss = 339.9, baseline_loss = 105.12, learner_queue_size = 32, _tick = 38, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:04:00,735[0m][[34mroot[0m][[32mINFO[0m] - Step 102400 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 460.7, step = 102400, mean_episode_return = 0.12715, mean_episode_step = 68.842, total_loss = 815.33, entropy_loss = -8.6818, pg_loss = 653.77, baseline_loss = 170.24, learner_queue_size = 32, _tick = 39, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:04:05,740[0m][[34mroot[0m][[32mINFO[0m] - Step 102400 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 465.7, step = 102400, mean_episode_return = 0.12715, mean_episode_step = 68.842, total_loss = 815.33, entropy_loss = -8.6818, pg_loss = 653.77, baseline_loss = 170.24, learner_queue_size = 32, _tick = 39, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:04:10,745[0m][[34mroot[0m][[32mINFO[0m] - Step 104960 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 470.7, step = 104960, mean_episode_return = 0.23197, mean_episode_step = 72.497, total_loss = -178.69, entropy_loss = -8.7568, pg_loss = -276.05, baseline_loss = 106.13, learner_queue_size = 32, _tick = 40, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:04:15,750[0m][[34mroot[0m][[32mINFO[0m] - Step 104960 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 475.7, step = 104960, mean_episode_return = 0.23197, mean_episode_step = 72.497, total_loss = -178.69, entropy_loss = -8.7568, pg_loss = -276.05, baseline_loss = 106.13, learner_queue_size = 32, _tick = 40, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:04:20,756[0m][[34mroot[0m][[32mINFO[0m] - Step 107520 @ 511.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 480.8, step = 107520, mean_episode_return = 0.2971, mean_episode_step = 67.801, total_loss = 649.9, entropy_loss = -8.7413, pg_loss = 534.68, baseline_loss = 123.96, learner_queue_size = 32, _tick = 41, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:04:25,761[0m][[34mroot[0m][[32mINFO[0m] - Step 107520 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 485.8, step = 107520, mean_episode_return = 0.2971, mean_episode_step = 67.801, total_loss = 649.9, entropy_loss = -8.7413, pg_loss = 534.68, baseline_loss = 123.96, learner_queue_size = 32, _tick = 41, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:04:30,766[0m][[34mroot[0m][[32mINFO[0m] - Step 110080 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 490.8, step = 110080, mean_episode_return = 0.22647, mean_episode_step = 54.457, total_loss = -450.24, entropy_loss = -8.8207, pg_loss = -543.72, baseline_loss = 102.3, learner_queue_size = 32, _tick = 42, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:04:35,772[0m][[34mroot[0m][[32mINFO[0m] - Step 112640 @ 511.4 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 495.8, step = 112640, mean_episode_return = 0.32543, mean_episode_step = 78.511, total_loss = 230.3, entropy_loss = -8.7777, pg_loss = 144.0, baseline_loss = 95.077, learner_queue_size = 32, _tick = 43, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:04:40,775[0m][[34mroot[0m][[32mINFO[0m] - Step 112640 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 500.8, step = 112640, mean_episode_return = 0.32543, mean_episode_step = 78.511, total_loss = 230.3, entropy_loss = -8.7777, pg_loss = 144.0, baseline_loss = 95.077, learner_queue_size = 32, _tick = 43, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:04:45,782[0m][[34mroot[0m][[32mINFO[0m] - Step 115200 @ 511.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 505.8, step = 115200, mean_episode_return = 0.15833, mean_episode_step = 59.177, total_loss = 378.68, entropy_loss = -8.743, pg_loss = 211.99, baseline_loss = 175.44, learner_queue_size = 32, _tick = 44, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:04:50,788[0m][[34mroot[0m][[32mINFO[0m] - Step 115200 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 510.8, step = 115200, mean_episode_return = 0.15833, mean_episode_step = 59.177, total_loss = 378.68, entropy_loss = -8.743, pg_loss = 211.99, baseline_loss = 175.44, learner_queue_size = 32, _tick = 44, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:04:55,793[0m][[34mroot[0m][[32mINFO[0m] - Step 117760 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 515.8, step = 117760, mean_episode_return = 0.14475, mean_episode_step = 53.416, total_loss = -428.63, entropy_loss = -8.8708, pg_loss = -502.69, baseline_loss = 82.927, learner_queue_size = 32, _tick = 45, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:05:00,797[0m][[34mroot[0m][[32mINFO[0m] - Step 117760 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 520.8, step = 117760, mean_episode_return = 0.14475, mean_episode_step = 53.416, total_loss = -428.63, entropy_loss = -8.8708, pg_loss = -502.69, baseline_loss = 82.927, learner_queue_size = 32, _tick = 45, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:05:05,802[0m][[34mroot[0m][[32mINFO[0m] - Step 120320 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 525.8, step = 120320, mean_episode_return = 0.28888, mean_episode_step = 67.259, total_loss = 880.2, entropy_loss = -8.8118, pg_loss = 764.84, baseline_loss = 124.17, learner_queue_size = 32, _tick = 46, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:05:10,807[0m][[34mroot[0m][[32mINFO[0m] - Step 122880 @ 511.5 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (train_seconds = 530.8, step = 122880, mean_episode_return = 0.22821, mean_episode_step = 66.243, total_loss = 102.03, entropy_loss = -8.8042, pg_loss = -4.2173, baseline_loss = 115.05, learner_queue_size = 32, _tick = 47, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:05:15,813[0m][[34mroot[0m][[32mINFO[0m] - Step 122880 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 535.8, step = 122880, mean_episode_return = 0.22821, mean_episode_step = 66.243, total_loss = 102.03, entropy_loss = -8.8042, pg_loss = -4.2173, baseline_loss = 115.05, learner_queue_size = 32, _tick = 47, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:05:20,818[0m][[34mroot[0m][[32mINFO[0m] - Saving checkpoint to /opt/minihack/checkpoint_0.25.tar[0m
[[36m2025-01-18 11:05:20,873[0m][[34mroot[0m][[32mINFO[0m] - Step 125440 @ 511.5 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 540.8, step = 125440, mean_episode_return = 0.53142, mean_episode_step = 85.742, total_loss = -489.66, entropy_loss = -8.7077, pg_loss = -591.28, baseline_loss = 110.33, learner_queue_size = 32, _tick = 48, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:05:25,878[0m][[34mroot[0m][[32mINFO[0m] - Step 125440 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 545.9, step = 125440, mean_episode_return = 0.53142, mean_episode_step = 85.742, total_loss = -489.66, entropy_loss = -8.7077, pg_loss = -591.28, baseline_loss = 110.33, learner_queue_size = 32, _tick = 48, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:05:30,884[0m][[34mroot[0m][[32mINFO[0m] - Step 128000 @ 511.4 SPS. Inference batcher size: 5. Learner queue size: 32. Other stats: (train_seconds = 550.9, step = 128000, mean_episode_return = 0.24905, mean_episode_step = 66.791, total_loss = -68.25, entropy_loss = -8.7047, pg_loss = -197.62, baseline_loss = 138.08, learner_queue_size = 32, _tick = 49, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:05:35,889[0m][[34mroot[0m][[32mINFO[0m] - Step 128000 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 555.9, step = 128000, mean_episode_return = 0.24905, mean_episode_step = 66.791, total_loss = -68.25, entropy_loss = -8.7047, pg_loss = -197.62, baseline_loss = 138.08, learner_queue_size = 32, _tick = 49, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:05:40,895[0m][[34mroot[0m][[32mINFO[0m] - Step 130560 @ 511.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 560.9, step = 130560, mean_episode_return = 0.23935, mean_episode_step = 61.718, total_loss = 127.55, entropy_loss = -8.6948, pg_loss = 27.088, baseline_loss = 109.15, learner_queue_size = 32, _tick = 50, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:05:45,900[0m][[34mroot[0m][[32mINFO[0m] - Step 133120 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 565.9, step = 133120, mean_episode_return = 0.36966, mean_episode_step = 54.387, total_loss = -356.62, entropy_loss = -8.6847, pg_loss = -453.22, baseline_loss = 105.29, learner_queue_size = 32, _tick = 51, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:05:50,906[0m][[34mroot[0m][[32mINFO[0m] - Step 133120 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 570.9, step = 133120, mean_episode_return = 0.36966, mean_episode_step = 54.387, total_loss = -356.62, entropy_loss = -8.6847, pg_loss = -453.22, baseline_loss = 105.29, learner_queue_size = 32, _tick = 51, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:05:55,911[0m][[34mroot[0m][[32mINFO[0m] - Step 135680 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 575.9, step = 135680, mean_episode_return = 0.4576, mean_episode_step = 68.258, total_loss = 412.61, entropy_loss = -8.666, pg_loss = 309.48, baseline_loss = 111.79, learner_queue_size = 32, _tick = 52, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:06:00,916[0m][[34mroot[0m][[32mINFO[0m] - Step 135680 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 580.9, step = 135680, mean_episode_return = 0.4576, mean_episode_step = 68.258, total_loss = 412.61, entropy_loss = -8.666, pg_loss = 309.48, baseline_loss = 111.79, learner_queue_size = 32, _tick = 52, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:06:05,922[0m][[34mroot[0m][[32mINFO[0m] - Step 138240 @ 511.4 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 585.9, step = 138240, mean_episode_return = 0.19118, mean_episode_step = 76.087, total_loss = 396.72, entropy_loss = -8.5962, pg_loss = 294.89, baseline_loss = 110.43, learner_queue_size = 32, _tick = 53, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:06:10,927[0m][[34mroot[0m][[32mINFO[0m] - Step 140800 @ 511.5 SPS. Inference batcher size: 23. Learner queue size: 32. Other stats: (train_seconds = 590.9, step = 140800, mean_episode_return = 0.34141, mean_episode_step = 78.675, total_loss = 110.29, entropy_loss = -8.5266, pg_loss = 8.737, baseline_loss = 110.08, learner_queue_size = 32, _tick = 54, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:06:15,932[0m][[34mroot[0m][[32mINFO[0m] - Step 140800 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 595.9, step = 140800, mean_episode_return = 0.34141, mean_episode_step = 78.675, total_loss = 110.29, entropy_loss = -8.5266, pg_loss = 8.737, baseline_loss = 110.08, learner_queue_size = 32, _tick = 54, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:06:20,937[0m][[34mroot[0m][[32mINFO[0m] - Saving checkpoint to /opt/minihack/checkpoint.tar[0m
[[36m2025-01-18 11:06:21,006[0m][[34mroot[0m][[32mINFO[0m] - Step 143360 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 600.9, step = 143360, mean_episode_return = 0.35278, mean_episode_step = 57.97, total_loss = 394.96, entropy_loss = -8.5179, pg_loss = 287.35, baseline_loss = 116.13, learner_queue_size = 32, _tick = 55, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:06:26,012[0m][[34mroot[0m][[32mINFO[0m] - Step 143360 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 606.0, step = 143360, mean_episode_return = 0.35278, mean_episode_step = 57.97, total_loss = 394.96, entropy_loss = -8.5179, pg_loss = 287.35, baseline_loss = 116.13, learner_queue_size = 32, _tick = 55, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:06:31,017[0m][[34mroot[0m][[32mINFO[0m] - Step 145920 @ 511.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 611.0, step = 145920, mean_episode_return = 0.279, mean_episode_step = 75.486, total_loss = -468.43, entropy_loss = -8.5149, pg_loss = -537.39, baseline_loss = 77.475, learner_queue_size = 32, _tick = 56, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:06:36,023[0m][[34mroot[0m][[32mINFO[0m] - Step 145920 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 616.0, step = 145920, mean_episode_return = 0.279, mean_episode_step = 75.486, total_loss = -468.43, entropy_loss = -8.5149, pg_loss = -537.39, baseline_loss = 77.475, learner_queue_size = 32, _tick = 56, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:06:41,029[0m][[34mroot[0m][[32mINFO[0m] - Step 148480 @ 511.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 621.0, step = 148480, mean_episode_return = 0.53636, mean_episode_step = 76.997, total_loss = 274.27, entropy_loss = -8.5437, pg_loss = 153.68, baseline_loss = 129.13, learner_queue_size = 32, _tick = 57, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:06:46,034[0m][[34mroot[0m][[32mINFO[0m] - Step 148480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 626.0, step = 148480, mean_episode_return = 0.53636, mean_episode_step = 76.997, total_loss = 274.27, entropy_loss = -8.5437, pg_loss = 153.68, baseline_loss = 129.13, learner_queue_size = 32, _tick = 57, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:06:51,039[0m][[34mroot[0m][[32mINFO[0m] - Step 151040 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 631.0, step = 151040, mean_episode_return = 0.305, mean_episode_step = 65.035, total_loss = 161.19, entropy_loss = -8.4929, pg_loss = 49.616, baseline_loss = 120.07, learner_queue_size = 32, _tick = 58, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:06:56,045[0m][[34mroot[0m][[32mINFO[0m] - Step 151040 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 636.0, step = 151040, mean_episode_return = 0.305, mean_episode_step = 65.035, total_loss = 161.19, entropy_loss = -8.4929, pg_loss = 49.616, baseline_loss = 120.07, learner_queue_size = 32, _tick = 58, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:07:01,050[0m][[34mroot[0m][[32mINFO[0m] - Step 153600 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 641.0, step = 153600, mean_episode_return = 0.16884, mean_episode_step = 84.039, total_loss = -251.35, entropy_loss = -8.607, pg_loss = -304.43, baseline_loss = 61.679, learner_queue_size = 32, _tick = 59, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:07:06,055[0m][[34mroot[0m][[32mINFO[0m] - Step 153600 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 646.1, step = 153600, mean_episode_return = 0.16884, mean_episode_step = 84.039, total_loss = -251.35, entropy_loss = -8.607, pg_loss = -304.43, baseline_loss = 61.679, learner_queue_size = 32, _tick = 59, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:07:11,060[0m][[34mroot[0m][[32mINFO[0m] - Step 156160 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 651.1, step = 156160, mean_episode_return = 0.18854, mean_episode_step = 74.051, total_loss = -27.782, entropy_loss = -8.6832, pg_loss = -68.847, baseline_loss = 49.747, learner_queue_size = 32, _tick = 60, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:07:16,066[0m][[34mroot[0m][[32mINFO[0m] - Step 158720 @ 511.4 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (train_seconds = 656.1, step = 158720, mean_episode_return = 0.50874, mean_episode_step = 72.746, total_loss = 664.8, entropy_loss = -8.5325, pg_loss = 600.46, baseline_loss = 72.87, learner_queue_size = 32, _tick = 61, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:07:21,072[0m][[34mroot[0m][[32mINFO[0m] - Step 158720 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 661.1, step = 158720, mean_episode_return = 0.50874, mean_episode_step = 72.746, total_loss = 664.8, entropy_loss = -8.5325, pg_loss = 600.46, baseline_loss = 72.87, learner_queue_size = 32, _tick = 61, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:07:26,077[0m][[34mroot[0m][[32mINFO[0m] - Step 161280 @ 511.5 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (train_seconds = 666.1, step = 161280, mean_episode_return = 0.30407, mean_episode_step = 60.963, total_loss = -485.53, entropy_loss = -8.5658, pg_loss = -544.16, baseline_loss = 67.193, learner_queue_size = 32, _tick = 62, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:07:31,082[0m][[34mroot[0m][[32mINFO[0m] - Step 161280 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 671.1, step = 161280, mean_episode_return = 0.30407, mean_episode_step = 60.963, total_loss = -485.53, entropy_loss = -8.5658, pg_loss = -544.16, baseline_loss = 67.193, learner_queue_size = 32, _tick = 62, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:07:36,087[0m][[34mroot[0m][[32mINFO[0m] - Step 163840 @ 511.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 676.1, step = 163840, mean_episode_return = 0.34043, mean_episode_step = 66.77, total_loss = -208.15, entropy_loss = -8.5118, pg_loss = -261.92, baseline_loss = 62.279, learner_queue_size = 32, _tick = 63, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:07:41,092[0m][[34mroot[0m][[32mINFO[0m] - Step 163840 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 681.1, step = 163840, mean_episode_return = 0.34043, mean_episode_step = 66.77, total_loss = -208.15, entropy_loss = -8.5118, pg_loss = -261.92, baseline_loss = 62.279, learner_queue_size = 32, _tick = 63, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:07:46,098[0m][[34mroot[0m][[32mINFO[0m] - Step 166400 @ 511.4 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (train_seconds = 686.1, step = 166400, mean_episode_return = 0.39268, mean_episode_step = 69.351, total_loss = -121.41, entropy_loss = -8.5133, pg_loss = -201.27, baseline_loss = 88.372, learner_queue_size = 32, _tick = 64, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:07:51,105[0m][[34mroot[0m][[32mINFO[0m] - Step 168960 @ 511.3 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (train_seconds = 691.1, step = 168960, mean_episode_return = 0.598, mean_episode_step = 73.992, total_loss = 264.56, entropy_loss = -8.5473, pg_loss = 189.53, baseline_loss = 83.573, learner_queue_size = 32, _tick = 65, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:07:56,111[0m][[34mroot[0m][[32mINFO[0m] - Step 168960 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 696.1, step = 168960, mean_episode_return = 0.598, mean_episode_step = 73.992, total_loss = 264.56, entropy_loss = -8.5473, pg_loss = 189.53, baseline_loss = 83.573, learner_queue_size = 32, _tick = 65, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:08:01,116[0m][[34mroot[0m][[32mINFO[0m] - Step 171520 @ 511.5 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (train_seconds = 701.1, step = 171520, mean_episode_return = 0.42714, mean_episode_step = 81.327, total_loss = 264.73, entropy_loss = -8.4892, pg_loss = 188.31, baseline_loss = 84.915, learner_queue_size = 32, _tick = 66, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:08:06,121[0m][[34mroot[0m][[32mINFO[0m] - Step 171520 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 706.1, step = 171520, mean_episode_return = 0.42714, mean_episode_step = 81.327, total_loss = 264.73, entropy_loss = -8.4892, pg_loss = 188.31, baseline_loss = 84.915, learner_queue_size = 32, _tick = 66, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:08:11,135[0m][[34mroot[0m][[32mINFO[0m] - Step 174080 @ 510.6 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (train_seconds = 711.1, step = 174080, mean_episode_return = 0.47639, mean_episode_step = 85.42, total_loss = 123.15, entropy_loss = -8.4498, pg_loss = 42.635, baseline_loss = 88.96, learner_queue_size = 32, _tick = 67, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:08:16,148[0m][[34mroot[0m][[32mINFO[0m] - Step 174080 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 716.1, step = 174080, mean_episode_return = 0.47639, mean_episode_step = 85.42, total_loss = 123.15, entropy_loss = -8.4498, pg_loss = 42.635, baseline_loss = 88.96, learner_queue_size = 32, _tick = 67, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:08:21,161[0m][[34mroot[0m][[32mINFO[0m] - Step 176640 @ 510.7 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 721.2, step = 176640, mean_episode_return = 0.32253, mean_episode_step = 71.559, total_loss = -325.53, entropy_loss = -8.4413, pg_loss = -403.81, baseline_loss = 86.718, learner_queue_size = 32, _tick = 68, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:08:26,175[0m][[34mroot[0m][[32mINFO[0m] - Step 179200 @ 510.6 SPS. Inference batcher size: 3. Learner queue size: 32. Other stats: (train_seconds = 726.2, step = 179200, mean_episode_return = 0.70039, mean_episode_step = 85.024, total_loss = 526.89, entropy_loss = -8.427, pg_loss = 425.49, baseline_loss = 109.83, learner_queue_size = 32, _tick = 69, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:08:31,186[0m][[34mroot[0m][[32mINFO[0m] - Step 179200 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 731.2, step = 179200, mean_episode_return = 0.70039, mean_episode_step = 85.024, total_loss = 526.89, entropy_loss = -8.427, pg_loss = 425.49, baseline_loss = 109.83, learner_queue_size = 32, _tick = 69, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:08:36,201[0m][[34mroot[0m][[32mINFO[0m] - Step 181760 @ 510.5 SPS. Inference batcher size: 22. Learner queue size: 32. Other stats: (train_seconds = 736.2, step = 181760, mean_episode_return = 0.48583, mean_episode_step = 61.507, total_loss = 71.401, entropy_loss = -8.4044, pg_loss = -44.402, baseline_loss = 124.21, learner_queue_size = 32, _tick = 70, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:08:41,214[0m][[34mroot[0m][[32mINFO[0m] - Step 181760 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 741.2, step = 181760, mean_episode_return = 0.48583, mean_episode_step = 61.507, total_loss = 71.401, entropy_loss = -8.4044, pg_loss = -44.402, baseline_loss = 124.21, learner_queue_size = 32, _tick = 70, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:08:46,220[0m][[34mroot[0m][[32mINFO[0m] - Step 184320 @ 511.4 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 746.2, step = 184320, mean_episode_return = 0.23525, mean_episode_step = 72.671, total_loss = -49.664, entropy_loss = -8.295, pg_loss = -113.17, baseline_loss = 71.805, learner_queue_size = 32, _tick = 71, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:08:51,225[0m][[34mroot[0m][[32mINFO[0m] - Step 186880 @ 511.4 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 751.2, step = 186880, mean_episode_return = 0.46538, mean_episode_step = 71.634, total_loss = 266.83, entropy_loss = -8.2768, pg_loss = 169.82, baseline_loss = 105.29, learner_queue_size = 32, _tick = 72, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:08:56,231[0m][[34mroot[0m][[32mINFO[0m] - Step 186880 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 756.2, step = 186880, mean_episode_return = 0.46538, mean_episode_step = 71.634, total_loss = 266.83, entropy_loss = -8.2768, pg_loss = 169.82, baseline_loss = 105.29, learner_queue_size = 32, _tick = 72, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:09:01,239[0m][[34mroot[0m][[32mINFO[0m] - Step 189440 @ 511.2 SPS. Inference batcher size: 26. Learner queue size: 32. Other stats: (train_seconds = 761.2, step = 189440, mean_episode_return = 0.39119, mean_episode_step = 65.492, total_loss = 4.6822, entropy_loss = -8.3489, pg_loss = -82.787, baseline_loss = 95.818, learner_queue_size = 32, _tick = 73, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:09:06,244[0m][[34mroot[0m][[32mINFO[0m] - Step 189440 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 766.2, step = 189440, mean_episode_return = 0.39119, mean_episode_step = 65.492, total_loss = 4.6822, entropy_loss = -8.3489, pg_loss = -82.787, baseline_loss = 95.818, learner_queue_size = 32, _tick = 73, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:09:11,251[0m][[34mroot[0m][[32mINFO[0m] - Step 192000 @ 511.3 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 771.3, step = 192000, mean_episode_return = 0.34843, mean_episode_step = 61.914, total_loss = -65.164, entropy_loss = -8.4051, pg_loss = -159.94, baseline_loss = 103.18, learner_queue_size = 32, _tick = 74, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:09:16,256[0m][[34mroot[0m][[32mINFO[0m] - Step 192000 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 776.3, step = 192000, mean_episode_return = 0.34843, mean_episode_step = 61.914, total_loss = -65.164, entropy_loss = -8.4051, pg_loss = -159.94, baseline_loss = 103.18, learner_queue_size = 32, _tick = 74, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:09:21,261[0m][[34mroot[0m][[32mINFO[0m] - Step 194560 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 781.3, step = 194560, mean_episode_return = 0.22057, mean_episode_step = 59.325, total_loss = -55.701, entropy_loss = -8.4412, pg_loss = -125.58, baseline_loss = 78.324, learner_queue_size = 32, _tick = 75, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:09:26,266[0m][[34mroot[0m][[32mINFO[0m] - Step 197120 @ 511.5 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (train_seconds = 786.3, step = 197120, mean_episode_return = 0.30411, mean_episode_step = 96.86, total_loss = 56.888, entropy_loss = -8.395, pg_loss = -16.676, baseline_loss = 81.96, learner_queue_size = 32, _tick = 76, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:09:31,272[0m][[34mroot[0m][[32mINFO[0m] - Step 197120 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 791.3, step = 197120, mean_episode_return = 0.30411, mean_episode_step = 96.86, total_loss = 56.888, entropy_loss = -8.395, pg_loss = -16.676, baseline_loss = 81.96, learner_queue_size = 32, _tick = 76, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:09:36,277[0m][[34mroot[0m][[32mINFO[0m] - Step 199680 @ 511.5 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 796.3, step = 199680, mean_episode_return = 0.51583, mean_episode_step = 61.223, total_loss = 261.19, entropy_loss = -8.4771, pg_loss = 184.16, baseline_loss = 85.502, learner_queue_size = 32, _tick = 77, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:09:41,283[0m][[34mroot[0m][[32mINFO[0m] - Step 199680 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 801.3, step = 199680, mean_episode_return = 0.51583, mean_episode_step = 61.223, total_loss = 261.19, entropy_loss = -8.4771, pg_loss = 184.16, baseline_loss = 85.502, learner_queue_size = 32, _tick = 77, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:09:46,289[0m][[34mroot[0m][[32mINFO[0m] - Step 202240 @ 511.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 806.3, step = 202240, mean_episode_return = 0.40024, mean_episode_step = 93.063, total_loss = 185.45, entropy_loss = -8.322, pg_loss = 124.54, baseline_loss = 69.235, learner_queue_size = 32, _tick = 78, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:09:51,294[0m][[34mroot[0m][[32mINFO[0m] - Step 202240 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 811.3, step = 202240, mean_episode_return = 0.40024, mean_episode_step = 93.063, total_loss = 185.45, entropy_loss = -8.322, pg_loss = 124.54, baseline_loss = 69.235, learner_queue_size = 32, _tick = 78, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:09:56,299[0m][[34mroot[0m][[32mINFO[0m] - Step 204800 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 816.3, step = 204800, mean_episode_return = 0.30539, mean_episode_step = 77.239, total_loss = 52.28, entropy_loss = -8.3491, pg_loss = -26.331, baseline_loss = 86.96, learner_queue_size = 32, _tick = 79, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:10:01,304[0m][[34mroot[0m][[32mINFO[0m] - Step 207360 @ 511.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (train_seconds = 821.3, step = 207360, mean_episode_return = 0.58165, mean_episode_step = 85.312, total_loss = 21.466, entropy_loss = -8.3309, pg_loss = -38.517, baseline_loss = 68.314, learner_queue_size = 32, _tick = 80, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:10:06,310[0m][[34mroot[0m][[32mINFO[0m] - Step 207360 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 826.3, step = 207360, mean_episode_return = 0.58165, mean_episode_step = 85.312, total_loss = 21.466, entropy_loss = -8.3309, pg_loss = -38.517, baseline_loss = 68.314, learner_queue_size = 32, _tick = 80, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:10:11,315[0m][[34mroot[0m][[32mINFO[0m] - Step 209920 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 831.3, step = 209920, mean_episode_return = 0.59362, mean_episode_step = 81.554, total_loss = 61.099, entropy_loss = -8.2604, pg_loss = -7.5185, baseline_loss = 76.877, learner_queue_size = 32, _tick = 81, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:10:16,321[0m][[34mroot[0m][[32mINFO[0m] - Step 209920 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 836.3, step = 209920, mean_episode_return = 0.59362, mean_episode_step = 81.554, total_loss = 61.099, entropy_loss = -8.2604, pg_loss = -7.5185, baseline_loss = 76.877, learner_queue_size = 32, _tick = 81, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:10:21,326[0m][[34mroot[0m][[32mINFO[0m] - Step 212480 @ 511.5 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (train_seconds = 841.3, step = 212480, mean_episode_return = 0.6351, mean_episode_step = 84.614, total_loss = 67.706, entropy_loss = -8.2501, pg_loss = -2.8239, baseline_loss = 78.78, learner_queue_size = 32, _tick = 82, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:10:26,332[0m][[34mroot[0m][[32mINFO[0m] - Step 212480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 846.3, step = 212480, mean_episode_return = 0.6351, mean_episode_step = 84.614, total_loss = 67.706, entropy_loss = -8.2501, pg_loss = -2.8239, baseline_loss = 78.78, learner_queue_size = 32, _tick = 82, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:10:31,338[0m][[34mroot[0m][[32mINFO[0m] - Step 215040 @ 511.4 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 851.3, step = 215040, mean_episode_return = 0.52036, mean_episode_step = 73.968, total_loss = -264.98, entropy_loss = -8.3532, pg_loss = -327.23, baseline_loss = 70.603, learner_queue_size = 32, _tick = 83, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:10:36,343[0m][[34mroot[0m][[32mINFO[0m] - Step 217600 @ 511.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 856.3, step = 217600, mean_episode_return = 0.40877, mean_episode_step = 73.141, total_loss = -152.44, entropy_loss = -8.2967, pg_loss = -212.3, baseline_loss = 68.16, learner_queue_size = 32, _tick = 84, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:10:41,348[0m][[34mroot[0m][[32mINFO[0m] - Step 217600 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 861.3, step = 217600, mean_episode_return = 0.40877, mean_episode_step = 73.141, total_loss = -152.44, entropy_loss = -8.2967, pg_loss = -212.3, baseline_loss = 68.16, learner_queue_size = 32, _tick = 84, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:10:46,353[0m][[34mroot[0m][[32mINFO[0m] - Step 220160 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 866.4, step = 220160, mean_episode_return = 0.50212, mean_episode_step = 93.568, total_loss = 208.54, entropy_loss = -8.2162, pg_loss = 163.18, baseline_loss = 53.579, learner_queue_size = 32, _tick = 85, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:10:51,358[0m][[34mroot[0m][[32mINFO[0m] - Step 220160 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 871.4, step = 220160, mean_episode_return = 0.50212, mean_episode_step = 93.568, total_loss = 208.54, entropy_loss = -8.2162, pg_loss = 163.18, baseline_loss = 53.579, learner_queue_size = 32, _tick = 85, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:10:56,363[0m][[34mroot[0m][[32mINFO[0m] - Step 222720 @ 511.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 876.4, step = 222720, mean_episode_return = 0.50586, mean_episode_step = 77.576, total_loss = -774.09, entropy_loss = -8.2975, pg_loss = -840.69, baseline_loss = 74.9, learner_queue_size = 32, _tick = 86, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:11:01,368[0m][[34mroot[0m][[32mINFO[0m] - Step 222720 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 881.4, step = 222720, mean_episode_return = 0.50586, mean_episode_step = 77.576, total_loss = -774.09, entropy_loss = -8.2975, pg_loss = -840.69, baseline_loss = 74.9, learner_queue_size = 32, _tick = 86, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:11:06,373[0m][[34mroot[0m][[32mINFO[0m] - Step 225280 @ 511.4 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 886.4, step = 225280, mean_episode_return = 0.75952, mean_episode_step = 86.852, total_loss = 6.6043, entropy_loss = -8.2178, pg_loss = -67.925, baseline_loss = 82.747, learner_queue_size = 32, _tick = 87, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:11:11,379[0m][[34mroot[0m][[32mINFO[0m] - Step 227840 @ 511.4 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 891.4, step = 227840, mean_episode_return = 0.631, mean_episode_step = 75.818, total_loss = 165.2, entropy_loss = -8.2385, pg_loss = 66.735, baseline_loss = 106.71, learner_queue_size = 32, _tick = 88, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:11:16,385[0m][[34mroot[0m][[32mINFO[0m] - Step 227840 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 896.4, step = 227840, mean_episode_return = 0.631, mean_episode_step = 75.818, total_loss = 165.2, entropy_loss = -8.2385, pg_loss = 66.735, baseline_loss = 106.71, learner_queue_size = 32, _tick = 88, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:11:21,393[0m][[34mroot[0m][[32mINFO[0m] - Step 230400 @ 511.2 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 901.4, step = 230400, mean_episode_return = 0.5072, mean_episode_step = 80.995, total_loss = -490.6, entropy_loss = -8.2175, pg_loss = -559.68, baseline_loss = 77.294, learner_queue_size = 32, _tick = 89, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:11:26,399[0m][[34mroot[0m][[32mINFO[0m] - Step 232960 @ 511.4 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (train_seconds = 906.4, step = 232960, mean_episode_return = 0.50981, mean_episode_step = 72.743, total_loss = 18.6, entropy_loss = -8.1899, pg_loss = -76.061, baseline_loss = 102.85, learner_queue_size = 32, _tick = 90, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:11:31,405[0m][[34mroot[0m][[32mINFO[0m] - Step 232960 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 911.4, step = 232960, mean_episode_return = 0.50981, mean_episode_step = 72.743, total_loss = 18.6, entropy_loss = -8.1899, pg_loss = -76.061, baseline_loss = 102.85, learner_queue_size = 32, _tick = 90, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:11:36,410[0m][[34mroot[0m][[32mINFO[0m] - Step 235520 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 916.4, step = 235520, mean_episode_return = 0.36433, mean_episode_step = 72.807, total_loss = 383.99, entropy_loss = -8.1014, pg_loss = 291.41, baseline_loss = 100.68, learner_queue_size = 32, _tick = 91, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:11:41,415[0m][[34mroot[0m][[32mINFO[0m] - Step 235520 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 921.4, step = 235520, mean_episode_return = 0.36433, mean_episode_step = 72.807, total_loss = 383.99, entropy_loss = -8.1014, pg_loss = 291.41, baseline_loss = 100.68, learner_queue_size = 32, _tick = 91, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:11:46,422[0m][[34mroot[0m][[32mINFO[0m] - Step 238080 @ 511.2 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 926.4, step = 238080, mean_episode_return = 0.40174, mean_episode_step = 69.854, total_loss = 200.15, entropy_loss = -8.1207, pg_loss = 129.09, baseline_loss = 79.186, learner_queue_size = 32, _tick = 92, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:11:51,425[0m][[34mroot[0m][[32mINFO[0m] - Step 238080 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 931.4, step = 238080, mean_episode_return = 0.40174, mean_episode_step = 69.854, total_loss = 200.15, entropy_loss = -8.1207, pg_loss = 129.09, baseline_loss = 79.186, learner_queue_size = 32, _tick = 92, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:11:56,430[0m][[34mroot[0m][[32mINFO[0m] - Step 240640 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 936.4, step = 240640, mean_episode_return = 0.38581, mean_episode_step = 77.821, total_loss = -607.74, entropy_loss = -8.0898, pg_loss = -675.66, baseline_loss = 76.014, learner_queue_size = 32, _tick = 93, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:12:01,436[0m][[34mroot[0m][[32mINFO[0m] - Step 240640 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 941.4, step = 240640, mean_episode_return = 0.38581, mean_episode_step = 77.821, total_loss = -607.74, entropy_loss = -8.0898, pg_loss = -675.66, baseline_loss = 76.014, learner_queue_size = 32, _tick = 93, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:12:06,441[0m][[34mroot[0m][[32mINFO[0m] - Step 243200 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 946.4, step = 243200, mean_episode_return = 0.55808, mean_episode_step = 74.286, total_loss = 736.66, entropy_loss = -7.9974, pg_loss = 647.43, baseline_loss = 97.219, learner_queue_size = 32, _tick = 94, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:12:11,446[0m][[34mroot[0m][[32mINFO[0m] - Step 245760 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 951.4, step = 245760, mean_episode_return = 0.45922, mean_episode_step = 71.377, total_loss = -290.42, entropy_loss = -8.1057, pg_loss = -400.71, baseline_loss = 118.39, learner_queue_size = 32, _tick = 95, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:12:16,451[0m][[34mroot[0m][[32mINFO[0m] - Step 245760 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 956.5, step = 245760, mean_episode_return = 0.45922, mean_episode_step = 71.377, total_loss = -290.42, entropy_loss = -8.1057, pg_loss = -400.71, baseline_loss = 118.39, learner_queue_size = 32, _tick = 95, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:12:21,457[0m][[34mroot[0m][[32mINFO[0m] - Step 248320 @ 511.4 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (train_seconds = 961.5, step = 248320, mean_episode_return = 0.48357, mean_episode_step = 80.137, total_loss = 42.081, entropy_loss = -8.0854, pg_loss = -50.057, baseline_loss = 100.22, learner_queue_size = 32, _tick = 96, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:12:26,463[0m][[34mroot[0m][[32mINFO[0m] - Step 248320 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 966.5, step = 248320, mean_episode_return = 0.48357, mean_episode_step = 80.137, total_loss = 42.081, entropy_loss = -8.0854, pg_loss = -50.057, baseline_loss = 100.22, learner_queue_size = 32, _tick = 96, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:12:31,468[0m][[34mroot[0m][[32mINFO[0m] - Saving checkpoint to /opt/minihack/checkpoint_0.5.tar[0m
[[36m2025-01-18 11:12:31,501[0m][[34mroot[0m][[32mINFO[0m] - Step 250880 @ 511.5 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (train_seconds = 971.5, step = 250880, mean_episode_return = 0.36756, mean_episode_step = 82.107, total_loss = -11.194, entropy_loss = -8.0333, pg_loss = -103.72, baseline_loss = 100.56, learner_queue_size = 32, _tick = 97, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:12:36,507[0m][[34mroot[0m][[32mINFO[0m] - Step 250880 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 976.5, step = 250880, mean_episode_return = 0.36756, mean_episode_step = 82.107, total_loss = -11.194, entropy_loss = -8.0333, pg_loss = -103.72, baseline_loss = 100.56, learner_queue_size = 32, _tick = 97, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:12:41,512[0m][[34mroot[0m][[32mINFO[0m] - Step 253440 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 981.5, step = 253440, mean_episode_return = 0.44312, mean_episode_step = 79.715, total_loss = -48.661, entropy_loss = -8.0596, pg_loss = -125.58, baseline_loss = 84.976, learner_queue_size = 32, _tick = 98, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:12:46,517[0m][[34mroot[0m][[32mINFO[0m] - Step 256000 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 986.5, step = 256000, mean_episode_return = 0.50452, mean_episode_step = 78.421, total_loss = 583.83, entropy_loss = -8.0274, pg_loss = 491.63, baseline_loss = 100.23, learner_queue_size = 32, _tick = 99, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:12:51,523[0m][[34mroot[0m][[32mINFO[0m] - Step 256000 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 991.5, step = 256000, mean_episode_return = 0.50452, mean_episode_step = 78.421, total_loss = 583.83, entropy_loss = -8.0274, pg_loss = 491.63, baseline_loss = 100.23, learner_queue_size = 32, _tick = 99, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:12:56,528[0m][[34mroot[0m][[32mINFO[0m] - Step 258560 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 996.5, step = 258560, mean_episode_return = 0.49694, mean_episode_step = 72.376, total_loss = -88.199, entropy_loss = -8.0273, pg_loss = -180.19, baseline_loss = 100.02, learner_queue_size = 32, _tick = 100, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:13:01,534[0m][[34mroot[0m][[32mINFO[0m] - Step 258560 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1001.5, step = 258560, mean_episode_return = 0.49694, mean_episode_step = 72.376, total_loss = -88.199, entropy_loss = -8.0273, pg_loss = -180.19, baseline_loss = 100.02, learner_queue_size = 32, _tick = 100, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:13:06,539[0m][[34mroot[0m][[32mINFO[0m] - Step 261120 @ 511.5 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 1006.5, step = 261120, mean_episode_return = 0.59862, mean_episode_step = 73.543, total_loss = 257.56, entropy_loss = -7.9747, pg_loss = 143.81, baseline_loss = 121.73, learner_queue_size = 32, _tick = 101, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:13:11,545[0m][[34mroot[0m][[32mINFO[0m] - Step 261120 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1011.5, step = 261120, mean_episode_return = 0.59862, mean_episode_step = 73.543, total_loss = 257.56, entropy_loss = -7.9747, pg_loss = 143.81, baseline_loss = 121.73, learner_queue_size = 32, _tick = 101, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:13:16,550[0m][[34mroot[0m][[32mINFO[0m] - Step 263680 @ 511.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1016.5, step = 263680, mean_episode_return = 0.60433, mean_episode_step = 93.611, total_loss = -493.57, entropy_loss = -7.9574, pg_loss = -585.98, baseline_loss = 100.37, learner_queue_size = 32, _tick = 102, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:13:21,555[0m][[34mroot[0m][[32mINFO[0m] - Step 266240 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1021.6, step = 266240, mean_episode_return = 0.50543, mean_episode_step = 82.401, total_loss = 342.39, entropy_loss = -7.9719, pg_loss = 246.21, baseline_loss = 104.15, learner_queue_size = 32, _tick = 103, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:13:26,561[0m][[34mroot[0m][[32mINFO[0m] - Step 266240 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1026.6, step = 266240, mean_episode_return = 0.50543, mean_episode_step = 82.401, total_loss = 342.39, entropy_loss = -7.9719, pg_loss = 246.21, baseline_loss = 104.15, learner_queue_size = 32, _tick = 103, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:13:31,566[0m][[34mroot[0m][[32mINFO[0m] - Step 268800 @ 511.5 SPS. Inference batcher size: 31. Learner queue size: 32. Other stats: (train_seconds = 1031.6, step = 268800, mean_episode_return = 0.43193, mean_episode_step = 63.773, total_loss = -25.126, entropy_loss = -8.0266, pg_loss = -165.1, baseline_loss = 148.0, learner_queue_size = 32, _tick = 104, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:13:36,571[0m][[34mroot[0m][[32mINFO[0m] - Step 268800 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1036.6, step = 268800, mean_episode_return = 0.43193, mean_episode_step = 63.773, total_loss = -25.126, entropy_loss = -8.0266, pg_loss = -165.1, baseline_loss = 148.0, learner_queue_size = 32, _tick = 104, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:13:41,578[0m][[34mroot[0m][[32mINFO[0m] - Step 271360 @ 511.3 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 1041.6, step = 271360, mean_episode_return = 0.75716, mean_episode_step = 90.859, total_loss = -152.0, entropy_loss = -7.9662, pg_loss = -274.38, baseline_loss = 130.34, learner_queue_size = 32, _tick = 105, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:13:46,583[0m][[34mroot[0m][[32mINFO[0m] - Step 273920 @ 511.5 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 1046.6, step = 273920, mean_episode_return = 0.55108, mean_episode_step = 68.78, total_loss = 76.481, entropy_loss = -8.0013, pg_loss = -44.666, baseline_loss = 129.15, learner_queue_size = 32, _tick = 106, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:13:51,588[0m][[34mroot[0m][[32mINFO[0m] - Step 273920 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1051.6, step = 273920, mean_episode_return = 0.55108, mean_episode_step = 68.78, total_loss = 76.481, entropy_loss = -8.0013, pg_loss = -44.666, baseline_loss = 129.15, learner_queue_size = 32, _tick = 106, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:13:56,594[0m][[34mroot[0m][[32mINFO[0m] - Step 276480 @ 511.4 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 1056.6, step = 276480, mean_episode_return = 0.52887, mean_episode_step = 72.792, total_loss = 578.6, entropy_loss = -7.9523, pg_loss = 464.8, baseline_loss = 121.75, learner_queue_size = 32, _tick = 107, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:14:01,599[0m][[34mroot[0m][[32mINFO[0m] - Step 276480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1061.6, step = 276480, mean_episode_return = 0.52887, mean_episode_step = 72.792, total_loss = 578.6, entropy_loss = -7.9523, pg_loss = 464.8, baseline_loss = 121.75, learner_queue_size = 32, _tick = 107, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:14:06,604[0m][[34mroot[0m][[32mINFO[0m] - Step 279040 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1066.6, step = 279040, mean_episode_return = 0.66674, mean_episode_step = 73.347, total_loss = 42.799, entropy_loss = -7.9685, pg_loss = -46.01, baseline_loss = 96.778, learner_queue_size = 32, _tick = 108, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:14:11,610[0m][[34mroot[0m][[32mINFO[0m] - Step 279040 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1071.6, step = 279040, mean_episode_return = 0.66674, mean_episode_step = 73.347, total_loss = 42.799, entropy_loss = -7.9685, pg_loss = -46.01, baseline_loss = 96.778, learner_queue_size = 32, _tick = 108, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:14:16,615[0m][[34mroot[0m][[32mINFO[0m] - Step 281600 @ 511.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1076.6, step = 281600, mean_episode_return = 0.43197, mean_episode_step = 72.61, total_loss = -418.12, entropy_loss = -8.0301, pg_loss = -504.7, baseline_loss = 94.609, learner_queue_size = 32, _tick = 109, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:14:21,621[0m][[34mroot[0m][[32mINFO[0m] - Step 284160 @ 511.4 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (train_seconds = 1081.6, step = 284160, mean_episode_return = 0.73573, mean_episode_step = 101.96, total_loss = 105.66, entropy_loss = -7.9446, pg_loss = 30.975, baseline_loss = 82.628, learner_queue_size = 32, _tick = 110, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:14:26,627[0m][[34mroot[0m][[32mINFO[0m] - Step 284160 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1086.6, step = 284160, mean_episode_return = 0.73573, mean_episode_step = 101.96, total_loss = 105.66, entropy_loss = -7.9446, pg_loss = 30.975, baseline_loss = 82.628, learner_queue_size = 32, _tick = 110, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:14:31,632[0m][[34mroot[0m][[32mINFO[0m] - Step 286720 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 1091.6, step = 286720, mean_episode_return = 0.41207, mean_episode_step = 79.389, total_loss = 156.33, entropy_loss = -8.0398, pg_loss = 93.436, baseline_loss = 70.929, learner_queue_size = 32, _tick = 111, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:14:36,638[0m][[34mroot[0m][[32mINFO[0m] - Step 286720 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1096.6, step = 286720, mean_episode_return = 0.41207, mean_episode_step = 79.389, total_loss = 156.33, entropy_loss = -8.0398, pg_loss = 93.436, baseline_loss = 70.929, learner_queue_size = 32, _tick = 111, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:14:41,644[0m][[34mroot[0m][[32mINFO[0m] - Step 289280 @ 511.4 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 1101.6, step = 289280, mean_episode_return = 0.54785, mean_episode_step = 80.268, total_loss = -285.87, entropy_loss = -8.0306, pg_loss = -344.46, baseline_loss = 66.617, learner_queue_size = 32, _tick = 112, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:14:46,648[0m][[34mroot[0m][[32mINFO[0m] - Step 289280 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1106.6, step = 289280, mean_episode_return = 0.54785, mean_episode_step = 80.268, total_loss = -285.87, entropy_loss = -8.0306, pg_loss = -344.46, baseline_loss = 66.617, learner_queue_size = 32, _tick = 112, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:14:51,653[0m][[34mroot[0m][[32mINFO[0m] - Step 291840 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1111.7, step = 291840, mean_episode_return = 0.81639, mean_episode_step = 80.686, total_loss = -130.11, entropy_loss = -8.0137, pg_loss = -191.27, baseline_loss = 69.172, learner_queue_size = 32, _tick = 113, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:14:56,658[0m][[34mroot[0m][[32mINFO[0m] - Step 294400 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1116.7, step = 294400, mean_episode_return = 0.40468, mean_episode_step = 73.706, total_loss = -15.219, entropy_loss = -7.9344, pg_loss = -60.03, baseline_loss = 52.745, learner_queue_size = 32, _tick = 114, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:15:01,671[0m][[34mroot[0m][[32mINFO[0m] - Step 294400 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1121.7, step = 294400, mean_episode_return = 0.40468, mean_episode_step = 73.706, total_loss = -15.219, entropy_loss = -7.9344, pg_loss = -60.03, baseline_loss = 52.745, learner_queue_size = 32, _tick = 114, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:15:06,684[0m][[34mroot[0m][[32mINFO[0m] - Step 296960 @ 510.7 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1126.7, step = 296960, mean_episode_return = 0.58392, mean_episode_step = 76.318, total_loss = -85.261, entropy_loss = -7.9493, pg_loss = -133.97, baseline_loss = 56.656, learner_queue_size = 32, _tick = 115, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:15:11,689[0m][[34mroot[0m][[32mINFO[0m] - Step 299520 @ 511.5 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (train_seconds = 1131.7, step = 299520, mean_episode_return = 0.56471, mean_episode_step = 81.033, total_loss = 131.68, entropy_loss = -7.8556, pg_loss = 88.078, baseline_loss = 51.461, learner_queue_size = 32, _tick = 116, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:15:16,694[0m][[34mroot[0m][[32mINFO[0m] - Step 299520 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1136.7, step = 299520, mean_episode_return = 0.56471, mean_episode_step = 81.033, total_loss = 131.68, entropy_loss = -7.8556, pg_loss = 88.078, baseline_loss = 51.461, learner_queue_size = 32, _tick = 116, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:15:21,699[0m][[34mroot[0m][[32mINFO[0m] - Step 302080 @ 511.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 1141.7, step = 302080, mean_episode_return = 0.64009, mean_episode_step = 77.224, total_loss = 182.65, entropy_loss = -7.7606, pg_loss = 141.17, baseline_loss = 49.237, learner_queue_size = 32, _tick = 117, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:15:26,704[0m][[34mroot[0m][[32mINFO[0m] - Step 302080 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1146.7, step = 302080, mean_episode_return = 0.64009, mean_episode_step = 77.224, total_loss = 182.65, entropy_loss = -7.7606, pg_loss = 141.17, baseline_loss = 49.237, learner_queue_size = 32, _tick = 117, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:15:31,709[0m][[34mroot[0m][[32mINFO[0m] - Step 304640 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1151.7, step = 304640, mean_episode_return = 0.63482, mean_episode_step = 80.225, total_loss = -117.7, entropy_loss = -7.7968, pg_loss = -169.93, baseline_loss = 60.029, learner_queue_size = 32, _tick = 118, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:15:36,715[0m][[34mroot[0m][[32mINFO[0m] - Step 304640 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1156.7, step = 304640, mean_episode_return = 0.63482, mean_episode_step = 80.225, total_loss = -117.7, entropy_loss = -7.7968, pg_loss = -169.93, baseline_loss = 60.029, learner_queue_size = 32, _tick = 118, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:15:41,720[0m][[34mroot[0m][[32mINFO[0m] - Step 307200 @ 511.4 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 1161.7, step = 307200, mean_episode_return = 0.55735, mean_episode_step = 77.526, total_loss = -209.84, entropy_loss = -7.8134, pg_loss = -263.58, baseline_loss = 61.557, learner_queue_size = 32, _tick = 119, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:15:46,725[0m][[34mroot[0m][[32mINFO[0m] - Step 307200 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1166.7, step = 307200, mean_episode_return = 0.55735, mean_episode_step = 77.526, total_loss = -209.84, entropy_loss = -7.8134, pg_loss = -263.58, baseline_loss = 61.557, learner_queue_size = 32, _tick = 119, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:15:51,730[0m][[34mroot[0m][[32mINFO[0m] - Step 309760 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 1171.7, step = 309760, mean_episode_return = 0.54454, mean_episode_step = 81.312, total_loss = 185.4, entropy_loss = -7.7987, pg_loss = 127.53, baseline_loss = 65.674, learner_queue_size = 32, _tick = 120, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:15:56,736[0m][[34mroot[0m][[32mINFO[0m] - Step 309760 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1176.7, step = 309760, mean_episode_return = 0.54454, mean_episode_step = 81.312, total_loss = 185.4, entropy_loss = -7.7987, pg_loss = 127.53, baseline_loss = 65.674, learner_queue_size = 32, _tick = 120, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:16:01,742[0m][[34mroot[0m][[32mINFO[0m] - Step 312320 @ 511.4 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 1181.7, step = 312320, mean_episode_return = 0.60921, mean_episode_step = 75.563, total_loss = 386.98, entropy_loss = -7.845, pg_loss = 283.78, baseline_loss = 111.04, learner_queue_size = 32, _tick = 121, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:16:06,748[0m][[34mroot[0m][[32mINFO[0m] - Step 312320 @ 0.0 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (train_seconds = 1186.7, step = 312320, mean_episode_return = 0.60921, mean_episode_step = 75.563, total_loss = 386.98, entropy_loss = -7.845, pg_loss = 283.78, baseline_loss = 111.04, learner_queue_size = 32, _tick = 121, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:16:11,754[0m][[34mroot[0m][[32mINFO[0m] - Step 314880 @ 511.5 SPS. Inference batcher size: 8. Learner queue size: 32. Other stats: (train_seconds = 1191.8, step = 314880, mean_episode_return = 0.59546, mean_episode_step = 78.604, total_loss = 426.11, entropy_loss = -7.8801, pg_loss = 325.64, baseline_loss = 108.35, learner_queue_size = 32, _tick = 122, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:16:16,763[0m][[34mroot[0m][[32mINFO[0m] - Step 314880 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1196.8, step = 314880, mean_episode_return = 0.59546, mean_episode_step = 78.604, total_loss = 426.11, entropy_loss = -7.8801, pg_loss = 325.64, baseline_loss = 108.35, learner_queue_size = 32, _tick = 122, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:16:21,768[0m][[34mroot[0m][[32mINFO[0m] - Saving checkpoint to /opt/minihack/checkpoint.tar[0m
[[36m2025-01-18 11:16:22,197[0m][[34mroot[0m][[32mINFO[0m] - Step 317440 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 1201.8, step = 317440, mean_episode_return = 0.42428, mean_episode_step = 79.656, total_loss = -147.02, entropy_loss = -7.886, pg_loss = -232.84, baseline_loss = 93.709, learner_queue_size = 32, _tick = 123, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:16:27,202[0m][[34mroot[0m][[32mINFO[0m] - Step 317440 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1207.2, step = 317440, mean_episode_return = 0.42428, mean_episode_step = 79.656, total_loss = -147.02, entropy_loss = -7.886, pg_loss = -232.84, baseline_loss = 93.709, learner_queue_size = 32, _tick = 123, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:16:32,207[0m][[34mroot[0m][[32mINFO[0m] - Step 320000 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1212.2, step = 320000, mean_episode_return = 0.59231, mean_episode_step = 81.672, total_loss = -191.07, entropy_loss = -7.9262, pg_loss = -274.73, baseline_loss = 91.588, learner_queue_size = 32, _tick = 124, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:16:37,212[0m][[34mroot[0m][[32mINFO[0m] - Step 320000 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1217.2, step = 320000, mean_episode_return = 0.59231, mean_episode_step = 81.672, total_loss = -191.07, entropy_loss = -7.9262, pg_loss = -274.73, baseline_loss = 91.588, learner_queue_size = 32, _tick = 124, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:16:42,217[0m][[34mroot[0m][[32mINFO[0m] - Step 322560 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1222.2, step = 322560, mean_episode_return = 0.56897, mean_episode_step = 81.585, total_loss = 292.81, entropy_loss = -7.8493, pg_loss = 194.44, baseline_loss = 106.22, learner_queue_size = 32, _tick = 125, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:16:47,223[0m][[34mroot[0m][[32mINFO[0m] - Step 322560 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1227.2, step = 322560, mean_episode_return = 0.56897, mean_episode_step = 81.585, total_loss = 292.81, entropy_loss = -7.8493, pg_loss = 194.44, baseline_loss = 106.22, learner_queue_size = 32, _tick = 125, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:16:52,228[0m][[34mroot[0m][[32mINFO[0m] - Step 325120 @ 511.5 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 1232.2, step = 325120, mean_episode_return = 0.59707, mean_episode_step = 81.961, total_loss = -250.08, entropy_loss = -7.9118, pg_loss = -327.76, baseline_loss = 85.592, learner_queue_size = 32, _tick = 126, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:16:57,234[0m][[34mroot[0m][[32mINFO[0m] - Step 325120 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1237.2, step = 325120, mean_episode_return = 0.59707, mean_episode_step = 81.961, total_loss = -250.08, entropy_loss = -7.9118, pg_loss = -327.76, baseline_loss = 85.592, learner_queue_size = 32, _tick = 126, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:17:02,245[0m][[34mroot[0m][[32mINFO[0m] - Step 327680 @ 510.8 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 1242.2, step = 327680, mean_episode_return = 0.55955, mean_episode_step = 78.727, total_loss = -36.951, entropy_loss = -7.8854, pg_loss = -136.99, baseline_loss = 107.93, learner_queue_size = 32, _tick = 127, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:17:07,252[0m][[34mroot[0m][[32mINFO[0m] - Step 327680 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1247.3, step = 327680, mean_episode_return = 0.55955, mean_episode_step = 78.727, total_loss = -36.951, entropy_loss = -7.8854, pg_loss = -136.99, baseline_loss = 107.93, learner_queue_size = 32, _tick = 127, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:17:12,259[0m][[34mroot[0m][[32mINFO[0m] - Step 330240 @ 511.3 SPS. Inference batcher size: 6. Learner queue size: 32. Other stats: (train_seconds = 1252.3, step = 330240, mean_episode_return = 0.40676, mean_episode_step = 66.995, total_loss = -583.76, entropy_loss = -7.9559, pg_loss = -642.53, baseline_loss = 66.731, learner_queue_size = 32, _tick = 128, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:17:17,266[0m][[34mroot[0m][[32mINFO[0m] - Step 330240 @ 0.0 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 1257.3, step = 330240, mean_episode_return = 0.40676, mean_episode_step = 66.995, total_loss = -583.76, entropy_loss = -7.9559, pg_loss = -642.53, baseline_loss = 66.731, learner_queue_size = 32, _tick = 128, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:17:22,271[0m][[34mroot[0m][[32mINFO[0m] - Step 330240 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1262.3, step = 330240, mean_episode_return = 0.40676, mean_episode_step = 66.995, total_loss = -583.76, entropy_loss = -7.9559, pg_loss = -642.53, baseline_loss = 66.731, learner_queue_size = 32, _tick = 128, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:17:27,277[0m][[34mroot[0m][[32mINFO[0m] - Step 332800 @ 511.4 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1267.3, step = 332800, mean_episode_return = 0.65929, mean_episode_step = 68.795, total_loss = 264.34, entropy_loss = -7.9105, pg_loss = 193.0, baseline_loss = 79.245, learner_queue_size = 32, _tick = 129, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:17:32,282[0m][[34mroot[0m][[32mINFO[0m] - Step 332800 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1272.3, step = 332800, mean_episode_return = 0.65929, mean_episode_step = 68.795, total_loss = 264.34, entropy_loss = -7.9105, pg_loss = 193.0, baseline_loss = 79.245, learner_queue_size = 32, _tick = 129, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:17:37,287[0m][[34mroot[0m][[32mINFO[0m] - Step 335360 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1277.3, step = 335360, mean_episode_return = 0.52, mean_episode_step = 84.701, total_loss = -61.261, entropy_loss = -7.8798, pg_loss = -155.48, baseline_loss = 102.1, learner_queue_size = 32, _tick = 130, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:17:42,293[0m][[34mroot[0m][[32mINFO[0m] - Step 335360 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1282.3, step = 335360, mean_episode_return = 0.52, mean_episode_step = 84.701, total_loss = -61.261, entropy_loss = -7.8798, pg_loss = -155.48, baseline_loss = 102.1, learner_queue_size = 32, _tick = 130, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:17:47,298[0m][[34mroot[0m][[32mINFO[0m] - Step 337920 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1287.3, step = 337920, mean_episode_return = 0.53844, mean_episode_step = 72.178, total_loss = 42.236, entropy_loss = -7.8564, pg_loss = -15.203, baseline_loss = 65.296, learner_queue_size = 32, _tick = 131, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:17:52,303[0m][[34mroot[0m][[32mINFO[0m] - Step 337920 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1292.3, step = 337920, mean_episode_return = 0.53844, mean_episode_step = 72.178, total_loss = 42.236, entropy_loss = -7.8564, pg_loss = -15.203, baseline_loss = 65.296, learner_queue_size = 32, _tick = 131, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:17:57,308[0m][[34mroot[0m][[32mINFO[0m] - Step 340480 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1297.3, step = 340480, mean_episode_return = 0.58236, mean_episode_step = 74.474, total_loss = -418.74, entropy_loss = -7.8715, pg_loss = -478.95, baseline_loss = 68.079, learner_queue_size = 32, _tick = 132, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:18:02,314[0m][[34mroot[0m][[32mINFO[0m] - Step 340480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1302.3, step = 340480, mean_episode_return = 0.58236, mean_episode_step = 74.474, total_loss = -418.74, entropy_loss = -7.8715, pg_loss = -478.95, baseline_loss = 68.079, learner_queue_size = 32, _tick = 132, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:18:07,321[0m][[34mroot[0m][[32mINFO[0m] - Step 343040 @ 511.3 SPS. Inference batcher size: 2. Learner queue size: 32. Other stats: (train_seconds = 1307.3, step = 343040, mean_episode_return = 0.96912, mean_episode_step = 88.876, total_loss = 647.63, entropy_loss = -7.7697, pg_loss = 562.45, baseline_loss = 92.949, learner_queue_size = 32, _tick = 133, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:18:12,327[0m][[34mroot[0m][[32mINFO[0m] - Step 343040 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1312.3, step = 343040, mean_episode_return = 0.96912, mean_episode_step = 88.876, total_loss = 647.63, entropy_loss = -7.7697, pg_loss = 562.45, baseline_loss = 92.949, learner_queue_size = 32, _tick = 133, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:18:17,332[0m][[34mroot[0m][[32mINFO[0m] - Step 345600 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1317.3, step = 345600, mean_episode_return = 0.53469, mean_episode_step = 71.464, total_loss = 95.944, entropy_loss = -7.8643, pg_loss = 12.518, baseline_loss = 91.291, learner_queue_size = 32, _tick = 134, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:18:22,337[0m][[34mroot[0m][[32mINFO[0m] - Step 345600 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1322.3, step = 345600, mean_episode_return = 0.53469, mean_episode_step = 71.464, total_loss = 95.944, entropy_loss = -7.8643, pg_loss = 12.518, baseline_loss = 91.291, learner_queue_size = 32, _tick = 134, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:18:27,342[0m][[34mroot[0m][[32mINFO[0m] - Step 348160 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1327.3, step = 348160, mean_episode_return = 0.56183, mean_episode_step = 88.143, total_loss = 376.13, entropy_loss = -7.8552, pg_loss = 288.99, baseline_loss = 94.995, learner_queue_size = 32, _tick = 135, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:18:32,347[0m][[34mroot[0m][[32mINFO[0m] - Step 348160 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1332.3, step = 348160, mean_episode_return = 0.56183, mean_episode_step = 88.143, total_loss = 376.13, entropy_loss = -7.8552, pg_loss = 288.99, baseline_loss = 94.995, learner_queue_size = 32, _tick = 135, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:18:37,352[0m][[34mroot[0m][[32mINFO[0m] - Step 350720 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1337.4, step = 350720, mean_episode_return = 0.60128, mean_episode_step = 65.093, total_loss = 826.77, entropy_loss = -7.9306, pg_loss = 689.36, baseline_loss = 145.34, learner_queue_size = 32, _tick = 136, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:18:42,357[0m][[34mroot[0m][[32mINFO[0m] - Step 350720 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1342.4, step = 350720, mean_episode_return = 0.60128, mean_episode_step = 65.093, total_loss = 826.77, entropy_loss = -7.9306, pg_loss = 689.36, baseline_loss = 145.34, learner_queue_size = 32, _tick = 136, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:18:47,362[0m][[34mroot[0m][[32mINFO[0m] - Step 353280 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 1347.4, step = 353280, mean_episode_return = 0.54191, mean_episode_step = 70.616, total_loss = -352.07, entropy_loss = -7.9631, pg_loss = -421.37, baseline_loss = 77.254, learner_queue_size = 32, _tick = 137, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:18:52,367[0m][[34mroot[0m][[32mINFO[0m] - Step 353280 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1352.4, step = 353280, mean_episode_return = 0.54191, mean_episode_step = 70.616, total_loss = -352.07, entropy_loss = -7.9631, pg_loss = -421.37, baseline_loss = 77.254, learner_queue_size = 32, _tick = 137, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:18:57,372[0m][[34mroot[0m][[32mINFO[0m] - Step 355840 @ 511.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 1357.4, step = 355840, mean_episode_return = 0.43412, mean_episode_step = 79.684, total_loss = -91.547, entropy_loss = -7.9287, pg_loss = -168.49, baseline_loss = 84.874, learner_queue_size = 32, _tick = 138, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:19:02,378[0m][[34mroot[0m][[32mINFO[0m] - Step 355840 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1362.4, step = 355840, mean_episode_return = 0.43412, mean_episode_step = 79.684, total_loss = -91.547, entropy_loss = -7.9287, pg_loss = -168.49, baseline_loss = 84.874, learner_queue_size = 32, _tick = 138, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:19:07,383[0m][[34mroot[0m][[32mINFO[0m] - Step 358400 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1367.4, step = 358400, mean_episode_return = 0.56497, mean_episode_step = 85.566, total_loss = 211.37, entropy_loss = -7.9152, pg_loss = 109.65, baseline_loss = 109.64, learner_queue_size = 32, _tick = 139, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:19:12,390[0m][[34mroot[0m][[32mINFO[0m] - Step 358400 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1372.4, step = 358400, mean_episode_return = 0.56497, mean_episode_step = 85.566, total_loss = 211.37, entropy_loss = -7.9152, pg_loss = 109.65, baseline_loss = 109.64, learner_queue_size = 32, _tick = 139, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:19:17,395[0m][[34mroot[0m][[32mINFO[0m] - Step 360960 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1377.4, step = 360960, mean_episode_return = 0.74407, mean_episode_step = 87.548, total_loss = 173.09, entropy_loss = -7.9225, pg_loss = 65.912, baseline_loss = 115.1, learner_queue_size = 32, _tick = 140, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:19:22,400[0m][[34mroot[0m][[32mINFO[0m] - Step 363520 @ 511.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1382.4, step = 363520, mean_episode_return = 0.60262, mean_episode_step = 72.398, total_loss = 89.818, entropy_loss = -7.9605, pg_loss = 0.31519, baseline_loss = 97.464, learner_queue_size = 32, _tick = 141, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:19:27,408[0m][[34mroot[0m][[32mINFO[0m] - Step 363520 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1387.4, step = 363520, mean_episode_return = 0.60262, mean_episode_step = 72.398, total_loss = 89.818, entropy_loss = -7.9605, pg_loss = 0.31519, baseline_loss = 97.464, learner_queue_size = 32, _tick = 141, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:19:32,414[0m][[34mroot[0m][[32mINFO[0m] - Step 366080 @ 511.4 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 1392.4, step = 366080, mean_episode_return = 0.79906, mean_episode_step = 85.981, total_loss = -323.33, entropy_loss = -7.8769, pg_loss = -407.19, baseline_loss = 91.738, learner_queue_size = 32, _tick = 142, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:19:37,419[0m][[34mroot[0m][[32mINFO[0m] - Step 366080 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1397.4, step = 366080, mean_episode_return = 0.79906, mean_episode_step = 85.981, total_loss = -323.33, entropy_loss = -7.8769, pg_loss = -407.19, baseline_loss = 91.738, learner_queue_size = 32, _tick = 142, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:19:42,424[0m][[34mroot[0m][[32mINFO[0m] - Step 368640 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1402.4, step = 368640, mean_episode_return = 0.48964, mean_episode_step = 79.084, total_loss = 218.44, entropy_loss = -7.8894, pg_loss = 120.93, baseline_loss = 105.4, learner_queue_size = 32, _tick = 143, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:19:47,430[0m][[34mroot[0m][[32mINFO[0m] - Step 368640 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1407.4, step = 368640, mean_episode_return = 0.48964, mean_episode_step = 79.084, total_loss = 218.44, entropy_loss = -7.8894, pg_loss = 120.93, baseline_loss = 105.4, learner_queue_size = 32, _tick = 143, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:19:52,436[0m][[34mroot[0m][[32mINFO[0m] - Step 371200 @ 511.4 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 1412.4, step = 371200, mean_episode_return = 0.48754, mean_episode_step = 73.416, total_loss = -466.35, entropy_loss = -7.8944, pg_loss = -530.19, baseline_loss = 71.734, learner_queue_size = 32, _tick = 144, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:19:57,441[0m][[34mroot[0m][[32mINFO[0m] - Step 371200 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1417.4, step = 371200, mean_episode_return = 0.48754, mean_episode_step = 73.416, total_loss = -466.35, entropy_loss = -7.8944, pg_loss = -530.19, baseline_loss = 71.734, learner_queue_size = 32, _tick = 144, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:20:02,446[0m][[34mroot[0m][[32mINFO[0m] - Step 373760 @ 511.5 SPS. Inference batcher size: 14. Learner queue size: 32. Other stats: (train_seconds = 1422.4, step = 373760, mean_episode_return = 0.71513, mean_episode_step = 75.07, total_loss = 117.7, entropy_loss = -7.8506, pg_loss = 53.13, baseline_loss = 72.423, learner_queue_size = 32, _tick = 145, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:20:07,452[0m][[34mroot[0m][[32mINFO[0m] - Step 373760 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1427.5, step = 373760, mean_episode_return = 0.71513, mean_episode_step = 75.07, total_loss = 117.7, entropy_loss = -7.8506, pg_loss = 53.13, baseline_loss = 72.423, learner_queue_size = 32, _tick = 145, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:20:12,457[0m][[34mroot[0m][[32mINFO[0m] - Saving checkpoint to /opt/minihack/checkpoint_0.75.tar[0m
[[36m2025-01-18 11:20:12,504[0m][[34mroot[0m][[32mINFO[0m] - Step 376320 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1432.5, step = 376320, mean_episode_return = 0.5728, mean_episode_step = 81.737, total_loss = -596.9, entropy_loss = -7.9479, pg_loss = -667.91, baseline_loss = 78.958, learner_queue_size = 32, _tick = 146, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:20:17,509[0m][[34mroot[0m][[32mINFO[0m] - Step 376320 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1437.5, step = 376320, mean_episode_return = 0.5728, mean_episode_step = 81.737, total_loss = -596.9, entropy_loss = -7.9479, pg_loss = -667.91, baseline_loss = 78.958, learner_queue_size = 32, _tick = 146, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:20:22,514[0m][[34mroot[0m][[32mINFO[0m] - Step 378880 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 1442.5, step = 378880, mean_episode_return = 0.58739, mean_episode_step = 74.326, total_loss = -414.16, entropy_loss = -7.8934, pg_loss = -483.62, baseline_loss = 77.352, learner_queue_size = 32, _tick = 147, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:20:27,521[0m][[34mroot[0m][[32mINFO[0m] - Step 378880 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1447.5, step = 378880, mean_episode_return = 0.58739, mean_episode_step = 74.326, total_loss = -414.16, entropy_loss = -7.8934, pg_loss = -483.62, baseline_loss = 77.352, learner_queue_size = 32, _tick = 147, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:20:32,527[0m][[34mroot[0m][[32mINFO[0m] - Step 381440 @ 511.4 SPS. Inference batcher size: 1. Learner queue size: 32. Other stats: (train_seconds = 1452.5, step = 381440, mean_episode_return = 0.47476, mean_episode_step = 73.157, total_loss = 791.45, entropy_loss = -7.8877, pg_loss = 678.55, baseline_loss = 120.79, learner_queue_size = 32, _tick = 148, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:20:37,532[0m][[34mroot[0m][[32mINFO[0m] - Step 384000 @ 511.5 SPS. Inference batcher size: 7. Learner queue size: 32. Other stats: (train_seconds = 1457.5, step = 384000, mean_episode_return = 0.96223, mean_episode_step = 99.741, total_loss = 388.48, entropy_loss = -7.8365, pg_loss = 310.68, baseline_loss = 85.641, learner_queue_size = 32, _tick = 149, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:20:42,537[0m][[34mroot[0m][[32mINFO[0m] - Step 384000 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1462.5, step = 384000, mean_episode_return = 0.96223, mean_episode_step = 99.741, total_loss = 388.48, entropy_loss = -7.8365, pg_loss = 310.68, baseline_loss = 85.641, learner_queue_size = 32, _tick = 149, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:20:47,542[0m][[34mroot[0m][[32mINFO[0m] - Step 386560 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1467.5, step = 386560, mean_episode_return = 0.51071, mean_episode_step = 76.454, total_loss = 15.825, entropy_loss = -7.9566, pg_loss = -80.346, baseline_loss = 104.13, learner_queue_size = 32, _tick = 150, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:20:52,547[0m][[34mroot[0m][[32mINFO[0m] - Step 386560 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1472.5, step = 386560, mean_episode_return = 0.51071, mean_episode_step = 76.454, total_loss = 15.825, entropy_loss = -7.9566, pg_loss = -80.346, baseline_loss = 104.13, learner_queue_size = 32, _tick = 150, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:20:57,553[0m][[34mroot[0m][[32mINFO[0m] - Step 389120 @ 511.4 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1477.6, step = 389120, mean_episode_return = 0.57483, mean_episode_step = 84.685, total_loss = -190.68, entropy_loss = -7.865, pg_loss = -259.24, baseline_loss = 76.419, learner_queue_size = 32, _tick = 151, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:21:02,558[0m][[34mroot[0m][[32mINFO[0m] - Step 389120 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1482.6, step = 389120, mean_episode_return = 0.57483, mean_episode_step = 84.685, total_loss = -190.68, entropy_loss = -7.865, pg_loss = -259.24, baseline_loss = 76.419, learner_queue_size = 32, _tick = 151, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:21:07,564[0m][[34mroot[0m][[32mINFO[0m] - Step 391680 @ 511.4 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (train_seconds = 1487.6, step = 391680, mean_episode_return = 0.47563, mean_episode_step = 77.378, total_loss = 268.86, entropy_loss = -7.8731, pg_loss = 183.34, baseline_loss = 93.391, learner_queue_size = 32, _tick = 152, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:21:12,569[0m][[34mroot[0m][[32mINFO[0m] - Step 391680 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1492.6, step = 391680, mean_episode_return = 0.47563, mean_episode_step = 77.378, total_loss = 268.86, entropy_loss = -7.8731, pg_loss = 183.34, baseline_loss = 93.391, learner_queue_size = 32, _tick = 152, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:21:17,574[0m][[34mroot[0m][[32mINFO[0m] - Step 394240 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1497.6, step = 394240, mean_episode_return = 0.45825, mean_episode_step = 80.145, total_loss = -218.0, entropy_loss = -7.9249, pg_loss = -319.72, baseline_loss = 109.65, learner_queue_size = 32, _tick = 153, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:21:22,579[0m][[34mroot[0m][[32mINFO[0m] - Step 394240 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1502.6, step = 394240, mean_episode_return = 0.45825, mean_episode_step = 80.145, total_loss = -218.0, entropy_loss = -7.9249, pg_loss = -319.72, baseline_loss = 109.65, learner_queue_size = 32, _tick = 153, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:21:27,584[0m][[34mroot[0m][[32mINFO[0m] - Step 396800 @ 511.4 SPS. Inference batcher size: 25. Learner queue size: 32. Other stats: (train_seconds = 1507.6, step = 396800, mean_episode_return = 0.72715, mean_episode_step = 80.626, total_loss = -530.05, entropy_loss = -7.9532, pg_loss = -580.89, baseline_loss = 58.786, learner_queue_size = 32, _tick = 154, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:21:32,590[0m][[34mroot[0m][[32mINFO[0m] - Step 396800 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1512.6, step = 396800, mean_episode_return = 0.72715, mean_episode_step = 80.626, total_loss = -530.05, entropy_loss = -7.9532, pg_loss = -580.89, baseline_loss = 58.786, learner_queue_size = 32, _tick = 154, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:21:37,595[0m][[34mroot[0m][[32mINFO[0m] - Step 399360 @ 511.5 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (train_seconds = 1517.6, step = 399360, mean_episode_return = 0.74781, mean_episode_step = 80.917, total_loss = 311.39, entropy_loss = -7.985, pg_loss = 258.07, baseline_loss = 61.3, learner_queue_size = 32, _tick = 155, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:21:42,600[0m][[34mroot[0m][[32mINFO[0m] - Step 399360 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1522.6, step = 399360, mean_episode_return = 0.74781, mean_episode_step = 80.917, total_loss = 311.39, entropy_loss = -7.985, pg_loss = 258.07, baseline_loss = 61.3, learner_queue_size = 32, _tick = 155, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:21:47,607[0m][[34mroot[0m][[32mINFO[0m] - Step 401920 @ 511.3 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 1527.6, step = 401920, mean_episode_return = 0.45878, mean_episode_step = 80.263, total_loss = -215.76, entropy_loss = -8.0016, pg_loss = -253.94, baseline_loss = 46.189, learner_queue_size = 32, _tick = 156, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:21:52,612[0m][[34mroot[0m][[32mINFO[0m] - Step 401920 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1532.6, step = 401920, mean_episode_return = 0.45878, mean_episode_step = 80.263, total_loss = -215.76, entropy_loss = -8.0016, pg_loss = -253.94, baseline_loss = 46.189, learner_queue_size = 32, _tick = 156, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:21:57,618[0m][[34mroot[0m][[32mINFO[0m] - Step 404480 @ 511.4 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 1537.6, step = 404480, mean_episode_return = 0.51016, mean_episode_step = 75.02, total_loss = 467.94, entropy_loss = -7.9682, pg_loss = 402.02, baseline_loss = 73.889, learner_queue_size = 32, _tick = 157, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:22:02,623[0m][[34mroot[0m][[32mINFO[0m] - Step 404480 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1542.6, step = 404480, mean_episode_return = 0.51016, mean_episode_step = 75.02, total_loss = 467.94, entropy_loss = -7.9682, pg_loss = 402.02, baseline_loss = 73.889, learner_queue_size = 32, _tick = 157, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:22:07,629[0m][[34mroot[0m][[32mINFO[0m] - Step 407040 @ 511.4 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1547.6, step = 407040, mean_episode_return = 0.32723, mean_episode_step = 70.634, total_loss = 82.951, entropy_loss = -8.0591, pg_loss = -10.938, baseline_loss = 101.95, learner_queue_size = 32, _tick = 158, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:22:12,635[0m][[34mroot[0m][[32mINFO[0m] - Step 407040 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1552.6, step = 407040, mean_episode_return = 0.32723, mean_episode_step = 70.634, total_loss = 82.951, entropy_loss = -8.0591, pg_loss = -10.938, baseline_loss = 101.95, learner_queue_size = 32, _tick = 158, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:22:17,640[0m][[34mroot[0m][[32mINFO[0m] - Step 409600 @ 511.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 1557.6, step = 409600, mean_episode_return = 0.75809, mean_episode_step = 97.943, total_loss = 5.7844, entropy_loss = -7.969, pg_loss = -47.398, baseline_loss = 61.151, learner_queue_size = 32, _tick = 159, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:22:22,645[0m][[34mroot[0m][[32mINFO[0m] - Step 409600 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1562.6, step = 409600, mean_episode_return = 0.75809, mean_episode_step = 97.943, total_loss = 5.7844, entropy_loss = -7.969, pg_loss = -47.398, baseline_loss = 61.151, learner_queue_size = 32, _tick = 159, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:22:27,650[0m][[34mroot[0m][[32mINFO[0m] - Step 412160 @ 511.5 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 1567.6, step = 412160, mean_episode_return = 0.60381, mean_episode_step = 73.771, total_loss = -48.471, entropy_loss = -7.9925, pg_loss = -112.44, baseline_loss = 71.966, learner_queue_size = 32, _tick = 160, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:22:32,655[0m][[34mroot[0m][[32mINFO[0m] - Step 412160 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1572.7, step = 412160, mean_episode_return = 0.60381, mean_episode_step = 73.771, total_loss = -48.471, entropy_loss = -7.9925, pg_loss = -112.44, baseline_loss = 71.966, learner_queue_size = 32, _tick = 160, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:22:37,660[0m][[34mroot[0m][[32mINFO[0m] - Step 414720 @ 511.5 SPS. Inference batcher size: 13. Learner queue size: 32. Other stats: (train_seconds = 1577.7, step = 414720, mean_episode_return = 0.4948, mean_episode_step = 69.794, total_loss = -343.28, entropy_loss = -7.967, pg_loss = -416.83, baseline_loss = 81.509, learner_queue_size = 32, _tick = 161, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:22:42,666[0m][[34mroot[0m][[32mINFO[0m] - Step 414720 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1582.7, step = 414720, mean_episode_return = 0.4948, mean_episode_step = 69.794, total_loss = -343.28, entropy_loss = -7.967, pg_loss = -416.83, baseline_loss = 81.509, learner_queue_size = 32, _tick = 161, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:22:47,671[0m][[34mroot[0m][[32mINFO[0m] - Step 417280 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1587.7, step = 417280, mean_episode_return = 0.60053, mean_episode_step = 91.254, total_loss = -49.087, entropy_loss = -7.9087, pg_loss = -98.857, baseline_loss = 57.678, learner_queue_size = 32, _tick = 162, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:22:52,676[0m][[34mroot[0m][[32mINFO[0m] - Step 419840 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1592.7, step = 419840, mean_episode_return = 0.70955, mean_episode_step = 86.776, total_loss = 282.17, entropy_loss = -7.9071, pg_loss = 224.81, baseline_loss = 65.267, learner_queue_size = 32, _tick = 163, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:22:57,681[0m][[34mroot[0m][[32mINFO[0m] - Step 419840 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1597.7, step = 419840, mean_episode_return = 0.70955, mean_episode_step = 86.776, total_loss = 282.17, entropy_loss = -7.9071, pg_loss = 224.81, baseline_loss = 65.267, learner_queue_size = 32, _tick = 163, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:23:02,687[0m][[34mroot[0m][[32mINFO[0m] - Step 422400 @ 511.4 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 1602.7, step = 422400, mean_episode_return = 0.45852, mean_episode_step = 79.742, total_loss = -186.63, entropy_loss = -7.8777, pg_loss = -243.42, baseline_loss = 64.673, learner_queue_size = 32, _tick = 164, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:23:07,692[0m][[34mroot[0m][[32mINFO[0m] - Step 422400 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1607.7, step = 422400, mean_episode_return = 0.45852, mean_episode_step = 79.742, total_loss = -186.63, entropy_loss = -7.8777, pg_loss = -243.42, baseline_loss = 64.673, learner_queue_size = 32, _tick = 164, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:23:12,697[0m][[34mroot[0m][[32mINFO[0m] - Step 424960 @ 511.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 1612.7, step = 424960, mean_episode_return = 0.69196, mean_episode_step = 84.085, total_loss = -139.77, entropy_loss = -7.889, pg_loss = -186.82, baseline_loss = 54.934, learner_queue_size = 32, _tick = 165, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:23:17,703[0m][[34mroot[0m][[32mINFO[0m] - Step 424960 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1617.7, step = 424960, mean_episode_return = 0.69196, mean_episode_step = 84.085, total_loss = -139.77, entropy_loss = -7.889, pg_loss = -186.82, baseline_loss = 54.934, learner_queue_size = 32, _tick = 165, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:23:22,708[0m][[34mroot[0m][[32mINFO[0m] - Step 427520 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1622.7, step = 427520, mean_episode_return = 0.39624, mean_episode_step = 81.009, total_loss = 195.05, entropy_loss = -7.8715, pg_loss = 132.95, baseline_loss = 69.973, learner_queue_size = 32, _tick = 166, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:23:27,713[0m][[34mroot[0m][[32mINFO[0m] - Step 427520 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1627.7, step = 427520, mean_episode_return = 0.39624, mean_episode_step = 81.009, total_loss = 195.05, entropy_loss = -7.8715, pg_loss = 132.95, baseline_loss = 69.973, learner_queue_size = 32, _tick = 166, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:23:32,718[0m][[34mroot[0m][[32mINFO[0m] - Step 430080 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1632.7, step = 430080, mean_episode_return = 0.77611, mean_episode_step = 69.505, total_loss = 631.52, entropy_loss = -7.8706, pg_loss = 548.63, baseline_loss = 90.754, learner_queue_size = 32, _tick = 167, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:23:37,723[0m][[34mroot[0m][[32mINFO[0m] - Step 432640 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 1637.7, step = 432640, mean_episode_return = 0.74703, mean_episode_step = 83.8, total_loss = 47.352, entropy_loss = -7.8456, pg_loss = -17.741, baseline_loss = 72.938, learner_queue_size = 32, _tick = 168, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:23:42,726[0m][[34mroot[0m][[32mINFO[0m] - Step 432640 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1642.7, step = 432640, mean_episode_return = 0.74703, mean_episode_step = 83.8, total_loss = 47.352, entropy_loss = -7.8456, pg_loss = -17.741, baseline_loss = 72.938, learner_queue_size = 32, _tick = 168, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:23:47,733[0m][[34mroot[0m][[32mINFO[0m] - Step 435200 @ 511.3 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1647.7, step = 435200, mean_episode_return = 0.56105, mean_episode_step = 79.86, total_loss = -237.23, entropy_loss = -7.9191, pg_loss = -293.88, baseline_loss = 64.566, learner_queue_size = 32, _tick = 169, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:23:52,738[0m][[34mroot[0m][[32mINFO[0m] - Step 435200 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1652.7, step = 435200, mean_episode_return = 0.56105, mean_episode_step = 79.86, total_loss = -237.23, entropy_loss = -7.9191, pg_loss = -293.88, baseline_loss = 64.566, learner_queue_size = 32, _tick = 169, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:23:57,743[0m][[34mroot[0m][[32mINFO[0m] - Step 437760 @ 511.5 SPS. Inference batcher size: 20. Learner queue size: 32. Other stats: (train_seconds = 1657.7, step = 437760, mean_episode_return = 0.61221, mean_episode_step = 90.423, total_loss = 69.266, entropy_loss = -7.8697, pg_loss = 16.338, baseline_loss = 60.798, learner_queue_size = 32, _tick = 170, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:24:02,748[0m][[34mroot[0m][[32mINFO[0m] - Step 437760 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1662.7, step = 437760, mean_episode_return = 0.61221, mean_episode_step = 90.423, total_loss = 69.266, entropy_loss = -7.8697, pg_loss = 16.338, baseline_loss = 60.798, learner_queue_size = 32, _tick = 170, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:24:07,755[0m][[34mroot[0m][[32mINFO[0m] - Step 440320 @ 511.3 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1667.8, step = 440320, mean_episode_return = 0.7826, mean_episode_step = 83.589, total_loss = 331.26, entropy_loss = -7.8775, pg_loss = 272.09, baseline_loss = 67.048, learner_queue_size = 32, _tick = 171, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:24:12,760[0m][[34mroot[0m][[32mINFO[0m] - Step 440320 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1672.8, step = 440320, mean_episode_return = 0.7826, mean_episode_step = 83.589, total_loss = 331.26, entropy_loss = -7.8775, pg_loss = 272.09, baseline_loss = 67.048, learner_queue_size = 32, _tick = 171, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:24:17,765[0m][[34mroot[0m][[32mINFO[0m] - Step 442880 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1677.8, step = 442880, mean_episode_return = 0.85593, mean_episode_step = 85.821, total_loss = -249.77, entropy_loss = -7.8641, pg_loss = -310.05, baseline_loss = 68.143, learner_queue_size = 32, _tick = 172, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:24:22,772[0m][[34mroot[0m][[32mINFO[0m] - Step 445440 @ 511.3 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 1682.8, step = 445440, mean_episode_return = 0.80062, mean_episode_step = 90.505, total_loss = -379.29, entropy_loss = -7.845, pg_loss = -431.27, baseline_loss = 59.826, learner_queue_size = 32, _tick = 173, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:24:27,777[0m][[34mroot[0m][[32mINFO[0m] - Step 445440 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1687.8, step = 445440, mean_episode_return = 0.80062, mean_episode_step = 90.505, total_loss = -379.29, entropy_loss = -7.845, pg_loss = -431.27, baseline_loss = 59.826, learner_queue_size = 32, _tick = 173, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:24:32,782[0m][[34mroot[0m][[32mINFO[0m] - Step 448000 @ 511.5 SPS. Inference batcher size: 9. Learner queue size: 32. Other stats: (train_seconds = 1692.8, step = 448000, mean_episode_return = 0.63983, mean_episode_step = 80.499, total_loss = -176.71, entropy_loss = -7.8517, pg_loss = -224.95, baseline_loss = 56.083, learner_queue_size = 32, _tick = 174, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:24:37,787[0m][[34mroot[0m][[32mINFO[0m] - Step 448000 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1697.8, step = 448000, mean_episode_return = 0.63983, mean_episode_step = 80.499, total_loss = -176.71, entropy_loss = -7.8517, pg_loss = -224.95, baseline_loss = 56.083, learner_queue_size = 32, _tick = 174, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:24:42,792[0m][[34mroot[0m][[32mINFO[0m] - Step 450560 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 1702.8, step = 450560, mean_episode_return = 0.49452, mean_episode_step = 84.629, total_loss = 156.38, entropy_loss = -7.7872, pg_loss = 109.27, baseline_loss = 54.901, learner_queue_size = 32, _tick = 175, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:24:47,797[0m][[34mroot[0m][[32mINFO[0m] - Step 450560 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1707.8, step = 450560, mean_episode_return = 0.49452, mean_episode_step = 84.629, total_loss = 156.38, entropy_loss = -7.7872, pg_loss = 109.27, baseline_loss = 54.901, learner_queue_size = 32, _tick = 175, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:24:52,804[0m][[34mroot[0m][[32mINFO[0m] - Step 453120 @ 511.3 SPS. Inference batcher size: 12. Learner queue size: 32. Other stats: (train_seconds = 1712.8, step = 453120, mean_episode_return = 0.60554, mean_episode_step = 82.286, total_loss = 157.74, entropy_loss = -7.8253, pg_loss = 85.852, baseline_loss = 79.714, learner_queue_size = 32, _tick = 176, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:24:57,810[0m][[34mroot[0m][[32mINFO[0m] - Step 453120 @ 0.0 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1717.8, step = 453120, mean_episode_return = 0.60554, mean_episode_step = 82.286, total_loss = 157.74, entropy_loss = -7.8253, pg_loss = 85.852, baseline_loss = 79.714, learner_queue_size = 32, _tick = 176, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:25:02,815[0m][[34mroot[0m][[32mINFO[0m] - Step 453120 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1722.8, step = 453120, mean_episode_return = 0.60554, mean_episode_step = 82.286, total_loss = 157.74, entropy_loss = -7.8253, pg_loss = 85.852, baseline_loss = 79.714, learner_queue_size = 32, _tick = 176, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:25:07,820[0m][[34mroot[0m][[32mINFO[0m] - Step 455680 @ 511.5 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (train_seconds = 1727.8, step = 455680, mean_episode_return = 0.62597, mean_episode_step = 67.573, total_loss = 368.69, entropy_loss = -7.8948, pg_loss = 257.0, baseline_loss = 119.59, learner_queue_size = 32, _tick = 177, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:25:12,825[0m][[34mroot[0m][[32mINFO[0m] - Step 455680 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1732.8, step = 455680, mean_episode_return = 0.62597, mean_episode_step = 67.573, total_loss = 368.69, entropy_loss = -7.8948, pg_loss = 257.0, baseline_loss = 119.59, learner_queue_size = 32, _tick = 177, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:25:17,830[0m][[34mroot[0m][[32mINFO[0m] - Step 458240 @ 511.5 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1737.8, step = 458240, mean_episode_return = 0.3303, mean_episode_step = 65.463, total_loss = -552.83, entropy_loss = -7.9244, pg_loss = -610.73, baseline_loss = 65.821, learner_queue_size = 32, _tick = 178, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:25:22,836[0m][[34mroot[0m][[32mINFO[0m] - Step 458240 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1742.8, step = 458240, mean_episode_return = 0.3303, mean_episode_step = 65.463, total_loss = -552.83, entropy_loss = -7.9244, pg_loss = -610.73, baseline_loss = 65.821, learner_queue_size = 32, _tick = 178, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:25:27,842[0m][[34mroot[0m][[32mINFO[0m] - Step 460800 @ 511.4 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1747.8, step = 460800, mean_episode_return = 0.74211, mean_episode_step = 86.539, total_loss = 25.697, entropy_loss = -7.8441, pg_loss = -28.555, baseline_loss = 62.097, learner_queue_size = 32, _tick = 179, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:25:32,847[0m][[34mroot[0m][[32mINFO[0m] - Step 460800 @ 0.0 SPS. Inference batcher size: 15. Learner queue size: 32. Other stats: (train_seconds = 1752.8, step = 460800, mean_episode_return = 0.74211, mean_episode_step = 86.539, total_loss = 25.697, entropy_loss = -7.8441, pg_loss = -28.555, baseline_loss = 62.097, learner_queue_size = 32, _tick = 179, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:25:37,851[0m][[34mroot[0m][[32mINFO[0m] - Step 460800 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1757.9, step = 460800, mean_episode_return = 0.74211, mean_episode_step = 86.539, total_loss = 25.697, entropy_loss = -7.8441, pg_loss = -28.555, baseline_loss = 62.097, learner_queue_size = 32, _tick = 179, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:25:42,856[0m][[34mroot[0m][[32mINFO[0m] - Step 463360 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 1762.9, step = 463360, mean_episode_return = 0.24519, mean_episode_step = 74.005, total_loss = -137.2, entropy_loss = -7.8586, pg_loss = -207.23, baseline_loss = 77.889, learner_queue_size = 32, _tick = 180, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:25:47,861[0m][[34mroot[0m][[32mINFO[0m] - Step 463360 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1767.9, step = 463360, mean_episode_return = 0.24519, mean_episode_step = 74.005, total_loss = -137.2, entropy_loss = -7.8586, pg_loss = -207.23, baseline_loss = 77.889, learner_queue_size = 32, _tick = 180, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:25:52,867[0m][[34mroot[0m][[32mINFO[0m] - Step 463360 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1772.9, step = 463360, mean_episode_return = 0.24519, mean_episode_step = 74.005, total_loss = -137.2, entropy_loss = -7.8586, pg_loss = -207.23, baseline_loss = 77.889, learner_queue_size = 32, _tick = 180, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:25:57,872[0m][[34mroot[0m][[32mINFO[0m] - Step 465920 @ 511.5 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 1777.9, step = 465920, mean_episode_return = 0.50379, mean_episode_step = 80.463, total_loss = -189.53, entropy_loss = -7.8191, pg_loss = -241.09, baseline_loss = 59.384, learner_queue_size = 32, _tick = 181, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:26:02,877[0m][[34mroot[0m][[32mINFO[0m] - Step 465920 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1782.9, step = 465920, mean_episode_return = 0.50379, mean_episode_step = 80.463, total_loss = -189.53, entropy_loss = -7.8191, pg_loss = -241.09, baseline_loss = 59.384, learner_queue_size = 32, _tick = 181, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:26:07,882[0m][[34mroot[0m][[32mINFO[0m] - Step 468480 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1787.9, step = 468480, mean_episode_return = 0.61767, mean_episode_step = 81.624, total_loss = -436.19, entropy_loss = -7.8038, pg_loss = -471.86, baseline_loss = 43.466, learner_queue_size = 32, _tick = 182, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:26:12,887[0m][[34mroot[0m][[32mINFO[0m] - Step 471040 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1792.9, step = 471040, mean_episode_return = 0.5778, mean_episode_step = 82.133, total_loss = -79.623, entropy_loss = -7.7716, pg_loss = -123.05, baseline_loss = 51.196, learner_queue_size = 32, _tick = 183, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:26:17,892[0m][[34mroot[0m][[32mINFO[0m] - Step 471040 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1797.9, step = 471040, mean_episode_return = 0.5778, mean_episode_step = 82.133, total_loss = -79.623, entropy_loss = -7.7716, pg_loss = -123.05, baseline_loss = 51.196, learner_queue_size = 32, _tick = 183, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:26:22,898[0m][[34mroot[0m][[32mINFO[0m] - Saving checkpoint to /opt/minihack/checkpoint.tar[0m
[[36m2025-01-18 11:26:23,186[0m][[34mroot[0m][[32mINFO[0m] - Step 473600 @ 511.4 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 1802.9, step = 473600, mean_episode_return = 0.88463, mean_episode_step = 89.322, total_loss = 487.71, entropy_loss = -7.7521, pg_loss = 424.3, baseline_loss = 71.168, learner_queue_size = 32, _tick = 184, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:26:28,191[0m][[34mroot[0m][[32mINFO[0m] - Step 473600 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1808.2, step = 473600, mean_episode_return = 0.88463, mean_episode_step = 89.322, total_loss = 487.71, entropy_loss = -7.7521, pg_loss = 424.3, baseline_loss = 71.168, learner_queue_size = 32, _tick = 184, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:26:33,196[0m][[34mroot[0m][[32mINFO[0m] - Step 476160 @ 511.5 SPS. Inference batcher size: 16. Learner queue size: 32. Other stats: (train_seconds = 1813.2, step = 476160, mean_episode_return = 0.58695, mean_episode_step = 79.316, total_loss = 230.5, entropy_loss = -7.7755, pg_loss = 172.82, baseline_loss = 65.459, learner_queue_size = 32, _tick = 185, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:26:38,201[0m][[34mroot[0m][[32mINFO[0m] - Step 476160 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1818.2, step = 476160, mean_episode_return = 0.58695, mean_episode_step = 79.316, total_loss = 230.5, entropy_loss = -7.7755, pg_loss = 172.82, baseline_loss = 65.459, learner_queue_size = 32, _tick = 185, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:26:43,206[0m][[34mroot[0m][[32mINFO[0m] - Step 478720 @ 511.5 SPS. Inference batcher size: 18. Learner queue size: 32. Other stats: (train_seconds = 1823.2, step = 478720, mean_episode_return = 0.47837, mean_episode_step = 69.174, total_loss = -110.96, entropy_loss = -7.8112, pg_loss = -160.9, baseline_loss = 57.745, learner_queue_size = 32, _tick = 186, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:26:48,211[0m][[34mroot[0m][[32mINFO[0m] - Step 478720 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1828.2, step = 478720, mean_episode_return = 0.47837, mean_episode_step = 69.174, total_loss = -110.96, entropy_loss = -7.8112, pg_loss = -160.9, baseline_loss = 57.745, learner_queue_size = 32, _tick = 186, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:26:53,216[0m][[34mroot[0m][[32mINFO[0m] - Step 481280 @ 511.5 SPS. Inference batcher size: 21. Learner queue size: 32. Other stats: (train_seconds = 1833.2, step = 481280, mean_episode_return = 0.56638, mean_episode_step = 67.388, total_loss = 16.641, entropy_loss = -7.8084, pg_loss = -44.37, baseline_loss = 68.82, learner_queue_size = 32, _tick = 187, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:26:58,228[0m][[34mroot[0m][[32mINFO[0m] - Step 481280 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1838.2, step = 481280, mean_episode_return = 0.56638, mean_episode_step = 67.388, total_loss = 16.641, entropy_loss = -7.8084, pg_loss = -44.37, baseline_loss = 68.82, learner_queue_size = 32, _tick = 187, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:27:03,234[0m][[34mroot[0m][[32mINFO[0m] - Step 483840 @ 511.4 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1843.2, step = 483840, mean_episode_return = 0.48732, mean_episode_step = 80.58, total_loss = -70.604, entropy_loss = -7.7781, pg_loss = -125.56, baseline_loss = 62.736, learner_queue_size = 32, _tick = 188, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:27:08,239[0m][[34mroot[0m][[32mINFO[0m] - Step 486400 @ 511.5 SPS. Inference batcher size: 10. Learner queue size: 32. Other stats: (train_seconds = 1848.2, step = 486400, mean_episode_return = 0.73862, mean_episode_step = 70.316, total_loss = 529.58, entropy_loss = -7.767, pg_loss = 447.54, baseline_loss = 89.81, learner_queue_size = 32, _tick = 189, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:27:13,245[0m][[34mroot[0m][[32mINFO[0m] - Step 486400 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1853.2, step = 486400, mean_episode_return = 0.73862, mean_episode_step = 70.316, total_loss = 529.58, entropy_loss = -7.767, pg_loss = 447.54, baseline_loss = 89.81, learner_queue_size = 32, _tick = 189, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:27:18,250[0m][[34mroot[0m][[32mINFO[0m] - Step 488960 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1858.2, step = 488960, mean_episode_return = 0.55806, mean_episode_step = 79.727, total_loss = 231.22, entropy_loss = -7.7271, pg_loss = 161.7, baseline_loss = 77.247, learner_queue_size = 32, _tick = 190, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:27:23,256[0m][[34mroot[0m][[32mINFO[0m] - Step 488960 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1863.3, step = 488960, mean_episode_return = 0.55806, mean_episode_step = 79.727, total_loss = 231.22, entropy_loss = -7.7271, pg_loss = 161.7, baseline_loss = 77.247, learner_queue_size = 32, _tick = 190, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:27:28,262[0m][[34mroot[0m][[32mINFO[0m] - Step 491520 @ 511.4 SPS. Inference batcher size: 19. Learner queue size: 32. Other stats: (train_seconds = 1868.3, step = 491520, mean_episode_return = 0.58173, mean_episode_step = 74.38, total_loss = 475.29, entropy_loss = -7.7481, pg_loss = 384.54, baseline_loss = 98.497, learner_queue_size = 32, _tick = 191, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:27:33,268[0m][[34mroot[0m][[32mINFO[0m] - Step 491520 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1873.3, step = 491520, mean_episode_return = 0.58173, mean_episode_step = 74.38, total_loss = 475.29, entropy_loss = -7.7481, pg_loss = 384.54, baseline_loss = 98.497, learner_queue_size = 32, _tick = 191, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:27:38,273[0m][[34mroot[0m][[32mINFO[0m] - Step 494080 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1878.3, step = 494080, mean_episode_return = 0.8488, mean_episode_step = 88.375, total_loss = 508.75, entropy_loss = -7.6947, pg_loss = 429.23, baseline_loss = 87.209, learner_queue_size = 32, _tick = 192, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:27:43,279[0m][[34mroot[0m][[32mINFO[0m] - Step 494080 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1883.3, step = 494080, mean_episode_return = 0.8488, mean_episode_step = 88.375, total_loss = 508.75, entropy_loss = -7.6947, pg_loss = 429.23, baseline_loss = 87.209, learner_queue_size = 32, _tick = 192, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:27:48,284[0m][[34mroot[0m][[32mINFO[0m] - Step 496640 @ 511.5 SPS. Inference batcher size: 17. Learner queue size: 32. Other stats: (train_seconds = 1888.3, step = 496640, mean_episode_return = 0.53316, mean_episode_step = 67.574, total_loss = -169.14, entropy_loss = -7.7598, pg_loss = -236.95, baseline_loss = 75.571, learner_queue_size = 32, _tick = 193, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:27:53,287[0m][[34mroot[0m][[32mINFO[0m] - Step 496640 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1893.3, step = 496640, mean_episode_return = 0.53316, mean_episode_step = 67.574, total_loss = -169.14, entropy_loss = -7.7598, pg_loss = -236.95, baseline_loss = 75.571, learner_queue_size = 32, _tick = 193, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:27:58,292[0m][[34mroot[0m][[32mINFO[0m] - Step 499200 @ 511.5 SPS. Inference batcher size: 11. Learner queue size: 32. Other stats: (train_seconds = 1898.3, step = 499200, mean_episode_return = 0.53411, mean_episode_step = 83.995, total_loss = 274.84, entropy_loss = -7.7195, pg_loss = 210.55, baseline_loss = 72.016, learner_queue_size = 32, _tick = 194, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:28:03,297[0m][[34mroot[0m][[32mINFO[0m] - Step 499200 @ 0.0 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1903.3, step = 499200, mean_episode_return = 0.53411, mean_episode_step = 83.995, total_loss = 274.84, entropy_loss = -7.7195, pg_loss = 210.55, baseline_loss = 72.016, learner_queue_size = 32, _tick = 194, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:28:08,302[0m][[34mroot[0m][[32mINFO[0m] - Step 501760 @ 511.5 SPS. Inference batcher size: 0. Learner queue size: 32. Other stats: (train_seconds = 1908.3, step = 501760, mean_episode_return = 0.51742, mean_episode_step = 86.119, total_loss = 163.59, entropy_loss = -7.7198, pg_loss = 103.5, baseline_loss = 67.814, learner_queue_size = 32, _tick = 195, _time = 1.7372e+09)[0m
[[36m2025-01-18 11:28:08,302[0m][[34mroot[0m][[32mINFO[0m] - Learning finished after 501760 steps.[0m
[[36m2025-01-18 11:28:08,303[0m][[34mroot[0m][[32mINFO[0m] - Saving checkpoint to /opt/minihack/checkpoint.tar[0m
